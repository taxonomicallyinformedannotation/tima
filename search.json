[
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite tima in publications use:\n\nRutz A, Allard P (2025). tima: Taxonomically Informed Metabolite Annotation. doi:10.5281/zenodo.5797920, R package, version 2.11.0.\n\n\nRutz A, Dounoue-Kubo M, Ollivier S, Bisson J, Bagheri M, Saesong T, Ebrahimi S, Ingkaninan K, Wolfender J, Allard P (2019). “Taxonomically Informed Scoring Enhances Confidence in Natural Products Annotation.” Frontiers in Plant Science, 10. ISSN 1664-462X, doi:10.3389/FPLS.2019.01329, https://doi.org/10.3389/fpls.2019.01329."
  },
  {
    "objectID": "man/calculate_similarity.html",
    "href": "man/calculate_similarity.html",
    "title": "tima",
    "section": "",
    "text": "Calculates similarity scores between query and target spectra using either entropy, cosine, or GNPS methods.\n\n\n\ncalculate_similarity(\n  method,\n  query_spectrum,\n  target_spectrum,\n  query_precursor,\n  target_precursor,\n  dalton,\n  ppm,\n  return_matched_peaks = FALSE,\n  ...\n)\n\n\n\n\n\n\n\nmethod\n\n\nCharacter string specifying method: \"entropy\", \"gnps\", or \"cosine\"\n\n\n\n\nquery_spectrum\n\n\nNumeric matrix with columns for mz and intensity\n\n\n\n\ntarget_spectrum\n\n\nNumeric matrix with columns for mz and intensity\n\n\n\n\nquery_precursor\n\n\nNumeric precursor m/z value for query\n\n\n\n\ntarget_precursor\n\n\nNumeric precursor m/z value for target\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching\n\n\n\n\nreturn_matched_peaks\n\n\nLogical; return matched peaks count? Not compatible with ‘entropy’ method. Default: FALSE\n\n\n\n\n…\n\n\nAdditional arguments passed to MsCoreUtils::join\n\n\n\n\n\n\nNumeric similarity score (0-1), or list with score and matches if return_matched_peaks = TRUE. Returns 0.0 if calculation fails.\n\n\n\n\nlibrary(\"tima\")\n\nsp_1 &lt;- cbind(\n  mz = c(10, 36, 63, 91, 93),\n  intensity = c(14, 15, 999, 650, 1)\n)\nprecursor_1 &lt;- 123.4567\nprecursor_2 &lt;- precursor_1 + 14\nsp_2 &lt;- cbind(\n  mz = c(10, 12, 50, 63, 105),\n  intensity = c(35, 5, 16, 999, 450)\n)\ncalculate_similarity(\n  method = \"entropy\",\n  query_spectrum = sp_1,\n  target_spectrum = sp_2,\n  query_precursor = precursor_1,\n  target_precursor = precursor_2,\n  dalton = 0.005,\n  ppm = 10.0\n)\n\n[1] 0.5427377\n\ncalculate_similarity(\n  method = \"gnps\",\n  query_spectrum = sp_1,\n  target_spectrum = sp_2,\n  query_precursor = precursor_1,\n  target_precursor = precursor_2,\n  dalton = 0.005,\n  ppm = 10.0,\n  return_matched_peaks = TRUE\n)\n\n$score\n[1] 0.9923501\n\n$matches\n[1] 4",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_similarity"
    ]
  },
  {
    "objectID": "man/calculate_similarity.html#calculate-similarity-between-spectra",
    "href": "man/calculate_similarity.html#calculate-similarity-between-spectra",
    "title": "tima",
    "section": "",
    "text": "Calculates similarity scores between query and target spectra using either entropy, cosine, or GNPS methods.\n\n\n\ncalculate_similarity(\n  method,\n  query_spectrum,\n  target_spectrum,\n  query_precursor,\n  target_precursor,\n  dalton,\n  ppm,\n  return_matched_peaks = FALSE,\n  ...\n)\n\n\n\n\n\n\n\nmethod\n\n\nCharacter string specifying method: \"entropy\", \"gnps\", or \"cosine\"\n\n\n\n\nquery_spectrum\n\n\nNumeric matrix with columns for mz and intensity\n\n\n\n\ntarget_spectrum\n\n\nNumeric matrix with columns for mz and intensity\n\n\n\n\nquery_precursor\n\n\nNumeric precursor m/z value for query\n\n\n\n\ntarget_precursor\n\n\nNumeric precursor m/z value for target\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching\n\n\n\n\nreturn_matched_peaks\n\n\nLogical; return matched peaks count? Not compatible with ‘entropy’ method. Default: FALSE\n\n\n\n\n…\n\n\nAdditional arguments passed to MsCoreUtils::join\n\n\n\n\n\n\nNumeric similarity score (0-1), or list with score and matches if return_matched_peaks = TRUE. Returns 0.0 if calculation fails.\n\n\n\n\nlibrary(\"tima\")\n\nsp_1 &lt;- cbind(\n  mz = c(10, 36, 63, 91, 93),\n  intensity = c(14, 15, 999, 650, 1)\n)\nprecursor_1 &lt;- 123.4567\nprecursor_2 &lt;- precursor_1 + 14\nsp_2 &lt;- cbind(\n  mz = c(10, 12, 50, 63, 105),\n  intensity = c(35, 5, 16, 999, 450)\n)\ncalculate_similarity(\n  method = \"entropy\",\n  query_spectrum = sp_1,\n  target_spectrum = sp_2,\n  query_precursor = precursor_1,\n  target_precursor = precursor_2,\n  dalton = 0.005,\n  ppm = 10.0\n)\n\n[1] 0.5427377\n\ncalculate_similarity(\n  method = \"gnps\",\n  query_spectrum = sp_1,\n  target_spectrum = sp_2,\n  query_precursor = precursor_1,\n  target_precursor = precursor_2,\n  dalton = 0.005,\n  ppm = 10.0,\n  return_matched_peaks = TRUE\n)\n\n$score\n[1] 0.9923501\n\n$matches\n[1] 4",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_similarity"
    ]
  },
  {
    "objectID": "man/import_spectra.html",
    "href": "man/import_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function imports mass spectra from various file formats (.mgf, .msp, .rds), harmonizes metadata field names, filters by MS level and polarity, optionally combines replicate spectra, and sanitizes peak data.\n\n\n\nimport_spectra(\n  file,\n  cutoff = 0,\n  dalton = 0.01,\n  polarity = NA,\n  ppm = 10,\n  sanitize = TRUE,\n  combine = TRUE\n)\n\n\n\n\n\n\n\nfile\n\n\nCharacter string path to the spectrum file (.mgf, .msp, or .rds)\n\n\n\n\ncutoff\n\n\nNumeric absolute minimal intensity threshold (default: 0)\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching (default: 0.01)\n\n\n\n\npolarity\n\n\nCharacter string for polarity filtering: \"pos\", \"neg\", or NA to keep all (default: NA)\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching (default: 10)\n\n\n\n\nsanitize\n\n\nLogical flag indicating whether to sanitize spectra (default: TRUE)\n\n\n\n\ncombine\n\n\nLogical flag indicating whether to combine replicate spectra (default: TRUE)\n\n\n\n\n\n\nSpectra object containing the imported and processed spectra\n\n\n\n\nlibrary(\"tima\")\n\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_default_paths()$data$source$spectra\n)\nimport_spectra(file = get_default_paths()$data$source$spectra)\nimport_spectra(\n  file = get_default_paths()$data$source$spectra,\n  sanitize = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "import_spectra"
    ]
  },
  {
    "objectID": "man/import_spectra.html#import-spectra",
    "href": "man/import_spectra.html#import-spectra",
    "title": "tima",
    "section": "",
    "text": "This function imports mass spectra from various file formats (.mgf, .msp, .rds), harmonizes metadata field names, filters by MS level and polarity, optionally combines replicate spectra, and sanitizes peak data.\n\n\n\nimport_spectra(\n  file,\n  cutoff = 0,\n  dalton = 0.01,\n  polarity = NA,\n  ppm = 10,\n  sanitize = TRUE,\n  combine = TRUE\n)\n\n\n\n\n\n\n\nfile\n\n\nCharacter string path to the spectrum file (.mgf, .msp, or .rds)\n\n\n\n\ncutoff\n\n\nNumeric absolute minimal intensity threshold (default: 0)\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching (default: 0.01)\n\n\n\n\npolarity\n\n\nCharacter string for polarity filtering: \"pos\", \"neg\", or NA to keep all (default: NA)\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching (default: 10)\n\n\n\n\nsanitize\n\n\nLogical flag indicating whether to sanitize spectra (default: TRUE)\n\n\n\n\ncombine\n\n\nLogical flag indicating whether to combine replicate spectra (default: TRUE)\n\n\n\n\n\n\nSpectra object containing the imported and processed spectra\n\n\n\n\nlibrary(\"tima\")\n\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_default_paths()$data$source$spectra\n)\nimport_spectra(file = get_default_paths()$data$source$spectra)\nimport_spectra(\n  file = get_default_paths()$data$source$spectra,\n  sanitize = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "import_spectra"
    ]
  },
  {
    "objectID": "man/get_example_sirius.html",
    "href": "man/get_example_sirius.html",
    "title": "tima",
    "section": "",
    "text": "This function downloads example SIRIUS annotation files for testing and demonstration purposes. Downloads both SIRIUS v5 and v6 format files.\n\n\n\nget_example_sirius(\n  url = get_default_paths()\\$urls\\$examples\\$sirius,\n  export = get_default_paths()\\$data\\$interim\\$annotations\\$example_sirius\n)\n\n\n\n\n\n\n\nurl\n\n\nList containing URLs for SIRIUS examples (must have $v5 and $v6 elements)\n\n\n\n\nexport\n\n\nList containing export paths for SIRIUS examples (must have $v5 and $v6 elements)\n\n\n\n\n\n\nNULL (invisibly). Downloads files as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\nget_example_sirius()"
  },
  {
    "objectID": "man/get_example_sirius.html#get-example-sirius",
    "href": "man/get_example_sirius.html#get-example-sirius",
    "title": "tima",
    "section": "",
    "text": "This function downloads example SIRIUS annotation files for testing and demonstration purposes. Downloads both SIRIUS v5 and v6 format files.\n\n\n\nget_example_sirius(\n  url = get_default_paths()\\$urls\\$examples\\$sirius,\n  export = get_default_paths()\\$data\\$interim\\$annotations\\$example_sirius\n)\n\n\n\n\n\n\n\nurl\n\n\nList containing URLs for SIRIUS examples (must have $v5 and $v6 elements)\n\n\n\n\nexport\n\n\nList containing export paths for SIRIUS examples (must have $v5 and $v6 elements)\n\n\n\n\n\n\nNULL (invisibly). Downloads files as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\nget_example_sirius()"
  },
  {
    "objectID": "man/extract_spectra.html",
    "href": "man/extract_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function extracts and harmonizes spectra data from a Spectra object into a flat data frame format. It handles column name inconsistencies, type conversions, and extracts peak lists (mz/intensity).\n\n\n\nextract_spectra(object)\n\n\n\n\n\n\n\nobject\n\n\nSpectra object from the Spectra package\n\n\n\n\n\n\nData frame containing harmonized spectra metadata with additional mz and intensity list columns containing peak data\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/extract_spectra.html#extract-spectra-from-a-spectra-object",
    "href": "man/extract_spectra.html#extract-spectra-from-a-spectra-object",
    "title": "tima",
    "section": "",
    "text": "This function extracts and harmonizes spectra data from a Spectra object into a flat data frame format. It handles column name inconsistencies, type conversions, and extracts peak lists (mz/intensity).\n\n\n\nextract_spectra(object)\n\n\n\n\n\n\n\nobject\n\n\nSpectra object from the Spectra package\n\n\n\n\n\n\nData frame containing harmonized spectra metadata with additional mz and intensity list columns containing peak data\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/select_annotations_columns.html",
    "href": "man/select_annotations_columns.html",
    "title": "tima",
    "section": "",
    "text": "This function selects and standardizes annotation columns by filtering to relevant fields, cleaning NULL/NA values, rounding numeric values, and complementing with structure metadata. Used to prepare annotation results for downstream analysis.\n\n\n\nselect_annotations_columns(\n  df,\n  str_stereo = get(\"str_stereo\", envir = parent.frame()),\n  str_met = get(\"str_met\", envir = parent.frame()),\n  str_nam = get(\"str_nam\", envir = parent.frame()),\n  str_tax_cla = get(\"str_tax_cla\", envir = parent.frame()),\n  str_tax_npc = get(\"str_tax_npc\", envir = parent.frame())\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing annotation results with structure and candidate information\n\n\n\n\nstr_stereo\n\n\nCharacter string path to file containing structure stereochemistry\n\n\n\n\nstr_met\n\n\nCharacter string path to file containing structure metadata\n\n\n\n\nstr_nam\n\n\nCharacter string path to file containing structure names\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to file containing Classyfire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to file containing NPClassifier taxonomy\n\n\n\n\n\n\nData frame with standardized annotation columns, cleaned values, and complemented metadata\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/select_annotations_columns.html#select-annotations-columns",
    "href": "man/select_annotations_columns.html#select-annotations-columns",
    "title": "tima",
    "section": "",
    "text": "This function selects and standardizes annotation columns by filtering to relevant fields, cleaning NULL/NA values, rounding numeric values, and complementing with structure metadata. Used to prepare annotation results for downstream analysis.\n\n\n\nselect_annotations_columns(\n  df,\n  str_stereo = get(\"str_stereo\", envir = parent.frame()),\n  str_met = get(\"str_met\", envir = parent.frame()),\n  str_nam = get(\"str_nam\", envir = parent.frame()),\n  str_tax_cla = get(\"str_tax_cla\", envir = parent.frame()),\n  str_tax_npc = get(\"str_tax_npc\", envir = parent.frame())\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing annotation results with structure and candidate information\n\n\n\n\nstr_stereo\n\n\nCharacter string path to file containing structure stereochemistry\n\n\n\n\nstr_met\n\n\nCharacter string path to file containing structure metadata\n\n\n\n\nstr_nam\n\n\nCharacter string path to file containing structure names\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to file containing Classyfire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to file containing NPClassifier taxonomy\n\n\n\n\n\n\nData frame with standardized annotation columns, cleaned values, and complemented metadata\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/get_example_files.html",
    "href": "man/get_example_files.html",
    "title": "tima",
    "section": "",
    "text": "This function downloads example data files for testing and demonstration purposes. Supports downloading features, metadata, SIRIUS annotations, mass spectra, and spectral libraries with retention times.\n\n\n\nget_example_files(\n  example = c(\"features\", \"metadata\", \"sirius\", \"spectra\"),\n  in_cache = TRUE\n)\n\n\n\n\n\n\n\nexample\n\n\nCharacter vector specifying which example files to download. Valid options: \"features\", \"metadata\", \"sirius\", \"spectra\", \"spectral_lib_with_rt\"\n\n\n\n\nin_cache\n\n\nLogical whether to store files in the cache directory (default: TRUE)\n\n\n\n\n\n\nNULL (invisibly). Downloads files as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Download features and metadata examples\nget_example_files(example = c(\"features\", \"metadata\"))\n\n# Download all example files to cache\nget_example_files(\n  example = c(\"features\", \"metadata\", \"sirius\", \"spectra\"),\n  in_cache = TRUE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_example_files"
    ]
  },
  {
    "objectID": "man/get_example_files.html#get-example-files",
    "href": "man/get_example_files.html#get-example-files",
    "title": "tima",
    "section": "",
    "text": "This function downloads example data files for testing and demonstration purposes. Supports downloading features, metadata, SIRIUS annotations, mass spectra, and spectral libraries with retention times.\n\n\n\nget_example_files(\n  example = c(\"features\", \"metadata\", \"sirius\", \"spectra\"),\n  in_cache = TRUE\n)\n\n\n\n\n\n\n\nexample\n\n\nCharacter vector specifying which example files to download. Valid options: \"features\", \"metadata\", \"sirius\", \"spectra\", \"spectral_lib_with_rt\"\n\n\n\n\nin_cache\n\n\nLogical whether to store files in the cache directory (default: TRUE)\n\n\n\n\n\n\nNULL (invisibly). Downloads files as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Download features and metadata examples\nget_example_files(example = c(\"features\", \"metadata\"))\n\n# Download all example files to cache\nget_example_files(\n  example = c(\"features\", \"metadata\", \"sirius\", \"spectra\"),\n  in_cache = TRUE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_example_files"
    ]
  },
  {
    "objectID": "man/process_smiles.html",
    "href": "man/process_smiles.html",
    "title": "tima",
    "section": "",
    "text": "This function processes SMILES strings using RDKit (via Python) to standardize structures, generate InChIKeys, calculate molecular properties, and extract 2D representations. Results are cached to avoid reprocessing.\n\n\n\nprocess_smiles(df, smiles_colname = \"structure_smiles_initial\", cache = NULL)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing SMILES strings to process\n\n\n\n\nsmiles_colname\n\n\nCharacter string name of the column containing SMILES (default: \"structure_smiles_initial\")\n\n\n\n\ncache\n\n\nCharacter string path to cached processed SMILES file, or NULL to skip caching (default: NULL)\n\n\n\n\n\n\nData frame with processed SMILES including InChIKey, molecular formula, exact mass, 2D SMILES, xLogP, and connectivity layer\n\n\n\n\nlibrary(\"tima\")\n\nsmiles &lt;- \"C=C[C@H]1[C@H](OC=C2C1=CCOC2=O)O[C@H]3[C@H]([C@H]([C@H]([C@H](O3)CO)O)O)O\"\ndata.frame(\n  \"structure_smiles_initial\" = smiles\n) |&gt;\n  process_smiles()",
    "crumbs": [
      "Get started",
      "Functions",
      "process_smiles"
    ]
  },
  {
    "objectID": "man/process_smiles.html#process-smiles",
    "href": "man/process_smiles.html#process-smiles",
    "title": "tima",
    "section": "",
    "text": "This function processes SMILES strings using RDKit (via Python) to standardize structures, generate InChIKeys, calculate molecular properties, and extract 2D representations. Results are cached to avoid reprocessing.\n\n\n\nprocess_smiles(df, smiles_colname = \"structure_smiles_initial\", cache = NULL)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing SMILES strings to process\n\n\n\n\nsmiles_colname\n\n\nCharacter string name of the column containing SMILES (default: \"structure_smiles_initial\")\n\n\n\n\ncache\n\n\nCharacter string path to cached processed SMILES file, or NULL to skip caching (default: NULL)\n\n\n\n\n\n\nData frame with processed SMILES including InChIKey, molecular formula, exact mass, 2D SMILES, xLogP, and connectivity layer\n\n\n\n\nlibrary(\"tima\")\n\nsmiles &lt;- \"C=C[C@H]1[C@H](OC=C2C1=CCOC2=O)O[C@H]3[C@H]([C@H]([C@H]([C@H](O3)CO)O)O)O\"\ndata.frame(\n  \"structure_smiles_initial\" = smiles\n) |&gt;\n  process_smiles()",
    "crumbs": [
      "Get started",
      "Functions",
      "process_smiles"
    ]
  },
  {
    "objectID": "man/weight_annotations.html",
    "href": "man/weight_annotations.html",
    "title": "tima",
    "section": "",
    "text": "This function weights annotations.\n\n\n\nweight_annotations(\n  library = get_params(step = \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  org_tax_ott = get_params(step =\n    \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  str_stereo = get_params(step =\n    \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  annotations = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$filtered,\n  canopus = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$prepared\\$canopus,\n  formula = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$prepared\\$formula,\n  components = get_params(step =\n    \"weight_annotations\")\\$files\\$networks\\$spectral\\$components\\$prepared,\n  edges = get_params(step = \"weight_annotations\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  taxa = get_params(step = \"weight_annotations\")\\$files\\$metadata\\$prepared,\n  output = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$processed,\n  candidates_neighbors = get_params(step =\n    \"weight_annotations\")\\$annotations\\$candidates\\$neighbors,\n  candidates_final = get_params(step = \"weight_annotations\")\\$annotations\\$candidates\\$final,\n  best_percentile = get_params(step =\n    \"weight_annotations\")\\$annotations\\$candidates\\$best_percentile,\n  weight_spectral = get_params(step = \"weight_annotations\")\\$weights\\$global\\$spectral,\n  weight_chemical = get_params(step = \"weight_annotations\")\\$weights\\$global\\$chemical,\n  weight_biological = get_params(step = \"weight_annotations\")\\$weights\\$global\\$biological,\n  score_biological_domain = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$domain,\n  score_biological_kingdom = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$kingdom,\n  score_biological_phylum = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$phylum,\n  score_biological_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$class,\n  score_biological_order = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$order,\n  score_biological_infraorder = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$infraorder,\n  score_biological_family = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$family,\n  score_biological_subfamily = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subfamily,\n  score_biological_tribe = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$tribe,\n  score_biological_subtribe = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subtribe,\n  score_biological_genus = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$genus,\n  score_biological_subgenus = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subgenus,\n  score_biological_species = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$species,\n  score_biological_subspecies = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subspecies,\n  score_biological_variety = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$variety,\n  score_chemical_cla_kingdom = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$kingdom,\n  score_chemical_cla_superclass = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$superclass,\n  score_chemical_cla_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$class,\n  score_chemical_cla_parent = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$parent,\n  score_chemical_npc_pathway = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$pathway,\n  score_chemical_npc_superclass = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$superclass,\n  score_chemical_npc_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$class,\n  minimal_consistency = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$consistency,\n  minimal_ms1_bio = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$biological,\n  minimal_ms1_chemo = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$chemical,\n  minimal_ms1_condition = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$condition,\n  ms1_only = get_params(step = \"weight_annotations\")\\$annotations\\$ms1only,\n  compounds_names = get_params(step = \"weight_annotations\")\\$options\\$compounds_names,\n  high_confidence = get_params(step = \"weight_annotations\")\\$options\\$high_confidence,\n  remove_ties = get_params(step = \"weight_annotations\")\\$options\\$remove_ties,\n  summarize = get_params(step = \"weight_annotations\")\\$options\\$summarize,\n  pattern = get_params(step = \"weight_annotations\")\\$files\\$pattern,\n  force = get_params(step = \"weight_annotations\")\\$options\\$force\n)\n\n\n\n\n\n\n\nlibrary\n\n\nLibrary containing the keys\n\n\n\n\norg_tax_ott\n\n\nFile containing organisms taxonomy (OTT)\n\n\n\n\nstr_stereo\n\n\nFile containing structures stereo\n\n\n\n\nannotations\n\n\nPrepared annotations file\n\n\n\n\ncanopus\n\n\nPrepared canopus file\n\n\n\n\nformula\n\n\nPrepared formula file\n\n\n\n\ncomponents\n\n\nPrepared components file\n\n\n\n\nedges\n\n\nPrepared edges file\n\n\n\n\ntaxa\n\n\nPrepared taxed features file\n\n\n\n\noutput\n\n\nOutput file\n\n\n\n\ncandidates_neighbors\n\n\nNumber of neighbors candidates to keep\n\n\n\n\ncandidates_final\n\n\nNumber of final candidates to keep\n\n\n\n\nbest_percentile\n\n\nNumeric percentile threshold (0-1) for selecting top candidates within each feature (default: 0.9). Used for consistent filtering between mini and filtered outputs.\n\n\n\n\nweight_spectral\n\n\nWeight for the spectral score\n\n\n\n\nweight_chemical\n\n\nWeight for the biological score\n\n\n\n\nweight_biological\n\n\nWeight for the chemical consistency score\n\n\n\n\nscore_biological_domain\n\n\nScore for a domain match (should be lower than kingdom)\n\n\n\n\nscore_biological_kingdom\n\n\nScore for a kingdom match (should be lower than phylum)\n\n\n\n\nscore_biological_phylum\n\n\nScore for a phylum match (should be lower than class)\n\n\n\n\nscore_biological_class\n\n\nScore for a class match (should be lower than order)\n\n\n\n\nscore_biological_order\n\n\nScore for a order match (should be lower than infraorder)\n\n\n\n\nscore_biological_infraorder\n\n\nScore for a infraorder match (should be lower than order)\n\n\n\n\nscore_biological_family\n\n\nScore for a family match (should be lower than subfamily)\n\n\n\n\nscore_biological_subfamily\n\n\nScore for a subfamily match (should be lower than family)\n\n\n\n\nscore_biological_tribe\n\n\nScore for a tribe match (should be lower than subtribe)\n\n\n\n\nscore_biological_subtribe\n\n\nScore for a subtribe match (should be lower than genus)\n\n\n\n\nscore_biological_genus\n\n\nScore for a genus match (should be lower than subgenus)\n\n\n\n\nscore_biological_subgenus\n\n\nScore for a subgenus match (should be lower than species)\n\n\n\n\nscore_biological_species\n\n\nScore for a species match (should be lower than subspecies)\n\n\n\n\nscore_biological_subspecies\n\n\nScore for a subspecies match (should be lower than variety)\n\n\n\n\nscore_biological_variety\n\n\nScore for a variety match (should be the highest)\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nScore for a Classyfire kingdom match (should be lower than  Classyfire superclass)\n\n\n\n\nscore_chemical_cla_superclass\n\n\nScore for a Classyfire superclass match (should be lower than Classyfire class)\n\n\n\n\nscore_chemical_cla_class\n\n\nScore for a Classyfire class match (should be lower than Classyfire parent)\n\n\n\n\nscore_chemical_cla_parent\n\n\nScore for a Classyfire parent match (should be the highest)\n\n\n\n\nscore_chemical_npc_pathway\n\n\nScore for a NPC pathway match (should be lower than  NPC superclass)\n\n\n\n\nscore_chemical_npc_superclass\n\n\nScore for a NPC superclass match (should be lower than NPC class)\n\n\n\n\nscore_chemical_npc_class\n\n\nScore for a NPC class match (should be the highest)\n\n\n\n\nminimal_consistency\n\n\nMinimal consistency score for a class. FLOAT\n\n\n\n\nminimal_ms1_bio\n\n\nMinimal biological score to keep MS1 based annotation\n\n\n\n\nminimal_ms1_chemo\n\n\nMinimal chemical score to keep MS1 based annotation\n\n\n\n\nminimal_ms1_condition\n\n\nCondition to be used. Must be \"OR\" or \"AND\".\n\n\n\n\nms1_only\n\n\nKeep only MS1 annotations. BOOLEAN\n\n\n\n\ncompounds_names\n\n\nReport compounds names. Can be very large. BOOLEAN\n\n\n\n\nhigh_confidence\n\n\nReport high confidence candidates only. BOOLEAN\n\n\n\n\nremove_ties\n\n\nRemove ties. BOOLEAN\n\n\n\n\nsummarize\n\n\nSummarize results (1 row per feature). BOOLEAN\n\n\n\n\npattern\n\n\nPattern to identify your job. STRING\n\n\n\n\nforce\n\n\nForce parameters. Use it at your own risk\n\n\n\n\n\n\nThe path to the weighted annotations\n\n\n\nannotate_masses weight_bio weight_chemo\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nlibrary &lt;- get_params(step = \"weight_annotations\")$files$libraries$sop$merged$keys |&gt;\n  gsub(\n    pattern = \".gz\",\n    replacement = \"\",\n    fixed = TRUE\n  )\norg_tax_ott &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/organisms/taxonomies/ott.tsv\"\n)\nstr_stereo &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/structures/stereo.tsv\"\n)\nannotations &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_annotationsFiltered.tsv\"\n)\ncanopus &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_canopusPrepared.tsv\"\n)\nformula &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_formulaPrepared.tsv\"\n)\ncomponents &lt;- paste0(\n  \"data/interim/features/\",\n  \"example_componentsPrepared.tsv\"\n)\nedges &lt;- paste0(\n  \"data/interim/features/\",\n  \"example_edges.tsv\"\n)\ntaxa &lt;- paste0(\n  \"data/interim/taxa/\",\n  \"example_taxed.tsv\"\n)\nget_file(url = paste0(dir, library), export = library)\nget_file(url = paste0(dir, org_tax_ott), export = org_tax_ott)\nget_file(url = paste0(dir, str_stereo), export = str_stereo)\nget_file(url = paste0(dir, annotations), export = annotations)\nget_file(url = paste0(dir, canopus), export = canopus)\nget_file(url = paste0(dir, formula), export = formula)\nget_file(url = paste0(dir, components), export = components)\nget_file(url = paste0(dir, edges), export = edges)\nget_file(url = paste0(dir, taxa), export = taxa)\nweight_annotations(\n  library = library,\n  org_tax_ott = org_tax_ott,\n  str_stereo = str_stereo,\n  annotations = annotations,\n  canopus = canopus,\n  formula = formula,\n  components = components,\n  edges = edges,\n  taxa = taxa\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "weight_annotations"
    ]
  },
  {
    "objectID": "man/weight_annotations.html#weight-annotations",
    "href": "man/weight_annotations.html#weight-annotations",
    "title": "tima",
    "section": "",
    "text": "This function weights annotations.\n\n\n\nweight_annotations(\n  library = get_params(step = \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  org_tax_ott = get_params(step =\n    \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  str_stereo = get_params(step =\n    \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  annotations = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$filtered,\n  canopus = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$prepared\\$canopus,\n  formula = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$prepared\\$formula,\n  components = get_params(step =\n    \"weight_annotations\")\\$files\\$networks\\$spectral\\$components\\$prepared,\n  edges = get_params(step = \"weight_annotations\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  taxa = get_params(step = \"weight_annotations\")\\$files\\$metadata\\$prepared,\n  output = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$processed,\n  candidates_neighbors = get_params(step =\n    \"weight_annotations\")\\$annotations\\$candidates\\$neighbors,\n  candidates_final = get_params(step = \"weight_annotations\")\\$annotations\\$candidates\\$final,\n  best_percentile = get_params(step =\n    \"weight_annotations\")\\$annotations\\$candidates\\$best_percentile,\n  weight_spectral = get_params(step = \"weight_annotations\")\\$weights\\$global\\$spectral,\n  weight_chemical = get_params(step = \"weight_annotations\")\\$weights\\$global\\$chemical,\n  weight_biological = get_params(step = \"weight_annotations\")\\$weights\\$global\\$biological,\n  score_biological_domain = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$domain,\n  score_biological_kingdom = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$kingdom,\n  score_biological_phylum = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$phylum,\n  score_biological_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$class,\n  score_biological_order = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$order,\n  score_biological_infraorder = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$infraorder,\n  score_biological_family = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$family,\n  score_biological_subfamily = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subfamily,\n  score_biological_tribe = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$tribe,\n  score_biological_subtribe = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subtribe,\n  score_biological_genus = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$genus,\n  score_biological_subgenus = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subgenus,\n  score_biological_species = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$species,\n  score_biological_subspecies = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subspecies,\n  score_biological_variety = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$variety,\n  score_chemical_cla_kingdom = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$kingdom,\n  score_chemical_cla_superclass = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$superclass,\n  score_chemical_cla_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$class,\n  score_chemical_cla_parent = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$parent,\n  score_chemical_npc_pathway = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$pathway,\n  score_chemical_npc_superclass = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$superclass,\n  score_chemical_npc_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$class,\n  minimal_consistency = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$consistency,\n  minimal_ms1_bio = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$biological,\n  minimal_ms1_chemo = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$chemical,\n  minimal_ms1_condition = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$condition,\n  ms1_only = get_params(step = \"weight_annotations\")\\$annotations\\$ms1only,\n  compounds_names = get_params(step = \"weight_annotations\")\\$options\\$compounds_names,\n  high_confidence = get_params(step = \"weight_annotations\")\\$options\\$high_confidence,\n  remove_ties = get_params(step = \"weight_annotations\")\\$options\\$remove_ties,\n  summarize = get_params(step = \"weight_annotations\")\\$options\\$summarize,\n  pattern = get_params(step = \"weight_annotations\")\\$files\\$pattern,\n  force = get_params(step = \"weight_annotations\")\\$options\\$force\n)\n\n\n\n\n\n\n\nlibrary\n\n\nLibrary containing the keys\n\n\n\n\norg_tax_ott\n\n\nFile containing organisms taxonomy (OTT)\n\n\n\n\nstr_stereo\n\n\nFile containing structures stereo\n\n\n\n\nannotations\n\n\nPrepared annotations file\n\n\n\n\ncanopus\n\n\nPrepared canopus file\n\n\n\n\nformula\n\n\nPrepared formula file\n\n\n\n\ncomponents\n\n\nPrepared components file\n\n\n\n\nedges\n\n\nPrepared edges file\n\n\n\n\ntaxa\n\n\nPrepared taxed features file\n\n\n\n\noutput\n\n\nOutput file\n\n\n\n\ncandidates_neighbors\n\n\nNumber of neighbors candidates to keep\n\n\n\n\ncandidates_final\n\n\nNumber of final candidates to keep\n\n\n\n\nbest_percentile\n\n\nNumeric percentile threshold (0-1) for selecting top candidates within each feature (default: 0.9). Used for consistent filtering between mini and filtered outputs.\n\n\n\n\nweight_spectral\n\n\nWeight for the spectral score\n\n\n\n\nweight_chemical\n\n\nWeight for the biological score\n\n\n\n\nweight_biological\n\n\nWeight for the chemical consistency score\n\n\n\n\nscore_biological_domain\n\n\nScore for a domain match (should be lower than kingdom)\n\n\n\n\nscore_biological_kingdom\n\n\nScore for a kingdom match (should be lower than phylum)\n\n\n\n\nscore_biological_phylum\n\n\nScore for a phylum match (should be lower than class)\n\n\n\n\nscore_biological_class\n\n\nScore for a class match (should be lower than order)\n\n\n\n\nscore_biological_order\n\n\nScore for a order match (should be lower than infraorder)\n\n\n\n\nscore_biological_infraorder\n\n\nScore for a infraorder match (should be lower than order)\n\n\n\n\nscore_biological_family\n\n\nScore for a family match (should be lower than subfamily)\n\n\n\n\nscore_biological_subfamily\n\n\nScore for a subfamily match (should be lower than family)\n\n\n\n\nscore_biological_tribe\n\n\nScore for a tribe match (should be lower than subtribe)\n\n\n\n\nscore_biological_subtribe\n\n\nScore for a subtribe match (should be lower than genus)\n\n\n\n\nscore_biological_genus\n\n\nScore for a genus match (should be lower than subgenus)\n\n\n\n\nscore_biological_subgenus\n\n\nScore for a subgenus match (should be lower than species)\n\n\n\n\nscore_biological_species\n\n\nScore for a species match (should be lower than subspecies)\n\n\n\n\nscore_biological_subspecies\n\n\nScore for a subspecies match (should be lower than variety)\n\n\n\n\nscore_biological_variety\n\n\nScore for a variety match (should be the highest)\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nScore for a Classyfire kingdom match (should be lower than  Classyfire superclass)\n\n\n\n\nscore_chemical_cla_superclass\n\n\nScore for a Classyfire superclass match (should be lower than Classyfire class)\n\n\n\n\nscore_chemical_cla_class\n\n\nScore for a Classyfire class match (should be lower than Classyfire parent)\n\n\n\n\nscore_chemical_cla_parent\n\n\nScore for a Classyfire parent match (should be the highest)\n\n\n\n\nscore_chemical_npc_pathway\n\n\nScore for a NPC pathway match (should be lower than  NPC superclass)\n\n\n\n\nscore_chemical_npc_superclass\n\n\nScore for a NPC superclass match (should be lower than NPC class)\n\n\n\n\nscore_chemical_npc_class\n\n\nScore for a NPC class match (should be the highest)\n\n\n\n\nminimal_consistency\n\n\nMinimal consistency score for a class. FLOAT\n\n\n\n\nminimal_ms1_bio\n\n\nMinimal biological score to keep MS1 based annotation\n\n\n\n\nminimal_ms1_chemo\n\n\nMinimal chemical score to keep MS1 based annotation\n\n\n\n\nminimal_ms1_condition\n\n\nCondition to be used. Must be \"OR\" or \"AND\".\n\n\n\n\nms1_only\n\n\nKeep only MS1 annotations. BOOLEAN\n\n\n\n\ncompounds_names\n\n\nReport compounds names. Can be very large. BOOLEAN\n\n\n\n\nhigh_confidence\n\n\nReport high confidence candidates only. BOOLEAN\n\n\n\n\nremove_ties\n\n\nRemove ties. BOOLEAN\n\n\n\n\nsummarize\n\n\nSummarize results (1 row per feature). BOOLEAN\n\n\n\n\npattern\n\n\nPattern to identify your job. STRING\n\n\n\n\nforce\n\n\nForce parameters. Use it at your own risk\n\n\n\n\n\n\nThe path to the weighted annotations\n\n\n\nannotate_masses weight_bio weight_chemo\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nlibrary &lt;- get_params(step = \"weight_annotations\")$files$libraries$sop$merged$keys |&gt;\n  gsub(\n    pattern = \".gz\",\n    replacement = \"\",\n    fixed = TRUE\n  )\norg_tax_ott &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/organisms/taxonomies/ott.tsv\"\n)\nstr_stereo &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/structures/stereo.tsv\"\n)\nannotations &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_annotationsFiltered.tsv\"\n)\ncanopus &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_canopusPrepared.tsv\"\n)\nformula &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_formulaPrepared.tsv\"\n)\ncomponents &lt;- paste0(\n  \"data/interim/features/\",\n  \"example_componentsPrepared.tsv\"\n)\nedges &lt;- paste0(\n  \"data/interim/features/\",\n  \"example_edges.tsv\"\n)\ntaxa &lt;- paste0(\n  \"data/interim/taxa/\",\n  \"example_taxed.tsv\"\n)\nget_file(url = paste0(dir, library), export = library)\nget_file(url = paste0(dir, org_tax_ott), export = org_tax_ott)\nget_file(url = paste0(dir, str_stereo), export = str_stereo)\nget_file(url = paste0(dir, annotations), export = annotations)\nget_file(url = paste0(dir, canopus), export = canopus)\nget_file(url = paste0(dir, formula), export = formula)\nget_file(url = paste0(dir, components), export = components)\nget_file(url = paste0(dir, edges), export = edges)\nget_file(url = paste0(dir, taxa), export = taxa)\nweight_annotations(\n  library = library,\n  org_tax_ott = org_tax_ott,\n  str_stereo = str_stereo,\n  annotations = annotations,\n  canopus = canopus,\n  formula = formula,\n  components = components,\n  edges = edges,\n  taxa = taxa\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "weight_annotations"
    ]
  },
  {
    "objectID": "man/prepare_annotations_sirius.html",
    "href": "man/prepare_annotations_sirius.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares SIRIUS annotation results (structure predictions, CANOPUS chemical classifications, and formula predictions) by harmonizing formats across SIRIUS versions (v5/v6), standardizing column names, and integrating with structure metadata.\n\n\n\nprepare_annotations_sirius(\n  input_directory = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$raw\\$sirius,\n  output_ann = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$structural\\$sirius,\n  output_can = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$canopus,\n  output_for = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$formula,\n  sirius_version = get_params(step = \"prepare_annotations_sirius\")\\$tools\\$sirius\\$version,\n  str_stereo = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput_directory\n\n\nCharacter string path to directory or zip file containing SIRIUS results\n\n\n\n\noutput_ann\n\n\nCharacter string path for prepared annotation results output\n\n\n\n\noutput_can\n\n\nCharacter string path for prepared CANOPUS results output\n\n\n\n\noutput_for\n\n\nCharacter string path for prepared formula results output\n\n\n\n\nsirius_version\n\n\nCharacter string SIRIUS version (\"v5\" or \"v6\")\n\n\n\n\nstr_stereo\n\n\nCharacter string path to file with structure stereochemistry\n\n\n\n\nstr_met\n\n\nCharacter string path to file with structure metadata\n\n\n\n\nstr_nam\n\n\nCharacter string path to file with structure names\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to file with ClassyFire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to file with NPClassifier taxonomy\n\n\n\n\n\n\nCharacter string path to the prepared SIRIUS annotations file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_sirius()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_sirius"
    ]
  },
  {
    "objectID": "man/prepare_annotations_sirius.html#prepare-annotations-sirius",
    "href": "man/prepare_annotations_sirius.html#prepare-annotations-sirius",
    "title": "tima",
    "section": "",
    "text": "This function prepares SIRIUS annotation results (structure predictions, CANOPUS chemical classifications, and formula predictions) by harmonizing formats across SIRIUS versions (v5/v6), standardizing column names, and integrating with structure metadata.\n\n\n\nprepare_annotations_sirius(\n  input_directory = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$raw\\$sirius,\n  output_ann = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$structural\\$sirius,\n  output_can = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$canopus,\n  output_for = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$formula,\n  sirius_version = get_params(step = \"prepare_annotations_sirius\")\\$tools\\$sirius\\$version,\n  str_stereo = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput_directory\n\n\nCharacter string path to directory or zip file containing SIRIUS results\n\n\n\n\noutput_ann\n\n\nCharacter string path for prepared annotation results output\n\n\n\n\noutput_can\n\n\nCharacter string path for prepared CANOPUS results output\n\n\n\n\noutput_for\n\n\nCharacter string path for prepared formula results output\n\n\n\n\nsirius_version\n\n\nCharacter string SIRIUS version (\"v5\" or \"v6\")\n\n\n\n\nstr_stereo\n\n\nCharacter string path to file with structure stereochemistry\n\n\n\n\nstr_met\n\n\nCharacter string path to file with structure metadata\n\n\n\n\nstr_nam\n\n\nCharacter string path to file with structure names\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to file with ClassyFire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to file with NPClassifier taxonomy\n\n\n\n\n\n\nCharacter string path to the prepared SIRIUS annotations file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_sirius()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_sirius"
    ]
  },
  {
    "objectID": "man/filter_high_confidence_only.html",
    "href": "man/filter_high_confidence_only.html",
    "title": "tima",
    "section": "",
    "text": "This function filters annotation results to retain only high-confidence candidates based on multiple scoring criteria (biological, initial, chemical) and retention time accuracy.\n\n\n\nfilter_high_confidence_only(\n  df,\n  score_bio_min = 0.85,\n  score_ini_min = 0.95,\n  score_final_min = 0.75,\n  error_rt_max = 0.1\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing annotation results with score columns\n\n\n\n\nscore_bio_min\n\n\nNumeric minimum biological score threshold (default: 0.85)\n\n\n\n\nscore_ini_min\n\n\nNumeric minimum initial score threshold (default: 0.95)\n\n\n\n\nscore_final_min\n\n\nNumeric minimum final (chemical) score threshold (default: 0.75)\n\n\n\n\nerror_rt_max\n\n\nNumeric maximum retention time error in minutes (default: 0.1)\n\n\n\n\n\n\nData frame containing only high-confidence annotations that meet at least one score threshold and pass RT error filtering\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/filter_high_confidence_only.html#filter-high-confidence-only",
    "href": "man/filter_high_confidence_only.html#filter-high-confidence-only",
    "title": "tima",
    "section": "",
    "text": "This function filters annotation results to retain only high-confidence candidates based on multiple scoring criteria (biological, initial, chemical) and retention time accuracy.\n\n\n\nfilter_high_confidence_only(\n  df,\n  score_bio_min = 0.85,\n  score_ini_min = 0.95,\n  score_final_min = 0.75,\n  error_rt_max = 0.1\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing annotation results with score columns\n\n\n\n\nscore_bio_min\n\n\nNumeric minimum biological score threshold (default: 0.85)\n\n\n\n\nscore_ini_min\n\n\nNumeric minimum initial score threshold (default: 0.95)\n\n\n\n\nscore_final_min\n\n\nNumeric minimum final (chemical) score threshold (default: 0.75)\n\n\n\n\nerror_rt_max\n\n\nNumeric maximum retention time error in minutes (default: 0.1)\n\n\n\n\n\n\nData frame containing only high-confidence annotations that meet at least one score threshold and pass RT error filtering\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/copy_backbone.html",
    "href": "man/copy_backbone.html",
    "title": "tima",
    "section": "",
    "text": "This function copies the package backbone (default directory structure, configuration files, and parameters) to a cache directory. This sets up the working environment for TIMA workflows.\n\n\n\ncopy_backbone(cache_dir = fs::path_home(\".tima\"), package = \"tima\")\n\n\n\n\n\n\n\ncache_dir\n\n\nCharacter string path to the cache directory (default: \"~/.tima\" in user’s home directory)\n\n\n\n\npackage\n\n\nCharacter string name of the package (default: \"tima\")\n\n\n\n\n\n\nNULL (invisibly). Creates cache directory structure as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Copy to default cache location\ncopy_backbone()\n\n# Copy to custom location\ncopy_backbone(cache_dir = \"~/my_tima_cache\")"
  },
  {
    "objectID": "man/copy_backbone.html#copy-backbone",
    "href": "man/copy_backbone.html#copy-backbone",
    "title": "tima",
    "section": "",
    "text": "This function copies the package backbone (default directory structure, configuration files, and parameters) to a cache directory. This sets up the working environment for TIMA workflows.\n\n\n\ncopy_backbone(cache_dir = fs::path_home(\".tima\"), package = \"tima\")\n\n\n\n\n\n\n\ncache_dir\n\n\nCharacter string path to the cache directory (default: \"~/.tima\" in user’s home directory)\n\n\n\n\npackage\n\n\nCharacter string name of the package (default: \"tima\")\n\n\n\n\n\n\nNULL (invisibly). Creates cache directory structure as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Copy to default cache location\ncopy_backbone()\n\n# Copy to custom location\ncopy_backbone(cache_dir = \"~/my_tima_cache\")"
  },
  {
    "objectID": "man/prepare_annotations_gnps.html",
    "href": "man/prepare_annotations_gnps.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares GNPS spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows.\n\n\n\nprepare_annotations_gnps(\n  input = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$annotations\\$raw\\$spectral\\$gnps,\n  output = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$annotations\\$prepared\\$structural\\$gnps,\n  str_stereo = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string or vector of paths to GNPS annotation files\n\n\n\n\noutput\n\n\nCharacter string path for prepared GNPS annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared GNPS annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_gnps()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_gnps"
    ]
  },
  {
    "objectID": "man/prepare_annotations_gnps.html#prepare-annotations-gnps",
    "href": "man/prepare_annotations_gnps.html#prepare-annotations-gnps",
    "title": "tima",
    "section": "",
    "text": "This function prepares GNPS spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows.\n\n\n\nprepare_annotations_gnps(\n  input = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$annotations\\$raw\\$spectral\\$gnps,\n  output = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$annotations\\$prepared\\$structural\\$gnps,\n  str_stereo = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string or vector of paths to GNPS annotation files\n\n\n\n\noutput\n\n\nCharacter string path for prepared GNPS annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared GNPS annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_gnps()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_gnps"
    ]
  },
  {
    "objectID": "man/export_output.html",
    "href": "man/export_output.html",
    "title": "tima",
    "section": "",
    "text": "This function creates the output directory if it doesn’t exist and exports a data frame to a tab-delimited file.\n\n\n\nexport_output(x, file)\n\n\n\n\n\n\n\nx\n\n\nData frame to be exported\n\n\n\n\nfile\n\n\nCharacter string path to the output file\n\n\n\n\n\n\nThe path to the exported file\n\n\n\n\nlibrary(\"tima\")\n\nexport_output(x = data.frame(), file = \"output/file.tsv\")\nunlink(\"output\", recursive = TRUE)"
  },
  {
    "objectID": "man/export_output.html#export-output",
    "href": "man/export_output.html#export-output",
    "title": "tima",
    "section": "",
    "text": "This function creates the output directory if it doesn’t exist and exports a data frame to a tab-delimited file.\n\n\n\nexport_output(x, file)\n\n\n\n\n\n\n\nx\n\n\nData frame to be exported\n\n\n\n\nfile\n\n\nCharacter string path to the output file\n\n\n\n\n\n\nThe path to the exported file\n\n\n\n\nlibrary(\"tima\")\n\nexport_output(x = data.frame(), file = \"output/file.tsv\")\nunlink(\"output\", recursive = TRUE)"
  },
  {
    "objectID": "man/go_to_cache.html",
    "href": "man/go_to_cache.html",
    "title": "tima",
    "section": "",
    "text": "This function creates a cache directory in the user’s home directory (if it doesn’t exist) and changes the working directory to it. Useful for storing temporary files and intermediate results.\n\n\n\ngo_to_cache(dir = \".tima\")\n\n\n\n\n\n\n\ndir\n\n\nCharacter string name of the cache directory (default: \".tima\"). Will be created in the user’s home directory.\n\n\n\n\n\n\nNULL (invisibly). Changes working directory as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Create and navigate to cache directory\ngo_to_cache()\n\n# Use custom cache directory name\ngo_to_cache(dir = \".my_cache\")"
  },
  {
    "objectID": "man/go_to_cache.html#go-to-cache",
    "href": "man/go_to_cache.html#go-to-cache",
    "title": "tima",
    "section": "",
    "text": "This function creates a cache directory in the user’s home directory (if it doesn’t exist) and changes the working directory to it. Useful for storing temporary files and intermediate results.\n\n\n\ngo_to_cache(dir = \".tima\")\n\n\n\n\n\n\n\ndir\n\n\nCharacter string name of the cache directory (default: \".tima\"). Will be created in the user’s home directory.\n\n\n\n\n\n\nNULL (invisibly). Changes working directory as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Create and navigate to cache directory\ngo_to_cache()\n\n# Use custom cache directory name\ngo_to_cache(dir = \".my_cache\")"
  },
  {
    "objectID": "man/get_default_paths.html",
    "href": "man/get_default_paths.html",
    "title": "tima",
    "section": "",
    "text": "This function loads and parses the default paths configuration from a YAML file. These paths define locations for data files, libraries, parameters, and other resources used throughout the TIMA workflow.\n\n\n\nget_default_paths(yaml = system.file(\"paths.yaml\", package = \"tima\"))\n\n\n\n\n\n\n\nyaml\n\n\nCharacter string path to the YAML file containing path definitions. Default is the paths.yaml file included in the package installation.\n\n\n\n\n\n\nNamed list containing all configured paths and settings. The structure mirrors the YAML file hierarchy with nested lists for organized access.\n\n\n\n\nlibrary(\"tima\")\n\n# Get default paths\npaths &lt;- get_default_paths()\n\n# Access specific paths\ndata_path &lt;- paths$data$source$path"
  },
  {
    "objectID": "man/get_default_paths.html#get-default-paths",
    "href": "man/get_default_paths.html#get-default-paths",
    "title": "tima",
    "section": "",
    "text": "This function loads and parses the default paths configuration from a YAML file. These paths define locations for data files, libraries, parameters, and other resources used throughout the TIMA workflow.\n\n\n\nget_default_paths(yaml = system.file(\"paths.yaml\", package = \"tima\"))\n\n\n\n\n\n\n\nyaml\n\n\nCharacter string path to the YAML file containing path definitions. Default is the paths.yaml file included in the package installation.\n\n\n\n\n\n\nNamed list containing all configured paths and settings. The structure mirrors the YAML file hierarchy with nested lists for organized access.\n\n\n\n\nlibrary(\"tima\")\n\n# Get default paths\npaths &lt;- get_default_paths()\n\n# Access specific paths\ndata_path &lt;- paths$data$source$path"
  },
  {
    "objectID": "man/calculate_mass_of_m.html",
    "href": "man/calculate_mass_of_m.html",
    "title": "tima",
    "section": "",
    "text": "This function calculates the neutral mass (M) from an observed m/z value and adduct notation. It accounts for charge, multimers, isotopes, and adduct modifications.\n\n\n\ncalculate_mass_of_m(adduct_string, mz, electron_mass = 0.0005485799)\n\n\n\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct (e.g., [M+H]+, [2M+Na]+)\n\n\n\n\nmz\n\n\nNumeric observed m/z value\n\n\n\n\nelectron_mass\n\n\nNumeric electron mass in Daltons (default: 5.485799E-4, CODATA 2018 value)\n\n\n\n\n\n\nNumeric neutral mass (M) in Daltons. Returns 0 if adduct parsing fails.\n\n\n\n\nlibrary(\"tima\")\n\ncalculate_mass_of_m(mz = 123.4567, adduct_string = \"[M+H]+\")\n\n[1] 122.4483\n\ncalculate_mass_of_m(mz = 123.4567, adduct_string = \"[M+Na]+\")\n\n[1] 100.4664\n\ncalculate_mass_of_m(mz = 123.4567, adduct_string = \"[2M1-C6H12O6 (hexose)+NaCl+H]2+\")\n\n[1] 185.0046",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_mass_of_m"
    ]
  },
  {
    "objectID": "man/calculate_mass_of_m.html#calculate-mass-of-m",
    "href": "man/calculate_mass_of_m.html#calculate-mass-of-m",
    "title": "tima",
    "section": "",
    "text": "This function calculates the neutral mass (M) from an observed m/z value and adduct notation. It accounts for charge, multimers, isotopes, and adduct modifications.\n\n\n\ncalculate_mass_of_m(adduct_string, mz, electron_mass = 0.0005485799)\n\n\n\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct (e.g., [M+H]+, [2M+Na]+)\n\n\n\n\nmz\n\n\nNumeric observed m/z value\n\n\n\n\nelectron_mass\n\n\nNumeric electron mass in Daltons (default: 5.485799E-4, CODATA 2018 value)\n\n\n\n\n\n\nNumeric neutral mass (M) in Daltons. Returns 0 if adduct parsing fails.\n\n\n\n\nlibrary(\"tima\")\n\ncalculate_mass_of_m(mz = 123.4567, adduct_string = \"[M+H]+\")\n\n[1] 122.4483\n\ncalculate_mass_of_m(mz = 123.4567, adduct_string = \"[M+Na]+\")\n\n[1] 100.4664\n\ncalculate_mass_of_m(mz = 123.4567, adduct_string = \"[2M1-C6H12O6 (hexose)+NaCl+H]2+\")\n\n[1] 185.0046",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_mass_of_m"
    ]
  },
  {
    "objectID": "man/prepare_libraries_spectra.html",
    "href": "man/prepare_libraries_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares spectra to be used for spectral matching\n\n\n\nprepare_libraries_spectra(\n  input = get_params(step = \"prepare_libraries_spectra\")\\$files\\$libraries\\$spectral\\$raw,\n  nam_lib = get_params(step = \"prepare_libraries_spectra\")\\$names\\$libraries,\n  col_ad = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$adduct,\n  col_ce = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$collision_energy,\n  col_ci = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$compound_id,\n  col_em = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$exact_mass,\n  col_in = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchi,\n  col_io = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchi_no_stereo,\n  col_ik = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchikey,\n  col_il = get_params(step =\n    \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchikey_connectivity_layer,\n  col_mf = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$molecular_formula,\n  col_na = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$name,\n  col_po = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$polarity,\n  col_sm = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$smiles,\n  col_sn = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$smiles_no_stereo,\n  col_si = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$spectrum_id,\n  col_sp = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$splash,\n  col_sy = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$synonyms,\n  col_xl = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$xlogp\n)\n\n\n\n\n\n\n\ninput\n\n\nFile containing spectra\n\n\n\n\nnam_lib\n\n\nMetadata to identify the library\n\n\n\n\ncol_ad\n\n\nName of the adduct in mgf\n\n\n\n\ncol_ce\n\n\nName of the collision energy in mgf\n\n\n\n\ncol_ci\n\n\nName of the compound id in mgf\n\n\n\n\ncol_em\n\n\nName of the exact mass in mgf\n\n\n\n\ncol_in\n\n\nName of the InChI in mgf\n\n\n\n\ncol_io\n\n\nName of the InChI without stereo in mgf\n\n\n\n\ncol_ik\n\n\nName of the InChIKey in mgf\n\n\n\n\ncol_il\n\n\nName of the InChIKey without stereo in mgf\n\n\n\n\ncol_mf\n\n\nName of the molecular formula in mgf\n\n\n\n\ncol_na\n\n\nName of the name in mgf\n\n\n\n\ncol_po\n\n\nName of the polarity in mgf\n\n\n\n\ncol_sm\n\n\nName of the SMILES in mgf\n\n\n\n\ncol_sn\n\n\nName of the SMILES without stereo in mgf\n\n\n\n\ncol_si\n\n\nName of the spectrum id in mgf\n\n\n\n\ncol_sp\n\n\nName of the SPLASH in mgf\n\n\n\n\ncol_sy\n\n\nName of the synonyms in mgf\n\n\n\n\ncol_xl\n\n\nName of the xlogp in mgf\n\n\n\n\npolarity\n\n\nMS polarity\n\n\n\n\n\n\nThe path to the prepared spectral library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_spectra()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "prepare_libraries_spectra"
    ]
  },
  {
    "objectID": "man/prepare_libraries_spectra.html#prepare-libraries-of-spectra",
    "href": "man/prepare_libraries_spectra.html#prepare-libraries-of-spectra",
    "title": "tima",
    "section": "",
    "text": "This function prepares spectra to be used for spectral matching\n\n\n\nprepare_libraries_spectra(\n  input = get_params(step = \"prepare_libraries_spectra\")\\$files\\$libraries\\$spectral\\$raw,\n  nam_lib = get_params(step = \"prepare_libraries_spectra\")\\$names\\$libraries,\n  col_ad = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$adduct,\n  col_ce = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$collision_energy,\n  col_ci = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$compound_id,\n  col_em = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$exact_mass,\n  col_in = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchi,\n  col_io = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchi_no_stereo,\n  col_ik = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchikey,\n  col_il = get_params(step =\n    \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchikey_connectivity_layer,\n  col_mf = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$molecular_formula,\n  col_na = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$name,\n  col_po = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$polarity,\n  col_sm = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$smiles,\n  col_sn = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$smiles_no_stereo,\n  col_si = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$spectrum_id,\n  col_sp = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$splash,\n  col_sy = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$synonyms,\n  col_xl = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$xlogp\n)\n\n\n\n\n\n\n\ninput\n\n\nFile containing spectra\n\n\n\n\nnam_lib\n\n\nMetadata to identify the library\n\n\n\n\ncol_ad\n\n\nName of the adduct in mgf\n\n\n\n\ncol_ce\n\n\nName of the collision energy in mgf\n\n\n\n\ncol_ci\n\n\nName of the compound id in mgf\n\n\n\n\ncol_em\n\n\nName of the exact mass in mgf\n\n\n\n\ncol_in\n\n\nName of the InChI in mgf\n\n\n\n\ncol_io\n\n\nName of the InChI without stereo in mgf\n\n\n\n\ncol_ik\n\n\nName of the InChIKey in mgf\n\n\n\n\ncol_il\n\n\nName of the InChIKey without stereo in mgf\n\n\n\n\ncol_mf\n\n\nName of the molecular formula in mgf\n\n\n\n\ncol_na\n\n\nName of the name in mgf\n\n\n\n\ncol_po\n\n\nName of the polarity in mgf\n\n\n\n\ncol_sm\n\n\nName of the SMILES in mgf\n\n\n\n\ncol_sn\n\n\nName of the SMILES without stereo in mgf\n\n\n\n\ncol_si\n\n\nName of the spectrum id in mgf\n\n\n\n\ncol_sp\n\n\nName of the SPLASH in mgf\n\n\n\n\ncol_sy\n\n\nName of the synonyms in mgf\n\n\n\n\ncol_xl\n\n\nName of the xlogp in mgf\n\n\n\n\npolarity\n\n\nMS polarity\n\n\n\n\n\n\nThe path to the prepared spectral library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_spectra()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "prepare_libraries_spectra"
    ]
  },
  {
    "objectID": "man/clean_chemo.html",
    "href": "man/clean_chemo.html",
    "title": "tima",
    "section": "",
    "text": "This function cleans chemically weighted annotation results by filtering to top candidates, optionally removing MS1-only annotations below score thresholds, and preparing final results for export. Can include compound names and apply high-confidence filters.\n\n\n\nclean_chemo(\n  annot_table_wei_chemo = get(\"annot_table_wei_chemo\", envir = parent.frame()),\n  components_table = get(\"components_table\", envir = parent.frame()),\n  features_table = get(\"features_table\", envir = parent.frame()),\n  structure_organism_pairs_table = get(\"structure_organism_pairs_table\", envir =\n    parent.frame()),\n  candidates_final = get(\"candidates_final\", envir = parent.frame()),\n  best_percentile = get(\"best_percentile\", envir = parent.frame()),\n  minimal_ms1_bio = get(\"minimal_ms1_bio\", envir = parent.frame()),\n  minimal_ms1_chemo = get(\"minimal_ms1_chemo\", envir = parent.frame()),\n  minimal_ms1_condition = get(\"minimal_ms1_condition\", envir = parent.frame()),\n  compounds_names = get(\"compounds_names\", envir = parent.frame()),\n  high_confidence = get(\"high_confidence\", envir = parent.frame()),\n  remove_ties = get(\"remove_ties\", envir = parent.frame()),\n  summarize = get(\"summarize\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_chemo\n\n\nData frame with chemically weighted annotations\n\n\n\n\ncomponents_table\n\n\nData frame with molecular network components\n\n\n\n\nfeatures_table\n\n\nData frame with feature metadata\n\n\n\n\nstructure_organism_pairs_table\n\n\nData frame with structure-organism pairs\n\n\n\n\ncandidates_final\n\n\nInteger number of final candidates to keep per feature\n\n\n\n\nbest_percentile\n\n\nNumeric percentile threshold (0-1) for selecting top candidates within each feature (default: 0.9, keeps candidates with scores &gt;= 90% of the maximum score for that feature). Used for both filtered and mini outputs to ensure consistent row counts.\n\n\n\n\nminimal_ms1_bio\n\n\nNumeric minimal biological score for MS1 annotations (0-1)\n\n\n\n\nminimal_ms1_chemo\n\n\nNumeric minimal chemical score for MS1 annotations (0-1)\n\n\n\n\nminimal_ms1_condition\n\n\nCharacter condition: \"OR\" or \"AND\" for MS1 filtering\n\n\n\n\ncompounds_names\n\n\nLogical whether to include compound names (can be large)\n\n\n\n\nhigh_confidence\n\n\nLogical whether to filter for high confidence only\n\n\n\n\nremove_ties\n\n\nLogical whether to remove tied scores\n\n\n\n\nsummarize\n\n\nLogical whether to summarize to 1 row per feature\n\n\n\n\n\n\nData frame with cleaned, filtered chemically weighted annotations\n\n\n\nweight_chemo\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/clean_chemo.html#clean-chemo",
    "href": "man/clean_chemo.html#clean-chemo",
    "title": "tima",
    "section": "",
    "text": "This function cleans chemically weighted annotation results by filtering to top candidates, optionally removing MS1-only annotations below score thresholds, and preparing final results for export. Can include compound names and apply high-confidence filters.\n\n\n\nclean_chemo(\n  annot_table_wei_chemo = get(\"annot_table_wei_chemo\", envir = parent.frame()),\n  components_table = get(\"components_table\", envir = parent.frame()),\n  features_table = get(\"features_table\", envir = parent.frame()),\n  structure_organism_pairs_table = get(\"structure_organism_pairs_table\", envir =\n    parent.frame()),\n  candidates_final = get(\"candidates_final\", envir = parent.frame()),\n  best_percentile = get(\"best_percentile\", envir = parent.frame()),\n  minimal_ms1_bio = get(\"minimal_ms1_bio\", envir = parent.frame()),\n  minimal_ms1_chemo = get(\"minimal_ms1_chemo\", envir = parent.frame()),\n  minimal_ms1_condition = get(\"minimal_ms1_condition\", envir = parent.frame()),\n  compounds_names = get(\"compounds_names\", envir = parent.frame()),\n  high_confidence = get(\"high_confidence\", envir = parent.frame()),\n  remove_ties = get(\"remove_ties\", envir = parent.frame()),\n  summarize = get(\"summarize\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_chemo\n\n\nData frame with chemically weighted annotations\n\n\n\n\ncomponents_table\n\n\nData frame with molecular network components\n\n\n\n\nfeatures_table\n\n\nData frame with feature metadata\n\n\n\n\nstructure_organism_pairs_table\n\n\nData frame with structure-organism pairs\n\n\n\n\ncandidates_final\n\n\nInteger number of final candidates to keep per feature\n\n\n\n\nbest_percentile\n\n\nNumeric percentile threshold (0-1) for selecting top candidates within each feature (default: 0.9, keeps candidates with scores &gt;= 90% of the maximum score for that feature). Used for both filtered and mini outputs to ensure consistent row counts.\n\n\n\n\nminimal_ms1_bio\n\n\nNumeric minimal biological score for MS1 annotations (0-1)\n\n\n\n\nminimal_ms1_chemo\n\n\nNumeric minimal chemical score for MS1 annotations (0-1)\n\n\n\n\nminimal_ms1_condition\n\n\nCharacter condition: \"OR\" or \"AND\" for MS1 filtering\n\n\n\n\ncompounds_names\n\n\nLogical whether to include compound names (can be large)\n\n\n\n\nhigh_confidence\n\n\nLogical whether to filter for high confidence only\n\n\n\n\nremove_ties\n\n\nLogical whether to remove tied scores\n\n\n\n\nsummarize\n\n\nLogical whether to summarize to 1 row per feature\n\n\n\n\n\n\nData frame with cleaned, filtered chemically weighted annotations\n\n\n\nweight_chemo\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/sanitize_spectra.html",
    "href": "man/sanitize_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function sanitizes MS spectra by removing noise, artifacts, precursor peaks, and empty spectra. It applies multiple cleaning steps including intensity filtering, peak reduction, and normalization.\n\n\n\nsanitize_spectra(spectra, cutoff = 0, dalton = 0.01, ppm = 10)\n\n\n\n\n\n\n\nspectra\n\n\nSpectra object from the Spectra package\n\n\n\n\ncutoff\n\n\nNumeric absolute minimal intensity threshold (default: 0). Peaks below this intensity are removed.\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching (default: 0.01)\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching (default: 10)\n\n\n\n\n\n\nA sanitized Spectra object with noise and artifacts removed\n\n\n\n\nlibrary(\"tima\")\n\ndata.frame(\n  FEATURE_ID = c(\"FT001\", \"FT002\", \"FT003\"),\n  mz = c(list(123.4567, 234.5678, 345.6789))\n) |&gt;\n  Spectra::Spectra() |&gt;\n  sanitize_spectra()",
    "crumbs": [
      "Get started",
      "Functions",
      "sanitize_spectra"
    ]
  },
  {
    "objectID": "man/sanitize_spectra.html#sanitize-spectra",
    "href": "man/sanitize_spectra.html#sanitize-spectra",
    "title": "tima",
    "section": "",
    "text": "This function sanitizes MS spectra by removing noise, artifacts, precursor peaks, and empty spectra. It applies multiple cleaning steps including intensity filtering, peak reduction, and normalization.\n\n\n\nsanitize_spectra(spectra, cutoff = 0, dalton = 0.01, ppm = 10)\n\n\n\n\n\n\n\nspectra\n\n\nSpectra object from the Spectra package\n\n\n\n\ncutoff\n\n\nNumeric absolute minimal intensity threshold (default: 0). Peaks below this intensity are removed.\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching (default: 0.01)\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching (default: 10)\n\n\n\n\n\n\nA sanitized Spectra object with noise and artifacts removed\n\n\n\n\nlibrary(\"tima\")\n\ndata.frame(\n  FEATURE_ID = c(\"FT001\", \"FT002\", \"FT003\"),\n  mz = c(list(123.4567, 234.5678, 345.6789))\n) |&gt;\n  Spectra::Spectra() |&gt;\n  sanitize_spectra()",
    "crumbs": [
      "Get started",
      "Functions",
      "sanitize_spectra"
    ]
  },
  {
    "objectID": "man/dist_get.html",
    "href": "man/dist_get.html",
    "title": "tima",
    "section": "",
    "text": "This function calculates the distance between two elements in a distance matrix by their indices. Returns 0 for identical elements.\n\n\n\ndist_get(d, idx1, idx2)\n\n\n\n\n\n\n\nd\n\n\nDistance matrix or dist object\n\n\n\n\nidx1\n\n\nInteger index of the first element\n\n\n\n\nidx2\n\n\nInteger index of the second element\n\n\n\n\n\n\nCredit goes to usedist package for the algorithm\n\n\n\nNumeric distance between the two elements. Returns 0 if indices are identical, NA if indices are invalid.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/dist_get.html#distance-between-two-elements-in-a-distance-matrix",
    "href": "man/dist_get.html#distance-between-two-elements-in-a-distance-matrix",
    "title": "tima",
    "section": "",
    "text": "This function calculates the distance between two elements in a distance matrix by their indices. Returns 0 for identical elements.\n\n\n\ndist_get(d, idx1, idx2)\n\n\n\n\n\n\n\nd\n\n\nDistance matrix or dist object\n\n\n\n\nidx1\n\n\nInteger index of the first element\n\n\n\n\nidx2\n\n\nInteger index of the second element\n\n\n\n\n\n\nCredit goes to usedist package for the algorithm\n\n\n\nNumeric distance between the two elements. Returns 0 if indices are identical, NA if indices are invalid.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/prepare_libraries_sop_closed.html",
    "href": "man/prepare_libraries_sop_closed.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares closed (private/restricted) structure- organism pair libraries by formatting columns, rounding values, and standardizing structure. Falls back to an empty template if the closed resource is not accessible.\n\n\n\nprepare_libraries_sop_closed(\n  input = get_params(step =\n    \"prepare_libraries_sop_closed\")\\$files\\$libraries\\$sop\\$raw\\$closed,\n  output = get_params(step =\n    \"prepare_libraries_sop_closed\")\\$files\\$libraries\\$sop\\$prepared\\$closed\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to input closed library file\n\n\n\n\noutput\n\n\nCharacter string path where prepared library should be saved\n\n\n\n\n\n\nCharacter string path to the prepared structure-organism pairs library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_closed()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_closed"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_closed.html#prepare-libraries-of-structure-organism-pairs-closed",
    "href": "man/prepare_libraries_sop_closed.html#prepare-libraries-of-structure-organism-pairs-closed",
    "title": "tima",
    "section": "",
    "text": "This function prepares closed (private/restricted) structure- organism pair libraries by formatting columns, rounding values, and standardizing structure. Falls back to an empty template if the closed resource is not accessible.\n\n\n\nprepare_libraries_sop_closed(\n  input = get_params(step =\n    \"prepare_libraries_sop_closed\")\\$files\\$libraries\\$sop\\$raw\\$closed,\n  output = get_params(step =\n    \"prepare_libraries_sop_closed\")\\$files\\$libraries\\$sop\\$prepared\\$closed\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to input closed library file\n\n\n\n\noutput\n\n\nCharacter string path where prepared library should be saved\n\n\n\n\n\n\nCharacter string path to the prepared structure-organism pairs library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_closed()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_closed"
    ]
  },
  {
    "objectID": "man/select_sop_columns.html",
    "href": "man/select_sop_columns.html",
    "title": "tima",
    "section": "",
    "text": "This function selects and renames structure-organism pair (SOP) columns to a standardized format, including chemical structures, taxonomic classifications, and organism taxonomy\n\n\n\nselect_sop_columns(df)\n\n\n\n\n\n\n\ndf\n\n\nDataframe containing structure-organism pair data\n\n\n\n\n\n\nThe dataframe with selected and renamed SOP columns in standardized format\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/select_sop_columns.html#select-sop-columns",
    "href": "man/select_sop_columns.html#select-sop-columns",
    "title": "tima",
    "section": "",
    "text": "This function selects and renames structure-organism pair (SOP) columns to a standardized format, including chemical structures, taxonomic classifications, and organism taxonomy\n\n\n\nselect_sop_columns(df)\n\n\n\n\n\n\n\ndf\n\n\nDataframe containing structure-organism pair data\n\n\n\n\n\n\nThe dataframe with selected and renamed SOP columns in standardized format\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/select_sirius_columns_formulas.html",
    "href": "man/select_sirius_columns_formulas.html",
    "title": "tima",
    "section": "",
    "text": "This function selects sirius columns (formulas)\n\n\n\nselect_sirius_columns_formulas(df, sirius_version)\n\n\n\n\n\n\n\ndf\n\n\nDataframe\n\n\n\n\nsirius_version\n\n\nSirius version\n\n\n\n\n\n\nThe dataframe with selected sirius columns\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/select_sirius_columns_formulas.html#select-sirius-columns-formulas",
    "href": "man/select_sirius_columns_formulas.html#select-sirius-columns-formulas",
    "title": "tima",
    "section": "",
    "text": "This function selects sirius columns (formulas)\n\n\n\nselect_sirius_columns_formulas(df, sirius_version)\n\n\n\n\n\n\n\ndf\n\n\nDataframe\n\n\n\n\nsirius_version\n\n\nSirius version\n\n\n\n\n\n\nThe dataframe with selected sirius columns\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/create_dir.html",
    "href": "man/create_dir.html",
    "title": "tima",
    "section": "",
    "text": "This function creates a directory at the specified path if it does not already exist. Handles both directory paths and file paths (extracting the directory component).\n\n\n\ncreate_dir(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path to the directory or file path from which to extract and create the directory\n\n\n\n\n\n\nNULL (invisibly). Creates directory as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\ncreate_dir(export = \"path/to/directory\")\ncreate_dir(export = \"path/to/file.txt\")"
  },
  {
    "objectID": "man/create_dir.html#create-directory",
    "href": "man/create_dir.html#create-directory",
    "title": "tima",
    "section": "",
    "text": "This function creates a directory at the specified path if it does not already exist. Handles both directory paths and file paths (extracting the directory component).\n\n\n\ncreate_dir(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path to the directory or file path from which to extract and create the directory\n\n\n\n\n\n\nNULL (invisibly). Creates directory as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\ncreate_dir(export = \"path/to/directory\")\ncreate_dir(export = \"path/to/file.txt\")"
  },
  {
    "objectID": "man/prepare_taxa.html",
    "href": "man/prepare_taxa.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares taxonomic information for features by matching organism names to Open Tree of Life taxonomy. Can attribute all features to a single organism or distribute them across multiple organisms based on relative intensities in samples.\n\n\n\nprepare_taxa(\n  input = get_params(step = \"prepare_taxa\")\\$files\\$features\\$prepared,\n  extension = get_params(step = \"prepare_taxa\")\\$names\\$extension,\n  name_filename = get_params(step = \"prepare_taxa\")\\$names\\$filename,\n  colname = get_params(step = \"prepare_taxa\")\\$names\\$taxon,\n  metadata = get_params(step = \"prepare_taxa\")\\$files\\$metadata\\$raw,\n  org_tax_ott = get_params(step =\n    \"prepare_taxa\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  output = get_params(step = \"prepare_taxa\")\\$files\\$metadata\\$prepared,\n  taxon = get_params(step = \"prepare_taxa\")\\$organisms\\$taxon\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to features file with intensities\n\n\n\n\nextension\n\n\nLogical whether column names contain file extensions\n\n\n\n\nname_filename\n\n\nCharacter string name of filename column in metadata\n\n\n\n\ncolname\n\n\nCharacter string name of column with biological source info\n\n\n\n\nmetadata\n\n\nCharacter string path to metadata file with organism info\n\n\n\n\norg_tax_ott\n\n\nCharacter string path to Open Tree of Life taxonomy file\n\n\n\n\noutput\n\n\nCharacter string path for output file\n\n\n\n\ntaxon\n\n\nCharacter string organism name to enforce for all features (e.g., \"Homo sapiens\"). If provided, overrides metadata-based assignment.\n\n\n\n\n\n\nDepending on whether features are aligned between samples from various organisms, this function either: - Attributes all features to a single organism (if taxon specified), or - Attributes features to multiple organisms based on their relative intensities across samples (using metadata)\n\n\n\nCharacter string path to the prepared taxa file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\norg_tax_ott &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/organisms/taxonomies/ott.tsv\"\n)\nget_file(url = paste0(dir, org_tax_ott), export = org_tax_ott)\nget_file(\n  url = paste0(dir, \"data/interim/features/example_features.tsv\"),\n  export = get_params(step = \"prepare_taxa\")$files$features$prepared\n)\nprepare_taxa(\n  taxon = \"Homo sapiens\",\n  org_tax_ott = org_tax_ott\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "prepare_taxa"
    ]
  },
  {
    "objectID": "man/prepare_taxa.html#prepare-taxa",
    "href": "man/prepare_taxa.html#prepare-taxa",
    "title": "tima",
    "section": "",
    "text": "This function prepares taxonomic information for features by matching organism names to Open Tree of Life taxonomy. Can attribute all features to a single organism or distribute them across multiple organisms based on relative intensities in samples.\n\n\n\nprepare_taxa(\n  input = get_params(step = \"prepare_taxa\")\\$files\\$features\\$prepared,\n  extension = get_params(step = \"prepare_taxa\")\\$names\\$extension,\n  name_filename = get_params(step = \"prepare_taxa\")\\$names\\$filename,\n  colname = get_params(step = \"prepare_taxa\")\\$names\\$taxon,\n  metadata = get_params(step = \"prepare_taxa\")\\$files\\$metadata\\$raw,\n  org_tax_ott = get_params(step =\n    \"prepare_taxa\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  output = get_params(step = \"prepare_taxa\")\\$files\\$metadata\\$prepared,\n  taxon = get_params(step = \"prepare_taxa\")\\$organisms\\$taxon\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to features file with intensities\n\n\n\n\nextension\n\n\nLogical whether column names contain file extensions\n\n\n\n\nname_filename\n\n\nCharacter string name of filename column in metadata\n\n\n\n\ncolname\n\n\nCharacter string name of column with biological source info\n\n\n\n\nmetadata\n\n\nCharacter string path to metadata file with organism info\n\n\n\n\norg_tax_ott\n\n\nCharacter string path to Open Tree of Life taxonomy file\n\n\n\n\noutput\n\n\nCharacter string path for output file\n\n\n\n\ntaxon\n\n\nCharacter string organism name to enforce for all features (e.g., \"Homo sapiens\"). If provided, overrides metadata-based assignment.\n\n\n\n\n\n\nDepending on whether features are aligned between samples from various organisms, this function either: - Attributes all features to a single organism (if taxon specified), or - Attributes features to multiple organisms based on their relative intensities across samples (using metadata)\n\n\n\nCharacter string path to the prepared taxa file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\norg_tax_ott &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/organisms/taxonomies/ott.tsv\"\n)\nget_file(url = paste0(dir, org_tax_ott), export = org_tax_ott)\nget_file(\n  url = paste0(dir, \"data/interim/features/example_features.tsv\"),\n  export = get_params(step = \"prepare_taxa\")$files$features$prepared\n)\nprepare_taxa(\n  taxon = \"Homo sapiens\",\n  org_tax_ott = org_tax_ott\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "prepare_taxa"
    ]
  },
  {
    "objectID": "man/get_path.html",
    "href": "man/get_path.html",
    "title": "tima",
    "section": "",
    "text": "This function resolves file paths by checking multiple locations, handling the difference between installed packages (where inst/ is removed) and development environments (where inst/ exists). It tries paths in order until one is found.\n\n\n\nget_path(base_path)\n\n\n\n\n\n\n\nbase_path\n\n\nCharacter string representing the base path to resolve. Can include \"inst/\" which will be handled appropriately.\n\n\n\n\n\n\nCharacter string of the resolved absolute path that exists\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/get_path.html#get-path",
    "href": "man/get_path.html#get-path",
    "title": "tima",
    "section": "",
    "text": "This function resolves file paths by checking multiple locations, handling the difference between installed packages (where inst/ is removed) and development environments (where inst/ exists). It tries paths in order until one is found.\n\n\n\nget_path(base_path)\n\n\n\n\n\n\n\nbase_path\n\n\nCharacter string representing the base path to resolve. Can include \"inst/\" which will be handled appropriately.\n\n\n\n\n\n\nCharacter string of the resolved absolute path that exists\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/dist_groups.html",
    "href": "man/dist_groups.html",
    "title": "tima",
    "section": "",
    "text": "This function calculates pairwise distances between observations and annotates them with group membership information. Optimized for\n\n\n\ndist_groups(d, g)\n\n\n\n\n\n\n\nd\n\n\nA distance object or matrix\n\n\n\n\ng\n\n\nA grouping vector for the observations in the distance object. Must have length equal to the number of observations.\n\n\n\n\n\n\nA data frame containing distance information between pairs of\nA data frame containing distance information between pairs of observations with columns:\n\n\n\nItem1\n\n\nIndex of first observation\n\n\n\n\nItem2\n\n\nIndex of second observation\n\n\n\n\nGroup1\n\n\nGroup label of first observation\n\n\n\n\nGroup2\n\n\nGroup label of second observation\n\n\n\n\nLabel\n\n\nFactor indicating if distance is within or between groups\n\n\n\n\nDistance\n\n\nNumeric distance value rounded to 5 digits\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/dist_groups.html#dist-groups",
    "href": "man/dist_groups.html#dist-groups",
    "title": "tima",
    "section": "",
    "text": "This function calculates pairwise distances between observations and annotates them with group membership information. Optimized for\n\n\n\ndist_groups(d, g)\n\n\n\n\n\n\n\nd\n\n\nA distance object or matrix\n\n\n\n\ng\n\n\nA grouping vector for the observations in the distance object. Must have length equal to the number of observations.\n\n\n\n\n\n\nA data frame containing distance information between pairs of\nA data frame containing distance information between pairs of observations with columns:\n\n\n\nItem1\n\n\nIndex of first observation\n\n\n\n\nItem2\n\n\nIndex of second observation\n\n\n\n\nGroup1\n\n\nGroup label of first observation\n\n\n\n\nGroup2\n\n\nGroup label of second observation\n\n\n\n\nLabel\n\n\nFactor indicating if distance is within or between groups\n\n\n\n\nDistance\n\n\nNumeric distance value rounded to 5 digits\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/clean_bio.html",
    "href": "man/clean_bio.html",
    "title": "tima",
    "section": "",
    "text": "This function cleans and filters biologically weighted annotation results by calculating chemical consistency scores across network neighbors. Only features with at least 2 neighbors are evaluated.\n\n\n\nclean_bio(\n  annot_table_wei_bio = get(\"annot_table_wei_bio\", envir = parent.frame()),\n  edges_table = get(\"edges_table\", envir = parent.frame()),\n  minimal_consistency = get(\"minimal_consistency\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_bio\n\n\nData frame containing biologically weighted annotations\n\n\n\n\nedges_table\n\n\nData frame containing network edges between features\n\n\n\n\nminimal_consistency\n\n\nNumeric minimum consistency score (0-1) required to retain a classification at each taxonomic level\n\n\n\n\n\n\nData frame containing filtered biologically weighted annotations with consistency scores\n\n\n\nweight_bio\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/clean_bio.html#clean-bio",
    "href": "man/clean_bio.html#clean-bio",
    "title": "tima",
    "section": "",
    "text": "This function cleans and filters biologically weighted annotation results by calculating chemical consistency scores across network neighbors. Only features with at least 2 neighbors are evaluated.\n\n\n\nclean_bio(\n  annot_table_wei_bio = get(\"annot_table_wei_bio\", envir = parent.frame()),\n  edges_table = get(\"edges_table\", envir = parent.frame()),\n  minimal_consistency = get(\"minimal_consistency\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_bio\n\n\nData frame containing biologically weighted annotations\n\n\n\n\nedges_table\n\n\nData frame containing network edges between features\n\n\n\n\nminimal_consistency\n\n\nNumeric minimum consistency score (0-1) required to retain a classification at each taxonomic level\n\n\n\n\n\n\nData frame containing filtered biologically weighted annotations with consistency scores\n\n\n\nweight_bio\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/install.html",
    "href": "man/install.html",
    "title": "tima",
    "section": "",
    "text": "This function installs the TIMA package and its dependencies, including setting up a Python virtual environment with RDKit for chemical structure processing. It handles different operating systems and provides fallback installation methods if needed.\n\n\n\ninstall(\n  package = \"tima\",\n  repos = c(\"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\", \"https://cloud.r-project.org\"),\n  dependencies = TRUE,\n  test = FALSE\n)\n\n\n\n\n\n\n\npackage\n\n\nCharacter string name of the package to install (default: \"tima\")\n\n\n\n\nrepos\n\n\nCharacter vector of repository URLs for install.packages\n\n\n\n\ndependencies\n\n\nLogical whether to install package dependencies (default: TRUE)\n\n\n\n\ntest\n\n\nLogical whether to use fallback/test installation mode (default: FALSE)\n\n\n\n\n\n\nNULL (invisibly). Installs packages and sets up Python environment as side effects.\n\n\n\n\nlibrary(\"tima\")\n\ninstall()"
  },
  {
    "objectID": "man/install.html#install",
    "href": "man/install.html#install",
    "title": "tima",
    "section": "",
    "text": "This function installs the TIMA package and its dependencies, including setting up a Python virtual environment with RDKit for chemical structure processing. It handles different operating systems and provides fallback installation methods if needed.\n\n\n\ninstall(\n  package = \"tima\",\n  repos = c(\"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\", \"https://cloud.r-project.org\"),\n  dependencies = TRUE,\n  test = FALSE\n)\n\n\n\n\n\n\n\npackage\n\n\nCharacter string name of the package to install (default: \"tima\")\n\n\n\n\nrepos\n\n\nCharacter vector of repository URLs for install.packages\n\n\n\n\ndependencies\n\n\nLogical whether to install package dependencies (default: TRUE)\n\n\n\n\ntest\n\n\nLogical whether to use fallback/test installation mode (default: FALSE)\n\n\n\n\n\n\nNULL (invisibly). Installs packages and sets up Python environment as side effects.\n\n\n\n\nlibrary(\"tima\")\n\ninstall()"
  },
  {
    "objectID": "man/decorate_masses.html",
    "href": "man/decorate_masses.html",
    "title": "tima",
    "section": "",
    "text": "This function logs summary statistics about MS1-based annotations, reporting the number of unique structures and features annotated\n\n\n\ndecorate_masses(\n  annotation_table_ms1 = get(\"annotation_table_ms1\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannotation_table_ms1\n\n\nData frame containing MS1 annotation results with columns for feature_id and candidate_structure_inchikey_connectivity_layer\n\n\n\n\n\n\nThe input annotation table (unchanged), for use in pipelines\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/decorate_masses.html#decorate-masses",
    "href": "man/decorate_masses.html#decorate-masses",
    "title": "tima",
    "section": "",
    "text": "This function logs summary statistics about MS1-based annotations, reporting the number of unique structures and features annotated\n\n\n\ndecorate_masses(\n  annotation_table_ms1 = get(\"annotation_table_ms1\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannotation_table_ms1\n\n\nData frame containing MS1 annotation results with columns for feature_id and candidate_structure_inchikey_connectivity_layer\n\n\n\n\n\n\nThe input annotation table (unchanged), for use in pipelines\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/get_spectra_ids.html",
    "href": "man/get_spectra_ids.html",
    "title": "tima",
    "section": "",
    "text": "This function extracts spectrum identifiers from a Spectra object. Since different data sources use different field names for spectrum IDs, this function checks multiple common field names in priority order.\n\n\n\nget_spectra_ids(spectra)\n\n\n\n\n\n\n\nspectra\n\n\nSpectra object from the Spectra package\n\n\n\n\n\n\nChecks for ID fields in this order: SLAW_ID, FEATURE_ID, acquisitionNum, spectrum_id. Returns the first valid field found.\n\n\n\nCharacter vector of spectrum IDs, or NULL if no valid ID field found\n\n\n\n\nlibrary(\"tima\")\n\n# Extract IDs from Spectra object\nspec_ids &lt;- get_spectra_ids(my_spectra)"
  },
  {
    "objectID": "man/get_spectra_ids.html#get-spectra-ids",
    "href": "man/get_spectra_ids.html#get-spectra-ids",
    "title": "tima",
    "section": "",
    "text": "This function extracts spectrum identifiers from a Spectra object. Since different data sources use different field names for spectrum IDs, this function checks multiple common field names in priority order.\n\n\n\nget_spectra_ids(spectra)\n\n\n\n\n\n\n\nspectra\n\n\nSpectra object from the Spectra package\n\n\n\n\n\n\nChecks for ID fields in this order: SLAW_ID, FEATURE_ID, acquisitionNum, spectrum_id. Returns the first valid field found.\n\n\n\nCharacter vector of spectrum IDs, or NULL if no valid ID field found\n\n\n\n\nlibrary(\"tima\")\n\n# Extract IDs from Spectra object\nspec_ids &lt;- get_spectra_ids(my_spectra)"
  },
  {
    "objectID": "man/decorate_bio.html",
    "href": "man/decorate_bio.html",
    "title": "tima",
    "section": "",
    "text": "This function logs summary statistics about biologically weighted annotations, showing how many structures were reranked at each taxonomic level based on organism occurrence data. The function validates that the required columns are present and handles empty inputs gracefully.\n\n\n\ndecorate_bio(\n  annot_table_wei_bio = get(\"annot_table_wei_bio\", envir = parent.frame()),\n  score_biological_kingdom = get(\"score_biological_kingdom\", envir = parent.frame()),\n  score_biological_phylum = get(\"score_biological_phylum\", envir = parent.frame()),\n  score_biological_class = get(\"score_biological_class\", envir = parent.frame()),\n  score_biological_order = get(\"score_biological_order\", envir = parent.frame()),\n  score_biological_family = get(\"score_biological_family\", envir = parent.frame()),\n  score_biological_tribe = get(\"score_biological_tribe\", envir = parent.frame()),\n  score_biological_genus = get(\"score_biological_genus\", envir = parent.frame()),\n  score_biological_species = get(\"score_biological_species\", envir = parent.frame()),\n  score_biological_variety = get(\"score_biological_variety\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_bio\n\n\nData frame containing biologically weighted annotations\n\n\n\n\nscore_biological_kingdom\n\n\nNumeric minimum score for kingdom-level matches\n\n\n\n\nscore_biological_phylum\n\n\nNumeric minimum score for phylum-level matches\n\n\n\n\nscore_biological_class\n\n\nNumeric minimum score for class-level matches\n\n\n\n\nscore_biological_order\n\n\nNumeric minimum score for order-level matches\n\n\n\n\nscore_biological_family\n\n\nNumeric minimum score for family-level matches\n\n\n\n\nscore_biological_tribe\n\n\nNumeric minimum score for tribe-level matches\n\n\n\n\nscore_biological_genus\n\n\nNumeric minimum score for genus-level matches\n\n\n\n\nscore_biological_species\n\n\nNumeric minimum score for species-level matches\n\n\n\n\nscore_biological_variety\n\n\nNumeric minimum score for variety-level matches\n\n\n\n\n\n\nThe input annotation table (unchanged), for use in pipelines\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/decorate_bio.html#decorate-bio",
    "href": "man/decorate_bio.html#decorate-bio",
    "title": "tima",
    "section": "",
    "text": "This function logs summary statistics about biologically weighted annotations, showing how many structures were reranked at each taxonomic level based on organism occurrence data. The function validates that the required columns are present and handles empty inputs gracefully.\n\n\n\ndecorate_bio(\n  annot_table_wei_bio = get(\"annot_table_wei_bio\", envir = parent.frame()),\n  score_biological_kingdom = get(\"score_biological_kingdom\", envir = parent.frame()),\n  score_biological_phylum = get(\"score_biological_phylum\", envir = parent.frame()),\n  score_biological_class = get(\"score_biological_class\", envir = parent.frame()),\n  score_biological_order = get(\"score_biological_order\", envir = parent.frame()),\n  score_biological_family = get(\"score_biological_family\", envir = parent.frame()),\n  score_biological_tribe = get(\"score_biological_tribe\", envir = parent.frame()),\n  score_biological_genus = get(\"score_biological_genus\", envir = parent.frame()),\n  score_biological_species = get(\"score_biological_species\", envir = parent.frame()),\n  score_biological_variety = get(\"score_biological_variety\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_bio\n\n\nData frame containing biologically weighted annotations\n\n\n\n\nscore_biological_kingdom\n\n\nNumeric minimum score for kingdom-level matches\n\n\n\n\nscore_biological_phylum\n\n\nNumeric minimum score for phylum-level matches\n\n\n\n\nscore_biological_class\n\n\nNumeric minimum score for class-level matches\n\n\n\n\nscore_biological_order\n\n\nNumeric minimum score for order-level matches\n\n\n\n\nscore_biological_family\n\n\nNumeric minimum score for family-level matches\n\n\n\n\nscore_biological_tribe\n\n\nNumeric minimum score for tribe-level matches\n\n\n\n\nscore_biological_genus\n\n\nNumeric minimum score for genus-level matches\n\n\n\n\nscore_biological_species\n\n\nNumeric minimum score for species-level matches\n\n\n\n\nscore_biological_variety\n\n\nNumeric minimum score for variety-level matches\n\n\n\n\n\n\nThe input annotation table (unchanged), for use in pipelines\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/get_last_version_from_zenodo.html",
    "href": "man/get_last_version_from_zenodo.html",
    "title": "tima",
    "section": "",
    "text": "This function retrieves the latest version of a file from a Zenodo repository record. It checks the file size and only downloads if the local file is missing or differs from the remote version.\n\n\n\nget_last_version_from_zenodo(doi, pattern, path)\n\n\n\n\n\n\n\ndoi\n\n\nCharacter string DOI of the Zenodo record (e.g., \"10.5281/zenodo.5794106\")\n\n\n\n\npattern\n\n\nCharacter string pattern to identify the specific file to download\n\n\n\n\npath\n\n\nCharacter string local path where the file should be saved\n\n\n\n\n\n\nCredit goes partially to https://inbo.github.io/inborutils/ This function handles the new Zenodo API format and file structure.\n\n\n\nCharacter string path to the downloaded file\n\n\n\n\nlibrary(\"tima\")\n\nget_last_version_from_zenodo(\n  doi = \"10.5281/zenodo.5794106\",\n  pattern = \"frozen.csv.gz\",\n  path = \"data/frozen.csv.gz\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_last_version_from_zenodo"
    ]
  },
  {
    "objectID": "man/get_last_version_from_zenodo.html#get-last-version-from-zenodo",
    "href": "man/get_last_version_from_zenodo.html#get-last-version-from-zenodo",
    "title": "tima",
    "section": "",
    "text": "This function retrieves the latest version of a file from a Zenodo repository record. It checks the file size and only downloads if the local file is missing or differs from the remote version.\n\n\n\nget_last_version_from_zenodo(doi, pattern, path)\n\n\n\n\n\n\n\ndoi\n\n\nCharacter string DOI of the Zenodo record (e.g., \"10.5281/zenodo.5794106\")\n\n\n\n\npattern\n\n\nCharacter string pattern to identify the specific file to download\n\n\n\n\npath\n\n\nCharacter string local path where the file should be saved\n\n\n\n\n\n\nCredit goes partially to https://inbo.github.io/inborutils/ This function handles the new Zenodo API format and file structure.\n\n\n\nCharacter string path to the downloaded file\n\n\n\n\nlibrary(\"tima\")\n\nget_last_version_from_zenodo(\n  doi = \"10.5281/zenodo.5794106\",\n  pattern = \"frozen.csv.gz\",\n  path = \"data/frozen.csv.gz\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_last_version_from_zenodo"
    ]
  },
  {
    "objectID": "man/read_mgf_opti.html",
    "href": "man/read_mgf_opti.html",
    "title": "tima",
    "section": "",
    "text": "This function reads a Mascot Generic Format (MGF) file using a memory-efficient approach. It mimics the MsBackendMgf implementation but uses significantly lower memory, making it suitable for processing large MGF files that might otherwise cause memory issues.\n\n\n\nread_mgf_opti(\n  f,\n  msLevel = 2L,\n  mapping = Spectra::spectraVariableMapping(MsBackendMgf::MsBackendMgf())\n)\n\n\n\n\n\n\n\nf\n\n\nCharacter string specifying the path to a single MGF file\n\n\n\n\nmsLevel\n\n\nInteger MS level to assign to spectra (default: 2L for MS2)\n\n\n\n\nmapping\n\n\nNamed character vector mapping MGF field names to standard spectra variable names. Default uses the mapping from MsBackendMgf.\n\n\n\n\n\n\nThe function processes spectra in batches to minimize memory usage and avoid loading the entire file into memory at once. It extracts all MGF fields and maps them to standard spectra variable names.\n\n\n\nA DataFrame containing the parsed spectra data with standardized variable names\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/read_mgf_opti.html#read-mgf-opti",
    "href": "man/read_mgf_opti.html#read-mgf-opti",
    "title": "tima",
    "section": "",
    "text": "This function reads a Mascot Generic Format (MGF) file using a memory-efficient approach. It mimics the MsBackendMgf implementation but uses significantly lower memory, making it suitable for processing large MGF files that might otherwise cause memory issues.\n\n\n\nread_mgf_opti(\n  f,\n  msLevel = 2L,\n  mapping = Spectra::spectraVariableMapping(MsBackendMgf::MsBackendMgf())\n)\n\n\n\n\n\n\n\nf\n\n\nCharacter string specifying the path to a single MGF file\n\n\n\n\nmsLevel\n\n\nInteger MS level to assign to spectra (default: 2L for MS2)\n\n\n\n\nmapping\n\n\nNamed character vector mapping MGF field names to standard spectra variable names. Default uses the mapping from MsBackendMgf.\n\n\n\n\n\n\nThe function processes spectra in batches to minimize memory usage and avoid loading the entire file into memory at once. It extracts all MGF fields and maps them to standard spectra variable names.\n\n\n\nA DataFrame containing the parsed spectra data with standardized variable names\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/read_from_sirius_zip.html",
    "href": "man/read_from_sirius_zip.html",
    "title": "tima",
    "section": "",
    "text": "This function reads specific files from a compressed SIRIUS workspace directory. It handles file selection, filtering empty files, and parsing tab-delimited data.\n\n\n\nread_from_sirius_zip(sirius_zip, file)\n\n\n\n\n\n\n\nsirius_zip\n\n\nCharacter string path to compressed SIRIUS workspace (.zip)\n\n\n\n\nfile\n\n\nCharacter string pattern to match files within the archive\n\n\n\n\n\n\nData frame (tidytable) containing the parsed file contents\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/read_from_sirius_zip.html#read-from-sirius-zip",
    "href": "man/read_from_sirius_zip.html#read-from-sirius-zip",
    "title": "tima",
    "section": "",
    "text": "This function reads specific files from a compressed SIRIUS workspace directory. It handles file selection, filtering empty files, and parsing tab-delimited data.\n\n\n\nread_from_sirius_zip(sirius_zip, file)\n\n\n\n\n\n\n\nsirius_zip\n\n\nCharacter string path to compressed SIRIUS workspace (.zip)\n\n\n\n\nfile\n\n\nCharacter string pattern to match files within the archive\n\n\n\n\n\n\nData frame (tidytable) containing the parsed file contents\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/annotate_masses.html",
    "href": "man/annotate_masses.html",
    "title": "tima",
    "section": "",
    "text": "This function annotates a feature table based on exact mass match. It requires a structural library, its metadata, and lists of adducts, clusters, and neutral losses to be considered. The polarity has to be pos or neg and retention time and mass tolerances should be given. The feature table is expected to be pre-formatted.\n\n\n\nannotate_masses(\n  features = get_params(step = \"annotate_masses\")\\$files\\$features\\$prepared,\n  output_annotations = get_params(step =\n    \"annotate_masses\")\\$files\\$annotations\\$prepared\\$structural\\$ms1,\n  output_edges = get_params(step =\n    \"annotate_masses\")\\$files\\$networks\\$spectral\\$edges\\$raw\\$ms1,\n  name_source = get_params(step = \"annotate_masses\")\\$names\\$source,\n  name_target = get_params(step = \"annotate_masses\")\\$names\\$target,\n  library = get_params(step = \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  str_stereo = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc,\n  adducts_list = get_params(step = \"annotate_masses\")\\$ms\\$adducts,\n  clusters_list = get_params(step = \"annotate_masses\")\\$ms\\$clusters,\n  neutral_losses_list = get_params(step = \"annotate_masses\")\\$ms\\$neutral_losses,\n  ms_mode = get_params(step = \"annotate_masses\")\\$ms\\$polarity,\n  tolerance_ppm = get_params(step = \"annotate_masses\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms1,\n  tolerance_rt = get_params(step = \"annotate_masses\")\\$ms\\$tolerances\\$rt\\$adducts\n)\n\n\n\n\n\n\n\nfeatures\n\n\nTable containing your previous annotation to complement\n\n\n\n\noutput_annotations\n\n\nOutput for mass based structural annotations\n\n\n\n\noutput_edges\n\n\nOutput for mass based edges\n\n\n\n\nname_source\n\n\nName of the source features column\n\n\n\n\nname_target\n\n\nName of the target features column\n\n\n\n\nlibrary\n\n\nLibrary containing the keys\n\n\n\n\nstr_stereo\n\n\nFile containing structures stereo\n\n\n\n\nstr_met\n\n\nFile containing structures metadata\n\n\n\n\nstr_nam\n\n\nFile containing structures names\n\n\n\n\nstr_tax_cla\n\n\nFile containing Classyfire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nFile containing NPClassifier taxonomy\n\n\n\n\nadducts_list\n\n\nList of adducts to be used\n\n\n\n\nclusters_list\n\n\nList of clusters to be used\n\n\n\n\nneutral_losses_list\n\n\nList of neutral losses to be used\n\n\n\n\nms_mode\n\n\nIonization mode. Must be ‘pos’ or ‘neg’\n\n\n\n\ntolerance_ppm\n\n\nTolerance to perform annotation. Should be &lt;= 20 ppm\n\n\n\n\ntolerance_rt\n\n\nTolerance to group adducts. Should be &lt;= 0.05 minutes\n\n\n\n\n\n\nThe path to the files containing MS1 annotations and edges\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo)\ndir &lt;- paste0(dir, data_interim)\ndir_sop_mer &lt;- paste0(dir, \"libraries/sop/merged/\")\ndir_str &lt;- paste0(dir_sop_mer, \"structures/\")\ndir_tax &lt;- paste0(dir_str, \"taxonomies/\")\nannotate_masses(\n  features = paste0(dir, \"features/example_features.tsv\"),\n  library = paste0(dir_sop_mer, \"keys.tsv\"),\n  str_stereo = paste0(dir_str, \"stereo.tsv\"),\n  str_met = paste0(dir_str, \"metadata.tsv\"),\n  str_nam = paste0(dir_str, \"names.tsv\"),\n  str_tax_cla = paste0(dir_tax, \"classyfire.tsv\"),\n  str_tax_npc = paste0(dir_tax, \"npc.tsv\")\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Annotate",
      "annotate_masses"
    ]
  },
  {
    "objectID": "man/annotate_masses.html#annotate-masses",
    "href": "man/annotate_masses.html#annotate-masses",
    "title": "tima",
    "section": "",
    "text": "This function annotates a feature table based on exact mass match. It requires a structural library, its metadata, and lists of adducts, clusters, and neutral losses to be considered. The polarity has to be pos or neg and retention time and mass tolerances should be given. The feature table is expected to be pre-formatted.\n\n\n\nannotate_masses(\n  features = get_params(step = \"annotate_masses\")\\$files\\$features\\$prepared,\n  output_annotations = get_params(step =\n    \"annotate_masses\")\\$files\\$annotations\\$prepared\\$structural\\$ms1,\n  output_edges = get_params(step =\n    \"annotate_masses\")\\$files\\$networks\\$spectral\\$edges\\$raw\\$ms1,\n  name_source = get_params(step = \"annotate_masses\")\\$names\\$source,\n  name_target = get_params(step = \"annotate_masses\")\\$names\\$target,\n  library = get_params(step = \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  str_stereo = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc,\n  adducts_list = get_params(step = \"annotate_masses\")\\$ms\\$adducts,\n  clusters_list = get_params(step = \"annotate_masses\")\\$ms\\$clusters,\n  neutral_losses_list = get_params(step = \"annotate_masses\")\\$ms\\$neutral_losses,\n  ms_mode = get_params(step = \"annotate_masses\")\\$ms\\$polarity,\n  tolerance_ppm = get_params(step = \"annotate_masses\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms1,\n  tolerance_rt = get_params(step = \"annotate_masses\")\\$ms\\$tolerances\\$rt\\$adducts\n)\n\n\n\n\n\n\n\nfeatures\n\n\nTable containing your previous annotation to complement\n\n\n\n\noutput_annotations\n\n\nOutput for mass based structural annotations\n\n\n\n\noutput_edges\n\n\nOutput for mass based edges\n\n\n\n\nname_source\n\n\nName of the source features column\n\n\n\n\nname_target\n\n\nName of the target features column\n\n\n\n\nlibrary\n\n\nLibrary containing the keys\n\n\n\n\nstr_stereo\n\n\nFile containing structures stereo\n\n\n\n\nstr_met\n\n\nFile containing structures metadata\n\n\n\n\nstr_nam\n\n\nFile containing structures names\n\n\n\n\nstr_tax_cla\n\n\nFile containing Classyfire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nFile containing NPClassifier taxonomy\n\n\n\n\nadducts_list\n\n\nList of adducts to be used\n\n\n\n\nclusters_list\n\n\nList of clusters to be used\n\n\n\n\nneutral_losses_list\n\n\nList of neutral losses to be used\n\n\n\n\nms_mode\n\n\nIonization mode. Must be ‘pos’ or ‘neg’\n\n\n\n\ntolerance_ppm\n\n\nTolerance to perform annotation. Should be &lt;= 20 ppm\n\n\n\n\ntolerance_rt\n\n\nTolerance to group adducts. Should be &lt;= 0.05 minutes\n\n\n\n\n\n\nThe path to the files containing MS1 annotations and edges\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo)\ndir &lt;- paste0(dir, data_interim)\ndir_sop_mer &lt;- paste0(dir, \"libraries/sop/merged/\")\ndir_str &lt;- paste0(dir_sop_mer, \"structures/\")\ndir_tax &lt;- paste0(dir_str, \"taxonomies/\")\nannotate_masses(\n  features = paste0(dir, \"features/example_features.tsv\"),\n  library = paste0(dir_sop_mer, \"keys.tsv\"),\n  str_stereo = paste0(dir_str, \"stereo.tsv\"),\n  str_met = paste0(dir_str, \"metadata.tsv\"),\n  str_nam = paste0(dir_str, \"names.tsv\"),\n  str_tax_cla = paste0(dir_tax, \"classyfire.tsv\"),\n  str_tax_npc = paste0(dir_tax, \"npc.tsv\")\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Annotate",
      "annotate_masses"
    ]
  },
  {
    "objectID": "man/fake_ecmdb.html",
    "href": "man/fake_ecmdb.html",
    "title": "tima",
    "section": "",
    "text": "This function creates a minimal fake ECMDB JSON file when the real download fails. Used as a fallback to prevent pipeline failures during testing or when external resources are unavailable.\n\n\n\nfake_ecmdb(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path where the fake ECMDB zip file should be saved\n\n\n\n\n\n\nCharacter string path to the created fake file\n\n\n\n\nlibrary(\"tima\")\n\nfake_ecmdb(export = \"data/source/ecmdb.json.zip\")"
  },
  {
    "objectID": "man/fake_ecmdb.html#fake-ecmdb",
    "href": "man/fake_ecmdb.html#fake-ecmdb",
    "title": "tima",
    "section": "",
    "text": "This function creates a minimal fake ECMDB JSON file when the real download fails. Used as a fallback to prevent pipeline failures during testing or when external resources are unavailable.\n\n\n\nfake_ecmdb(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path where the fake ECMDB zip file should be saved\n\n\n\n\n\n\nCharacter string path to the created fake file\n\n\n\n\nlibrary(\"tima\")\n\nfake_ecmdb(export = \"data/source/ecmdb.json.zip\")"
  },
  {
    "objectID": "man/harmonize_names_sirius.html",
    "href": "man/harmonize_names_sirius.html",
    "title": "tima",
    "section": "",
    "text": "This function harmonizes SIRIUS output column names by removing numeric prefixes and underscores. SIRIUS often prefixes column names with numbers and underscores (e.g., \"1_compound_name\"), and this function extracts the meaningful part (e.g., \"name\").\n\n\n\nharmonize_names_sirius(x)\n\n\n\n\n\n\n\nx\n\n\nCharacter string containing a SIRIUS column name to harmonize\n\n\n\n\n\n\nCharacter string with everything before and including the last underscore removed, leaving only the meaningful column name portion\n\n\n\n\nlibrary(\"tima\")\n\nharmonize_names_sirius(\"1_compound_name\") # Returns \"name\"\nharmonize_names_sirius(\"2_feature_id\") # Returns \"id\"\nharmonize_names_sirius(\"score\") # Returns \"score\" (no underscore)"
  },
  {
    "objectID": "man/harmonize_names_sirius.html#harmonize-names-sirius",
    "href": "man/harmonize_names_sirius.html#harmonize-names-sirius",
    "title": "tima",
    "section": "",
    "text": "This function harmonizes SIRIUS output column names by removing numeric prefixes and underscores. SIRIUS often prefixes column names with numbers and underscores (e.g., \"1_compound_name\"), and this function extracts the meaningful part (e.g., \"name\").\n\n\n\nharmonize_names_sirius(x)\n\n\n\n\n\n\n\nx\n\n\nCharacter string containing a SIRIUS column name to harmonize\n\n\n\n\n\n\nCharacter string with everything before and including the last underscore removed, leaving only the meaningful column name portion\n\n\n\n\nlibrary(\"tima\")\n\nharmonize_names_sirius(\"1_compound_name\") # Returns \"name\"\nharmonize_names_sirius(\"2_feature_id\") # Returns \"id\"\nharmonize_names_sirius(\"score\") # Returns \"score\" (no underscore)"
  },
  {
    "objectID": "man/clean_collapse.html",
    "href": "man/clean_collapse.html",
    "title": "tima",
    "section": "",
    "text": "This function collapses a grouped dataframe and trims it. It removes NA values, collapses unique values with \" $ \" separator, trims whitespace, and converts empty strings to NA.\n\n\n\nclean_collapse(grouped_df, cols = NULL)\n\n\n\n\n\n\n\ngrouped_df\n\n\nGrouped dataframe to be collapsed\n\n\n\n\ncols\n\n\nCharacter vector of column name(s) to apply collapse to. If NULL (default), applies to all columns.\n\n\n\n\n\n\nA cleaned and collapsed dataframe with unique values collapsed per group\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/clean_collapse.html#clean-collapse",
    "href": "man/clean_collapse.html#clean-collapse",
    "title": "tima",
    "section": "",
    "text": "This function collapses a grouped dataframe and trims it. It removes NA values, collapses unique values with \" $ \" separator, trims whitespace, and converts empty strings to NA.\n\n\n\nclean_collapse(grouped_df, cols = NULL)\n\n\n\n\n\n\n\ngrouped_df\n\n\nGrouped dataframe to be collapsed\n\n\n\n\ncols\n\n\nCharacter vector of column name(s) to apply collapse to. If NULL (default), applies to all columns.\n\n\n\n\n\n\nA cleaned and collapsed dataframe with unique values collapsed per group\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/round_reals.html",
    "href": "man/round_reals.html",
    "title": "tima",
    "section": "",
    "text": "This function rounds numeric values in specified columns of a dataframe. It only affects columns that contain the specified patterns in their names.\n\n\n\nround_reals(df, dig = 5L, cols = c(\"structure_exact_mass\", \"structure_xlogp\"))\n\n\n\n\n\n\n\ndf\n\n\nDataframe containing columns to round\n\n\n\n\ndig\n\n\nInteger number of decimal digits to round to (default: 5)\n\n\n\n\ncols\n\n\nCharacter vector of column name patterns to match (default: c(\"structure_exact_mass\", \"structure_xlogp\"))\n\n\n\n\n\n\nThe dataframe with specified numeric columns rounded to the specified number of digits\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/round_reals.html#round-reals",
    "href": "man/round_reals.html#round-reals",
    "title": "tima",
    "section": "",
    "text": "This function rounds numeric values in specified columns of a dataframe. It only affects columns that contain the specified patterns in their names.\n\n\n\nround_reals(df, dig = 5L, cols = c(\"structure_exact_mass\", \"structure_xlogp\"))\n\n\n\n\n\n\n\ndf\n\n\nDataframe containing columns to round\n\n\n\n\ndig\n\n\nInteger number of decimal digits to round to (default: 5)\n\n\n\n\ncols\n\n\nCharacter vector of column name patterns to match (default: c(\"structure_exact_mass\", \"structure_xlogp\"))\n\n\n\n\n\n\nThe dataframe with specified numeric columns rounded to the specified number of digits\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/fake_lotus.html",
    "href": "man/fake_lotus.html",
    "title": "tima",
    "section": "",
    "text": "This function creates a minimal fake LOTUS (natural products) database file when the real download fails. Creates an empty TSV with proper column structure to prevent pipeline failures during testing or when external resources are unavailable.\n\n\n\nfake_lotus(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path where the fake LOTUS TSV file should be saved\n\n\n\n\n\n\nCharacter string path to the created fake file\n\n\n\n\nlibrary(\"tima\")\n\nfake_lotus(export = \"data/source/lotus.tsv.gz\")"
  },
  {
    "objectID": "man/fake_lotus.html#fake-lotus",
    "href": "man/fake_lotus.html#fake-lotus",
    "title": "tima",
    "section": "",
    "text": "This function creates a minimal fake LOTUS (natural products) database file when the real download fails. Creates an empty TSV with proper column structure to prevent pipeline failures during testing or when external resources are unavailable.\n\n\n\nfake_lotus(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path where the fake LOTUS TSV file should be saved\n\n\n\n\n\n\nCharacter string path to the created fake file\n\n\n\n\nlibrary(\"tima\")\n\nfake_lotus(export = \"data/source/lotus.tsv.gz\")"
  },
  {
    "objectID": "man/columns_model.html",
    "href": "man/columns_model.html",
    "title": "tima",
    "section": "",
    "text": "This function defines the standardized column naming schema used throughout the TIMA package. It organizes columns into logical groups for features, candidates, components, and scoring.\n\n\n\ncolumns_model()\n\n\n\n\nA named list containing character vectors of column names organized by category:\n\n\n\nfeatures_columns\n\n\nBasic feature identifiers (ID, m/z, RT)\n\n\n\n\nfeatures_calculated_columns\n\n\nCalculated feature properties (entropy, taxonomy predictions)\n\n\n\n\ncandidates_calculated_columns\n\n\nCalculated candidate properties\n\n\n\n\ncandidates_sirius_for_columns\n\n\nSIRIUS formula-level annotations\n\n\n\n\ncandidates_sirius_str_columns\n\n\nSIRIUS structure-level scores\n\n\n\n\ncandidates_spectra_columns\n\n\nSpectral library matching results\n\n\n\n\ncandidates_structures_columns\n\n\nChemical structure metadata and taxonomy\n\n\n\n\ncomponents_columns\n\n\nMolecular network component IDs\n\n\n\n\nrank_columns\n\n\nCandidate ranking columns\n\n\n\n\nscore_columns\n\n\nCandidate scoring columns\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/columns_model.html#columns-model",
    "href": "man/columns_model.html#columns-model",
    "title": "tima",
    "section": "",
    "text": "This function defines the standardized column naming schema used throughout the TIMA package. It organizes columns into logical groups for features, candidates, components, and scoring.\n\n\n\ncolumns_model()\n\n\n\n\nA named list containing character vectors of column names organized by category:\n\n\n\nfeatures_columns\n\n\nBasic feature identifiers (ID, m/z, RT)\n\n\n\n\nfeatures_calculated_columns\n\n\nCalculated feature properties (entropy, taxonomy predictions)\n\n\n\n\ncandidates_calculated_columns\n\n\nCalculated candidate properties\n\n\n\n\ncandidates_sirius_for_columns\n\n\nSIRIUS formula-level annotations\n\n\n\n\ncandidates_sirius_str_columns\n\n\nSIRIUS structure-level scores\n\n\n\n\ncandidates_spectra_columns\n\n\nSpectral library matching results\n\n\n\n\ncandidates_structures_columns\n\n\nChemical structure metadata and taxonomy\n\n\n\n\ncomponents_columns\n\n\nMolecular network component IDs\n\n\n\n\nrank_columns\n\n\nCandidate ranking columns\n\n\n\n\nscore_columns\n\n\nCandidate scoring columns\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/fake_annotations_columns.html",
    "href": "man/fake_annotations_columns.html",
    "title": "tima",
    "section": "",
    "text": "This function creates a template data frame with all expected annotation columns initialized to NA. Used as a fallback when no annotations are available or to ensure consistent column structure.\n\n\n\nfake_annotations_columns()\n\n\n\n\nData frame with one row and columns for all annotation fields, with all values set to NA. Columns include feature IDs, structure information, scores, and taxonomic classifications.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/fake_annotations_columns.html#fake-annotations-columns",
    "href": "man/fake_annotations_columns.html#fake-annotations-columns",
    "title": "tima",
    "section": "",
    "text": "This function creates a template data frame with all expected annotation columns initialized to NA. Used as a fallback when no annotations are available or to ensure consistent column structure.\n\n\n\nfake_annotations_columns()\n\n\n\n\nData frame with one row and columns for all annotation fields, with all values set to NA. Columns include feature IDs, structure information, scores, and taxonomic classifications.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/prepare_params.html",
    "href": "man/prepare_params.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares and validates main parameters for the TIMA workflow. It loads YAML configuration files, extracts all parameters, and sets up the parameter structure for downstream analysis steps.\n\n\n\nprepare_params(\n  params_small = get_params(step = \"prepare_params\"),\n  params_advanced = get_params(step = \"prepare_params_advanced\"),\n  step = NA\n)\n\n\n\n\n\n\n\nparams_small\n\n\nList of basic parameters for the workflow\n\n\n\n\nparams_advanced\n\n\nList of advanced parameters for the workflow\n\n\n\n\nstep\n\n\nCharacter string specifying the workflow step (default: NA)\n\n\n\n\n\n\nCharacter vector of paths to YAML files containing prepared parameters\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/prepare_params.html#prepare-params",
    "href": "man/prepare_params.html#prepare-params",
    "title": "tima",
    "section": "",
    "text": "This function prepares and validates main parameters for the TIMA workflow. It loads YAML configuration files, extracts all parameters, and sets up the parameter structure for downstream analysis steps.\n\n\n\nprepare_params(\n  params_small = get_params(step = \"prepare_params\"),\n  params_advanced = get_params(step = \"prepare_params_advanced\"),\n  step = NA\n)\n\n\n\n\n\n\n\nparams_small\n\n\nList of basic parameters for the workflow\n\n\n\n\nparams_advanced\n\n\nList of advanced parameters for the workflow\n\n\n\n\nstep\n\n\nCharacter string specifying the workflow step (default: NA)\n\n\n\n\n\n\nCharacter vector of paths to YAML files containing prepared parameters\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/fake_hmdb.html",
    "href": "man/fake_hmdb.html",
    "title": "tima",
    "section": "",
    "text": "This function creates a minimal fake HMDB SDF file when the real download fails. Used as a fallback to prevent pipeline failures during testing or when external resources are unavailable.\n\n\n\nfake_hmdb(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path where the fake HMDB zip file should be saved\n\n\n\n\n\n\nCharacter string path to the created fake file\n\n\n\n\nlibrary(\"tima\")\n\nfake_hmdb(export = \"data/source/hmdb.sdf.zip\")"
  },
  {
    "objectID": "man/fake_hmdb.html#fake-hmdb",
    "href": "man/fake_hmdb.html#fake-hmdb",
    "title": "tima",
    "section": "",
    "text": "This function creates a minimal fake HMDB SDF file when the real download fails. Used as a fallback to prevent pipeline failures during testing or when external resources are unavailable.\n\n\n\nfake_hmdb(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path where the fake HMDB zip file should be saved\n\n\n\n\n\n\nCharacter string path to the created fake file\n\n\n\n\nlibrary(\"tima\")\n\nfake_hmdb(export = \"data/source/hmdb.sdf.zip\")"
  },
  {
    "objectID": "vignettes/articles/I-gathering.html",
    "href": "vignettes/articles/I-gathering.html",
    "title": "1 Gathering everything you need",
    "section": "",
    "text": "This vignette describes…"
  },
  {
    "objectID": "vignettes/articles/I-gathering.html#parameters",
    "href": "vignettes/articles/I-gathering.html#parameters",
    "title": "1 Gathering everything you need",
    "section": "Parameters",
    "text": "Parameters\nAll steps require parameters. Some default parameters are available and can be accessed in the params/default directory. If you prefer accessing them through the GUI, you can do so. Each parameter contains a small help menu, you can click on, as illustrated below.\n\n\n\n\nFor example, if you want to have an output compatible with Cytoscape, with multiple annotations per features:\n\n\nAll parameters will be saved and reported at the end of your analysis."
  },
  {
    "objectID": "vignettes/articles/I-gathering.html#inputs",
    "href": "vignettes/articles/I-gathering.html#inputs",
    "title": "1 Gathering everything you need",
    "section": "Inputs",
    "text": "Inputs\n\nYour own files\nYou should provide your own files in the main menu. For this tutorial, we will use some example files you can get running:\n\ntima::get_example_files()\n\n\n\nLibraries\nThe following paragraph describes the libraries available by default.\n\nSpectra\nAs a first step, you need spectral libraries to perform MS2-based annotation.\n\nExperimental\nYou can of course use your own experimental spectral library to perform MS2 annotation. We currently support spectral libraries in MSP or MGF format.\nTo get a small example:\n\ntima::get_example_files(\"spectral_lib_with_rt\")\n\n\nGNPS, MassBank & MERLIN\nGNPS, MassBank & MERLIN are downloaded and used by default, for more info about them, see https://github.com/Adafede/SpectRalLibRaRies.\nIn case you want to format your own spectral library to use it for spectral matching, adapt it the same way.\n\n\n\nIn silico\nAs the availability of experimental spectra is limited, we can take advantage of in silico generated spectra.\n\n\n\nWikidata\nWe generated an in silico spectral library of the structures found in Wikidata using CFM4. For more info, see https://doi.org/10.5281/zenodo.5607185. It is made available in both polarities.\nYou can also complement with the in silico spectra from HMDB (not running by default as quite long):\n\n\nHMDB\n\ntima::get_example_files(\"hmdb_is\")\n\n\n\nRetention times\nThis library is optional. As no standard LC method is shared (for now) among laboratories, this library will be heavily laboratory-dependent. It could also be a library of in silico predicted retention times. If you want to prepare you own library, have a look at params/user/prepare_libraries_rt.yaml.\n\n\nStructure-Organism Pairs\n\nLOTUS\nAs we developed LOTUS1 with Taxonomically Informed Metabolite Annotation in mind, we provide it here as a starting point for your structure-organism pairs library.\nThe process to download LOTUS looks like this:\n\n\n\n\n\n\nAs you can see, one target seems outdated. In reality, we force it to search if a new version of LOTUS exists each time. If a newer version exists, it will fetch it and re-run needed steps accordingly.\n\n\nECMDB\nBy default, we also complement LOTUS pairs with the ones coming from ECMDB.\n\n\nHMDB\nAnd we do the same with the ones coming from HMDB.\nFor these first steps, you do not need to change any parameters as they are implemented by default.\n\n\nOther libraries\nAs we want our tool to be flexible, you can also add your own library to LOTUS. You just need to format it in order to be compatible. As example, we prepared some ways too format closed, in house libraries. If you need help formatting your library or would like to share it with us for it to be implemented, feel free to contact us. Before running the corresponding code, do not forget to modify params/user/prepare_libraries_sop_closed.yaml.\n\n\nMerging\nOnce all sub-libraries are ready, they are then merged in a single file that will be used for the next steps.\n\n\n\n\n\n\nWe now recommend you to read the next vignette."
  },
  {
    "objectID": "vignettes/articles/I-gathering.html#footnotes",
    "href": "vignettes/articles/I-gathering.html#footnotes",
    "title": "1 Gathering everything you need",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more informations, see https://doi.org/10.7554/eLife.70780↩︎"
  },
  {
    "objectID": "vignettes/articles/III-processing.html",
    "href": "vignettes/articles/III-processing.html",
    "title": "3 Performing Taxonomically Informed Metabolite Annotation",
    "section": "",
    "text": "This vignette describes how Taxonomically Informed Metabolite Annotation is performed. If you followed all previous steps successfully, this should be a piece of cake, you deserve it!\n\ntima::tima_full()\n#&gt; + par_def_ann_spe dispatched\n#&gt; ✔ par_def_ann_spe completed [31ms, 2.14 kB]\n#&gt; + par_def_wei_ann dispatched\n#&gt; ✔ par_def_wei_ann completed [1ms, 5.27 kB]\n#&gt; + par_def_pre_ann_gnp dispatched\n#&gt; ✔ par_def_pre_ann_gnp completed [1ms, 1.42 kB]\n#&gt; + par_def_pre_lib_sop_mer dispatched\n#&gt; ✔ par_def_pre_lib_sop_mer completed [0ms, 3.40 kB]\n#&gt; + yaml_paths dispatched\n#&gt; ✔ yaml_paths completed [1ms, 11.52 kB]\n#&gt; + par_def_pre_lib_sop_lot dispatched\n#&gt; ✔ par_def_pre_lib_sop_lot completed [1ms, 494 B]\n#&gt; + par_def_ann_mas dispatched\n#&gt; ✔ par_def_ann_mas completed [0ms, 6.09 kB]\n#&gt; + par_def_pre_lib_sop_hmd dispatched\n#&gt; ✔ par_def_pre_lib_sop_hmd completed [1ms, 492 B]\n#&gt; + par_def_fil_ann dispatched\n#&gt; ✔ par_def_fil_ann completed [1ms, 1.34 kB]\n#&gt; + par_def_pre_lib_sop_clo dispatched\n#&gt; ✔ par_def_pre_lib_sop_clo completed [1ms, 523 B]\n#&gt; + par_def_pre_lib_spe dispatched\n#&gt; ✔ par_def_pre_lib_spe completed [1ms, 1.57 kB]\n#&gt; + par_def_pre_fea_com dispatched\n#&gt; ✔ par_def_pre_fea_com completed [1ms, 358 B]\n#&gt; + par_def_cre_com dispatched\n#&gt; ✔ par_def_cre_com completed [1ms, 375 B]\n#&gt; + par_def_cre_edg_spe dispatched\n#&gt; ✔ par_def_cre_edg_spe completed [1ms, 1.42 kB]\n#&gt; + par_def_pre_fea_edg dispatched\n#&gt; ✔ par_def_pre_fea_edg completed [1ms, 706 B]\n#&gt; + par_def_pre_fea_tab dispatched\n#&gt; ✔ par_def_pre_fea_tab completed [1ms, 860 B]\n#&gt; + par_def_pre_lib_rt dispatched\n#&gt; ✔ par_def_pre_lib_rt completed [1ms, 2.05 kB]\n#&gt; + par_def_pre_ann_spe dispatched\n#&gt; ✔ par_def_pre_ann_spe completed [1ms, 1.46 kB]\n#&gt; + par_def_pre_ann_sir dispatched\n#&gt; ✔ par_def_pre_ann_sir completed [1ms, 1.93 kB]\n#&gt; + par_def_pre_tax dispatched\n#&gt; ✔ par_def_pre_tax completed [1ms, 1.51 kB]\n#&gt; + par_def_pre_lib_sop_ecm dispatched\n#&gt; ✔ par_def_pre_lib_sop_ecm completed [1ms, 492 B]\n#&gt; + paths dispatched\n#&gt; ✔ paths completed [1ms, 2.52 kB]\n#&gt; + lib_spe_exp_gnp_pre_sop dispatched\n#&gt; [2025-11-06 16:17:38.866178] [INFO] Downloading file from: https://github.com/Adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/sop/gnps_11566051_prepared.tsv.gz\n#&gt; [2025-11-06 16:17:38.86841] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:17:38.869318] [DEBUG] Created directory: data/interim/libraries/sop\n#&gt; [2025-11-06 16:17:38.870042] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:17:39.41254] [INFO] Successfully downloaded 1.35 MB to: data/interim/libraries/sop/gnps_11566051_prepared.tsv.gz\n#&gt; ✔ lib_spe_exp_gnp_pre_sop completed [548ms, 1.42 MB]\n#&gt; + lib_spe_exp_mb_pre_sop dispatched\n#&gt; [2025-11-06 16:17:39.600323] [INFO] Downloading file from: https://github.com/Adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/sop/massbank_2025051_prepared.tsv.gz\n#&gt; [2025-11-06 16:17:39.601501] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:17:39.602338] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:17:39.996325] [INFO] Successfully downloaded 0.46 MB to: data/interim/libraries/sop/massbank_2025051_prepared.tsv.gz\n#&gt; ✔ lib_spe_exp_mb_pre_sop completed [397ms, 480.97 kB]\n#&gt; + lib_spe_exp_mer_pre_sop dispatched\n#&gt; [2025-11-06 16:17:40.188675] [INFO] Downloading file from: https://github.com/Adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/sop/merlin_13911806_prepared.tsv.gz\n#&gt; [2025-11-06 16:17:40.189801] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:17:40.190687] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:17:40.515526] [INFO] Successfully downloaded 1.14 MB to: data/interim/libraries/sop/merlin_13911806_prepared.tsv.gz\n#&gt; ✔ lib_spe_exp_mer_pre_sop completed [329ms, 1.19 MB]\n#&gt; + lib_spe_is_wik_pre_sop dispatched\n#&gt; [2025-11-06 16:17:40.701411] [INFO] Downloading file from: https://github.com/taxonomicallyinformedannotation/tima-example-files/raw/main/wikidata_spectral_5607185_prepared.tsv.gz\n#&gt; [2025-11-06 16:17:40.702594] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:17:40.703413] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:17:41.397165] [INFO] Successfully downloaded 36.15 MB to: data/interim/libraries/sop/wikidata_5607185_prepared.tsv.gz\n#&gt; ✔ lib_spe_is_wik_pre_sop completed [697ms, 37.90 MB]\n#&gt; + lib_spe_exp_mb_pre_pos dispatched\n#&gt; [2025-11-06 16:17:41.598005] [INFO] Downloading file from: https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/massbank_2025051_pos.rds\n#&gt; [2025-11-06 16:17:41.599365] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:17:41.600322] [DEBUG] Created directory: data/interim/libraries/spectra/exp\n#&gt; [2025-11-06 16:17:41.601065] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:17:42.365398] [INFO] Successfully downloaded 18.51 MB to: data/interim/libraries/spectra/exp/massbank_2025051_pos.rds\n#&gt; ✔ lib_spe_exp_mb_pre_pos completed [768ms, 19.41 MB]\n#&gt; + par_pre_par dispatched\n#&gt; ✔ par_pre_par completed [0ms, 1.38 kB]\n#&gt; + lib_spe_exp_mer_pre_neg dispatched\n#&gt; [2025-11-06 16:17:42.743134] [INFO] Downloading file from: https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/merlin_13911806_neg.rds\n#&gt; [2025-11-06 16:17:42.744132] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:17:42.744929] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:17:43.69844] [INFO] Successfully downloaded 30.08 MB to: data/interim/libraries/spectra/exp/merlin_13911806_neg.rds\n#&gt; ✔ lib_spe_exp_mer_pre_neg completed [957ms, 31.54 MB]\n#&gt; + lib_spe_is_wik_pre_neg dispatched\n#&gt; [2025-11-06 16:17:43.904856] [INFO] Downloading file from: https://github.com/taxonomicallyinformedannotation/tima-isdb-neg/raw/main/wikidata_5607185_neg.rds\n#&gt; [2025-11-06 16:17:43.905953] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:17:43.906865] [DEBUG] Created directory: data/interim/libraries/spectra/is\n#&gt; [2025-11-06 16:17:43.907592] [DEBUG] Download attempt 1/3\n#&gt; Downloading  13% ■■■■■                             7s\n#&gt; Downloading  29% ■■■■■■■■■■                        5s\n#&gt; Downloading  68% ■■■■■■■■■■■■■■■■■■■■■■            2s\n#&gt; Downloading 100% ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■   0s\n#&gt; [2025-11-06 16:17:51.938578] [INFO] Successfully downloaded 655.49 MB to: data/interim/libraries/spectra/is/wikidata_5607185_neg.rds\n#&gt; ✔ lib_spe_is_wik_pre_neg completed [8s, 687.33 MB]\n#&gt; + par_pre_par2 dispatched\n#&gt; ✔ par_pre_par2 completed [0ms, 21.39 kB]\n#&gt; + lib_spe_is_wik_pre_pos dispatched\n#&gt; [2025-11-06 16:17:52.574451] [INFO] Downloading file from: https://github.com/taxonomicallyinformedannotation/tima-isdb-pos/raw/main/wikidata_5607185_pos.rds\n#&gt; [2025-11-06 16:17:52.575685] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:17:52.576462] [DEBUG] Download attempt 1/3\n#&gt; Downloading  10% ■■■■                              9s\n#&gt; Downloading  21% ■■■■■■■                          10s\n#&gt; Downloading  51% ■■■■■■■■■■■■■■■■                  5s\n#&gt; Downloading  80% ■■■■■■■■■■■■■■■■■■■■■■■■■         2s\n#&gt; Downloading 100% ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■   0s\n#&gt; [2025-11-06 16:18:03.834081] [INFO] Successfully downloaded 823.93 MB to: data/interim/libraries/spectra/is/wikidata_5607185_pos.rds\n#&gt; ✔ lib_spe_is_wik_pre_pos completed [11.3s, 863.95 MB]\n#&gt; + lib_sop_lot dispatched\n#&gt; [2025-11-06 16:18:04.347023] [INFO] Retrieving latest version from Zenodo: 10.5281/zenodo.5794106\n#&gt; [2025-11-06 16:18:04.358332] [DEBUG] Fetching metadata from Zenodo API\n#&gt; [2025-11-06 16:18:06.778636] [INFO] Downloading 230106_frozen_metadata.csv.gz from https://doi.org/10.5281/zenodo.5794106 (The LOTUS Initiative for Open Natural Products Research: frozen dataset union wikidata (with metadata))\n#&gt; [2025-11-06 16:18:06.779627] [DEBUG] Size: 92979778 bytes\n#&gt; [2025-11-06 16:18:06.780452] [DEBUG] Created directory: data/source/libraries/sop\n#&gt; [2025-11-06 16:18:06.781224] [INFO] Downloading file from: https://zenodo.org/records/7534071/files/230106_frozen_metadata.csv.gz\n#&gt; [2025-11-06 16:18:06.781898] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:18:06.782591] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:20:54.439822] [INFO] Successfully downloaded 88.67 MB to: data/source/libraries/sop/lotus.csv.gz\n#&gt; ✔ lib_sop_lot completed [2m 50.1s, 92.98 MB]\n#&gt; + lib_sop_hmd dispatched\n#&gt; [2025-11-06 16:20:54.675652] [INFO] Downloading file from: https://hmdb.ca/system/downloads/current/structures.zip\n#&gt; [2025-11-06 16:20:54.676729] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:20:54.677661] [DEBUG] Created directory: data/source/libraries/sop/hmdb\n#&gt; [2025-11-06 16:20:54.67841] [DEBUG] Download attempt 1/3\n#&gt; Downloading  33% ■■■■■■■■■■■                       2s\n#&gt; Downloading  34% ■■■■■■■■■■■                       2s\n#&gt; Downloading 100% ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■   0s\n#&gt; [2025-11-06 16:21:24.264565] [INFO] Successfully downloaded 92.01 MB to: data/source/libraries/sop/hmdb/structures.zip\n#&gt; ✔ lib_sop_hmd completed [29.6s, 96.48 MB]\n#&gt; + lib_spe_exp_gnp_pre_neg dispatched\n#&gt; [2025-11-06 16:21:24.499121] [INFO] Downloading file from: https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/gnps_11566051_neg.rds\n#&gt; [2025-11-06 16:21:24.500516] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:21:24.501358] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:21:25.028996] [INFO] Successfully downloaded 146.98 MB to: data/interim/libraries/spectra/exp/gnps_11566051_neg.rds\n#&gt; ✔ lib_spe_exp_gnp_pre_neg completed [531ms, 154.12 MB]\n#&gt; + lib_spe_exp_mer_pre_pos dispatched\n#&gt; [2025-11-06 16:21:25.278587] [INFO] Downloading file from: https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/merlin_13911806_pos.rds\n#&gt; [2025-11-06 16:21:25.279744] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:21:25.280592] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:21:25.550682] [INFO] Successfully downloaded 81 MB to: data/interim/libraries/spectra/exp/merlin_13911806_pos.rds\n#&gt; ✔ lib_spe_exp_mer_pre_pos completed [274ms, 84.94 MB]\n#&gt; + lib_sop_ecm dispatched\n#&gt; [2025-11-06 16:21:25.778723] [INFO] Downloading file from: https://ecmdb.ca/download/ecmdb.json.zip\n#&gt; [2025-11-06 16:21:25.779818] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:21:25.780672] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:21:26.678057] [INFO] Successfully downloaded 1.27 MB to: data/source/libraries/sop/ecmdb.json.zip\n#&gt; ✔ lib_sop_ecm completed [901ms, 1.33 MB]\n#&gt; + lib_spe_exp_mb_pre_neg dispatched\n#&gt; [2025-11-06 16:21:26.871935] [INFO] Downloading file from: https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/massbank_2025051_neg.rds\n#&gt; [2025-11-06 16:21:26.873054] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:21:26.873869] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:21:26.991184] [INFO] Successfully downloaded 6.73 MB to: data/interim/libraries/spectra/exp/massbank_2025051_neg.rds\n#&gt; ✔ lib_spe_exp_mb_pre_neg completed [120ms, 7.06 MB]\n#&gt; + lib_spe_exp_gnp_pre_pos dispatched\n#&gt; [2025-11-06 16:21:27.195846] [INFO] Downloading file from: https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/gnps_11566051_pos.rds\n#&gt; [2025-11-06 16:21:27.196979] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:21:27.197824] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:21:28.256205] [INFO] Successfully downloaded 458.98 MB to: data/interim/libraries/spectra/exp/gnps_11566051_pos.rds\n#&gt; ✔ lib_spe_exp_gnp_pre_pos completed [1.1s, 481.27 MB]\n#&gt; + par_fin_par dispatched\n#&gt; ✔ par_fin_par completed [1ms, 307 B]\n#&gt; + par_fin_par2 dispatched\n#&gt; ✔ par_fin_par2 completed [2ms, 2.98 kB]\n#&gt; + par_usr_pre_lib_sop_mer dispatched\n#&gt; [2025-11-06 16:21:29.004794] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:29.005892] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:29.008778] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:29.026338] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:29.027185] [TRACE] Small params\n#&gt; [2025-11-06 16:21:29.027966] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:29.028618] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:29.029521] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:30.463172] [TRACE] Exporting params\n#&gt; [2025-11-06 16:21:30.464194] [DEBUG] Created directory: params/user\n#&gt; ✔ par_usr_pre_lib_sop_mer completed [1.5s, 1.55 kB]\n#&gt; + par_usr_pre_lib_sop_lot dispatched\n#&gt; [2025-11-06 16:21:30.677768] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:30.679142] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:30.681459] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:30.686559] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:30.687369] [TRACE] Small params\n#&gt; [2025-11-06 16:21:30.688042] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:30.688687] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:30.689531] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:32.046438] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_lib_sop_lot completed [1.4s, 174 B]\n#&gt; + par_usr_pre_tax dispatched\n#&gt; [2025-11-06 16:21:32.254063] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:32.255199] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:32.257561] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:32.262662] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:32.263467] [TRACE] Small params\n#&gt; [2025-11-06 16:21:32.264151] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:32.264798] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:32.265633] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:33.62306] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_tax completed [1.4s, 438 B]\n#&gt; + par_usr_pre_ann_gnp dispatched\n#&gt; [2025-11-06 16:21:33.841816] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:33.843256] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:33.845778] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:33.851019] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:33.851857] [TRACE] Small params\n#&gt; [2025-11-06 16:21:33.852516] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:33.853142] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:33.854002] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:35.403143] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_ann_gnp completed [1.6s, 708 B]\n#&gt; + par_usr_pre_lib_sop_hmd dispatched\n#&gt; [2025-11-06 16:21:35.619378] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:35.620744] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:35.623041] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:35.628086] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:35.628876] [TRACE] Small params\n#&gt; [2025-11-06 16:21:35.629539] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:35.630182] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:35.631007] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:37.002116] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_lib_sop_hmd completed [1.4s, 178 B]\n#&gt; + par_usr_cre_com dispatched\n#&gt; [2025-11-06 16:21:37.212652] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:37.213807] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:37.216205] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:37.221272] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:37.222093] [TRACE] Small params\n#&gt; [2025-11-06 16:21:37.222797] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:37.22342] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:37.224249] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:38.572821] [TRACE] Exporting params\n#&gt; ✔ par_usr_cre_com completed [1.4s, 200 B]\n#&gt; + par_usr_pre_lib_sop_clo dispatched\n#&gt; [2025-11-06 16:21:38.782918] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:38.784302] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:38.786731] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:38.791868] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:38.792708] [TRACE] Small params\n#&gt; [2025-11-06 16:21:38.793363] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:38.794008] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:38.794845] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:40.15498] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_lib_sop_clo completed [1.4s, 205 B]\n#&gt; + par_usr_cre_edg_spe dispatched\n#&gt; [2025-11-06 16:21:40.363317] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:40.364431] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:40.366782] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:40.37184] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:40.372712] [TRACE] Small params\n#&gt; [2025-11-06 16:21:40.373352] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:40.373998] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:40.374824] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:41.729473] [TRACE] Exporting params\n#&gt; ✔ par_usr_cre_edg_spe completed [1.4s, 452 B]\n#&gt; + par_usr_pre_fea_com dispatched\n#&gt; [2025-11-06 16:21:41.937487] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:41.938651] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:41.940996] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:41.946102] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:41.946928] [TRACE] Small params\n#&gt; [2025-11-06 16:21:41.947592] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:41.94822] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:41.949092] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:43.305647] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_fea_com completed [1.4s, 200 B]\n#&gt; + par_usr_pre_fea_edg dispatched\n#&gt; [2025-11-06 16:21:43.513932] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:43.515336] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:43.517765] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:43.522892] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:43.52374] [TRACE] Small params\n#&gt; [2025-11-06 16:21:43.5244] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:43.525039] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:43.525877] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:44.88438] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_fea_edg completed [1.4s, 328 B]\n#&gt; + par_usr_pre_lib_sop_ecm dispatched\n#&gt; [2025-11-06 16:21:45.092645] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:45.093833] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:45.096244] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:45.101439] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:45.102326] [TRACE] Small params\n#&gt; [2025-11-06 16:21:45.103022] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:45.10368] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:45.10454] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:46.450572] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_lib_sop_ecm completed [1.4s, 176 B]\n#&gt; + par_usr_fil_ann dispatched\n#&gt; [2025-11-06 16:21:46.660913] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:46.662317] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:46.664719] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:46.669892] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:46.670718] [TRACE] Small params\n#&gt; [2025-11-06 16:21:46.671373] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:46.672029] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:46.672881] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:48.03343] [TRACE] Exporting params\n#&gt; ✔ par_usr_fil_ann completed [1.4s, 668 B]\n#&gt; + par_usr_pre_fea_tab dispatched\n#&gt; [2025-11-06 16:21:48.243392] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:48.244563] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:48.24701] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:48.252086] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:48.252911] [TRACE] Small params\n#&gt; [2025-11-06 16:21:48.253578] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:48.254205] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:48.255083] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:49.618575] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_fea_tab completed [1.4s, 274 B]\n#&gt; + par_usr_pre_lib_rt dispatched\n#&gt; [2025-11-06 16:21:49.828955] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:49.830114] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:49.832473] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:49.83762] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:49.838416] [TRACE] Small params\n#&gt; [2025-11-06 16:21:49.839078] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:49.839726] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:49.840572] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:51.184592] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_lib_rt completed [1.4s, 440 B]\n#&gt; + par_usr_ann_spe dispatched\n#&gt; [2025-11-06 16:21:51.394082] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:51.395423] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:51.397776] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:51.402879] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:51.403695] [TRACE] Small params\n#&gt; [2025-11-06 16:21:51.404318] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:51.404947] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:51.405784] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:52.765233] [TRACE] Exporting params\n#&gt; ✔ par_usr_ann_spe completed [1.4s, 1.03 kB]\n#&gt; + par_usr_pre_ann_spe dispatched\n#&gt; [2025-11-06 16:21:52.973999] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:52.975143] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:52.977552] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:52.982685] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:52.983499] [TRACE] Small params\n#&gt; [2025-11-06 16:21:52.984152] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:52.984789] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:52.98562] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:54.328779] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_ann_spe completed [1.4s, 731 B]\n#&gt; + par_usr_pre_lib_spe dispatched\n#&gt; [2025-11-06 16:21:54.53713] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:54.538437] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:54.540817] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:54.545922] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:54.546739] [TRACE] Small params\n#&gt; [2025-11-06 16:21:54.547373] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:54.548008] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:54.548841] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:55.911065] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_lib_spe completed [1.4s, 298 B]\n#&gt; + par_usr_pre_ann_sir dispatched\n#&gt; [2025-11-06 16:21:56.124241] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:56.125567] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:56.127861] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:56.132871] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:56.13366] [TRACE] Small params\n#&gt; [2025-11-06 16:21:56.134278] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:56.13491] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:56.135707] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:57.518798] [TRACE] Exporting params\n#&gt; ✔ par_usr_pre_ann_sir completed [1.4s, 900 B]\n#&gt; + par_usr_ann_mas dispatched\n#&gt; [2025-11-06 16:21:57.736073] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:57.737347] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:57.740086] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:57.745393] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:57.746397] [TRACE] Small params\n#&gt; [2025-11-06 16:21:57.747237] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:57.748123] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:57.749296] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:21:59.123194] [TRACE] Exporting params\n#&gt; ✔ par_usr_ann_mas completed [1.4s, 2.68 kB]\n#&gt; + par_usr_wei_ann dispatched\n#&gt; [2025-11-06 16:21:59.335617] [INFO] Preparing TIMA workflow parameters\n#&gt; [2025-11-06 16:21:59.337056] [TRACE] Loading YAML parameter files\n#&gt; [2025-11-06 16:21:59.339498] [DEBUG] Using default parameters\n#&gt; [2025-11-06 16:21:59.344787] [TRACE] Loaded 22 YAML parameter files\n#&gt; [2025-11-06 16:21:59.345633] [TRACE] Small params\n#&gt; [2025-11-06 16:21:59.346295] [TRACE] Advanced params\n#&gt; [2025-11-06 16:21:59.346962] [TRACE] Changing params\n#&gt; [2025-11-06 16:21:59.347811] [TRACE] Changing filenames\n#&gt; [2025-11-06 16:22:00.716635] [TRACE] Exporting params\n#&gt; ✔ par_usr_wei_ann completed [1.4s, 1.78 kB]\n#&gt; + par_pre_lib_sop_mer dispatched\n#&gt; [2025-11-06 16:22:00.927559] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_sop_merged.yaml\n#&gt; ✔ par_pre_lib_sop_mer completed [2ms, 558 B]\n#&gt; + par_pre_lib_sop_lot dispatched\n#&gt; [2025-11-06 16:22:01.136398] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_sop_lotus.yaml\n#&gt; ✔ par_pre_lib_sop_lot completed [2ms, 186 B]\n#&gt; + par_pre_tax dispatched\n#&gt; [2025-11-06 16:22:01.335526] [DEBUG] Loading user-specified parameters from: params/user/prepare_taxa.yaml\n#&gt; ✔ par_pre_tax completed [2ms, 330 B]\n#&gt; + par_pre_ann_gnp dispatched\n#&gt; [2025-11-06 16:22:01.54436] [DEBUG] Loading user-specified parameters from: params/user/prepare_annotations_gnps.yaml\n#&gt; ✔ par_pre_ann_gnp completed [2ms, 336 B]\n#&gt; + par_pre_lib_sop_hmd dispatched\n#&gt; [2025-11-06 16:22:01.750566] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_sop_hmdb.yaml\n#&gt; ✔ par_pre_lib_sop_hmd completed [2ms, 191 B]\n#&gt; + par_cre_com dispatched\n#&gt; [2025-11-06 16:22:01.949646] [DEBUG] Loading user-specified parameters from: params/user/create_components.yaml\n#&gt; ✔ par_cre_com completed [2ms, 191 B]\n#&gt; + par_pre_lib_sop_clo dispatched\n#&gt; [2025-11-06 16:22:02.15182] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_sop_closed.yaml\n#&gt; ✔ par_pre_lib_sop_clo completed [2ms, 213 B]\n#&gt; + par_cre_edg_spe dispatched\n#&gt; [2025-11-06 16:22:02.348335] [DEBUG] Loading user-specified parameters from: params/user/create_edges_spectra.yaml\n#&gt; ✔ par_cre_edg_spe completed [2ms, 389 B]\n#&gt; + par_pre_fea_com dispatched\n#&gt; [2025-11-06 16:22:02.549552] [DEBUG] Loading user-specified parameters from: params/user/prepare_features_components.yaml\n#&gt; ✔ par_pre_fea_com completed [2ms, 184 B]\n#&gt; + par_pre_fea_edg dispatched\n#&gt; [2025-11-06 16:22:02.750867] [DEBUG] Loading user-specified parameters from: params/user/prepare_features_edges.yaml\n#&gt; ✔ par_pre_fea_edg completed [2ms, 244 B]\n#&gt; + par_pre_lib_sop_ecm dispatched\n#&gt; [2025-11-06 16:22:02.953674] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_sop_ecmdb.yaml\n#&gt; ✔ par_pre_lib_sop_ecm completed [2ms, 191 B]\n#&gt; + par_fil_ann dispatched\n#&gt; [2025-11-06 16:22:03.155282] [DEBUG] Loading user-specified parameters from: params/user/filter_annotations.yaml\n#&gt; ✔ par_fil_ann completed [2ms, 346 B]\n#&gt; + par_pre_fea_tab dispatched\n#&gt; [2025-11-06 16:22:03.362407] [DEBUG] Loading user-specified parameters from: params/user/prepare_features_tables.yaml\n#&gt; ✔ par_pre_fea_tab completed [2ms, 278 B]\n#&gt; + par_pre_lib_rt dispatched\n#&gt; [2025-11-06 16:22:03.564915] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_rt.yaml\n#&gt; ✔ par_pre_lib_rt completed [2ms, 341 B]\n#&gt; + par_ann_spe dispatched\n#&gt; [2025-11-06 16:22:03.761901] [DEBUG] Loading user-specified parameters from: params/user/annotate_spectra.yaml\n#&gt; ✔ par_ann_spe completed [2ms, 497 B]\n#&gt; + par_pre_ann_spe dispatched\n#&gt; [2025-11-06 16:22:03.967671] [DEBUG] Loading user-specified parameters from: params/user/prepare_annotations_spectra.yaml\n#&gt; ✔ par_pre_ann_spe completed [2ms, 334 B]\n#&gt; + par_pre_lib_spe dispatched\n#&gt; [2025-11-06 16:22:04.175429] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_spectra.yaml\n#&gt; ✔ par_pre_lib_spe completed [2ms, 404 B]\n#&gt; + par_pre_ann_sir dispatched\n#&gt; [2025-11-06 16:22:04.377716] [DEBUG] Loading user-specified parameters from: params/user/prepare_annotations_sirius.yaml\n#&gt; ✔ par_pre_ann_sir completed [2ms, 405 B]\n#&gt; + par_ann_mas dispatched\n#&gt; [2025-11-06 16:22:04.581785] [DEBUG] Loading user-specified parameters from: params/user/annotate_masses.yaml\n#&gt; ✔ par_ann_mas completed [2ms, 1.13 kB]\n#&gt; + par_wei_ann dispatched\n#&gt; [2025-11-06 16:22:04.78691] [DEBUG] Loading user-specified parameters from: params/user/weight_annotations.yaml\n#&gt; ✔ par_wei_ann completed [2ms, 954 B]\n#&gt; + lib_sop_mer_str_pro dispatched\n#&gt; [2025-11-06 16:22:04.989492] [INFO] Downloading file from: https://github.com/taxonomicallyinformedannotation/tima-example-files/raw/main/processed.csv.gz\n#&gt; [2025-11-06 16:22:04.990596] [DEBUG] Timeout limit: 3600 seconds\n#&gt; [2025-11-06 16:22:04.9916] [DEBUG] Created directory: data/interim/libraries/sop/merged/structures\n#&gt; [2025-11-06 16:22:04.992359] [DEBUG] Download attempt 1/3\n#&gt; [2025-11-06 16:22:05.386209] [INFO] Successfully downloaded 86.32 MB to: data/interim/libraries/sop/merged/structures/processed.csv.gz\n#&gt; ✔ lib_sop_mer_str_pro completed [398ms, 90.51 MB]\n#&gt; + lib_sop_lot_pre dispatched\n#&gt; [2025-11-06 16:22:05.615611] [INFO] Loading LOTUS database from: data/source/libraries/sop/lotus.csv.gz\n#&gt; [2025-11-06 16:22:05.61675] [DEBUG] File size: 88.67 MB\n#&gt; [2025-11-06 16:22:15.163223] [TRACE] Rounding 2 columns to 5 decimal places\n#&gt; [2025-11-06 16:22:17.153286] [INFO] Prepared 791809 unique structure-organism pairs from LOTUS\n#&gt; [2025-11-06 16:22:17.154709] [TRACE] Exporting prepared LOTUS data\n#&gt; [2025-11-06 16:22:17.15539] [INFO] Exporting data to: data/interim/libraries/sop/lotus_prepared.tsv.gz\n#&gt; [2025-11-06 16:22:17.156063] [DEBUG] Dimensions: 791809 rows x 29 columns (29 variables)\n#&gt; [2025-11-06 16:22:17.156838] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:22:20.646316] [INFO] Successfully exported 791809 rows to data/interim/libraries/sop/lotus_prepared.tsv.gz\n#&gt; ✔ lib_sop_lot_pre completed [15s, 46.52 MB]\n#&gt; + lib_sop_hmd_pre dispatched\n#&gt; [2025-11-06 16:22:21.081878] [INFO] Preparing HMDB structure-organism pairs\n#&gt; [2025-11-06 16:22:21.083278] [DEBUG] Processing HMDB from: data/source/libraries/sop/hmdb/structures.zip\n#&gt; [2025-11-06 16:22:21.08435] [TRACE] Unzipping HMDB\n#&gt; [2025-11-06 16:22:24.456545] [TRACE] Loading HMDB\n#&gt; [2025-11-06 16:23:07.597966] [TRACE] Formatting HMDB\n#&gt; [2025-11-06 16:23:07.907604] [TRACE] Rounding 2 columns to 5 decimal places\n#&gt; [2025-11-06 16:23:08.091923] [TRACE] Deleting unzipped file\n#&gt; [2025-11-06 16:23:08.4158] [INFO] Exporting data to: data/interim/libraries/sop/hmdb_prepared.tsv.gz\n#&gt; [2025-11-06 16:23:08.417091] [DEBUG] Dimensions: 217776 rows x 29 columns (29 variables)\n#&gt; [2025-11-06 16:23:08.418088] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:23:09.036435] [INFO] Successfully exported 217776 rows to data/interim/libraries/sop/hmdb_prepared.tsv.gz\n#&gt; ✔ lib_sop_hmd_pre completed [48s, 8.06 MB]\n#&gt; + lib_sop_clo_pre dispatched\n#&gt; [2025-11-06 16:23:09.682955] [INFO] Preparing closed structure-organism pairs library\n#&gt; [2025-11-06 16:23:09.6848] [WARN] Closed resource not accessible at: ~/Git/lotus-processor/data/processed/240412_closed_metadata.csv.gz. Returning empty template instead.\n#&gt; [2025-11-06 16:23:09.688795] [DEBUG] Loading user parameters from: params/user/prepare_libraries_sop_closed.yaml\n#&gt; [2025-11-06 16:23:09.689631] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_sop_closed.yaml\n#&gt; [2025-11-06 16:23:09.701016] [DEBUG] Created directory: data/interim/params\n#&gt; [2025-11-06 16:23:09.701769] [INFO] Exporting parameters to: data/interim/params/251106_162309_prepare_libraries_sop_closed.yaml\n#&gt; [2025-11-06 16:23:09.70255] [DEBUG] Successfully exported 1 parameters\n#&gt; [2025-11-06 16:23:09.703213] [INFO] Exporting data to: data/interim/libraries/sop/closed_prepared.tsv.gz\n#&gt; [2025-11-06 16:23:09.703852] [DEBUG] Dimensions: 1 rows x 29 columns (29 variables)\n#&gt; [2025-11-06 16:23:09.704816] [INFO] Successfully exported 1 rows to data/interim/libraries/sop/closed_prepared.tsv.gz\n#&gt; ✔ lib_sop_clo_pre completed [23ms, 273 B]\n#&gt; + lib_sop_ecm_pre dispatched\n#&gt; [2025-11-06 16:23:09.92116] [INFO] Preparing ECMDB structure-organism pairs\n#&gt; [2025-11-06 16:23:09.922251] [DEBUG] Processing ECMDB from: data/source/libraries/sop/ecmdb.json.zip\n#&gt; [2025-11-06 16:23:09.923024] [TRACE] Loading ECMDB resources\n#&gt; [2025-11-06 16:23:10.530353] [TRACE] Formatting ECMDB\n#&gt; [2025-11-06 16:23:10.555414] [TRACE] Rounding 2 columns to 5 decimal places\n#&gt; [2025-11-06 16:23:10.564808] [DEBUG] Loading user parameters from: params/user/prepare_libraries_sop_ecmdb.yaml\n#&gt; [2025-11-06 16:23:10.565715] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_sop_ecmdb.yaml\n#&gt; [2025-11-06 16:23:10.577532] [INFO] Exporting parameters to: data/interim/params/251106_162310_prepare_libraries_sop_ecmdb.yaml\n#&gt; [2025-11-06 16:23:10.578397] [DEBUG] Successfully exported 1 parameters\n#&gt; [2025-11-06 16:23:10.579102] [INFO] Exporting data to: data/interim/libraries/sop/ecmdb_prepared.tsv.gz\n#&gt; [2025-11-06 16:23:10.579753] [DEBUG] Dimensions: 3760 rows x 29 columns (29 variables)\n#&gt; [2025-11-06 16:23:10.594604] [INFO] Successfully exported 3760 rows to data/interim/libraries/sop/ecmdb_prepared.tsv.gz\n#&gt; ✔ lib_sop_ecm_pre completed [674ms, 177.47 kB]\n#&gt; + par_pre_fea_tab_fil_fea_raw dispatched\n#&gt; ✔ par_pre_fea_tab_fil_fea_raw completed [0ms, 451.55 kB]\n#&gt; + lib_rt dispatched\n#&gt; [2025-11-06 16:23:11.051601] [INFO] Preparing retention time libraries\n#&gt; [2025-11-06 16:23:11.052668] [DEBUG] RT unit: seconds\n#&gt; [2025-11-06 16:23:11.064006] [WARN] No retention time library found, returning empty sop table.\n#&gt; [2025-11-06 16:23:11.064795] [WARN] No retention time library found, returning empty retention time table.\n#&gt; [2025-11-06 16:23:11.068403] [DEBUG] Loading user parameters from: params/user/prepare_libraries_rt.yaml\n#&gt; [2025-11-06 16:23:11.069424] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_rt.yaml\n#&gt; [2025-11-06 16:23:11.105822] [INFO] Exporting parameters to: data/interim/params/251106_162311_prepare_libraries_rt.yaml\n#&gt; [2025-11-06 16:23:11.106704] [DEBUG] Successfully exported 3 parameters\n#&gt; [2025-11-06 16:23:11.107464] [DEBUG] Created directory: data/interim/libraries/rt\n#&gt; [2025-11-06 16:23:11.108162] [INFO] Exporting data to: data/interim/libraries/rt/prepared.tsv.gz\n#&gt; [2025-11-06 16:23:11.108836] [DEBUG] Dimensions: 1 rows x 3 columns (3 variables)\n#&gt; [2025-11-06 16:23:11.109796] [INFO] Successfully exported 1 rows to data/interim/libraries/rt/prepared.tsv.gz\n#&gt; [2025-11-06 16:23:11.11056] [INFO] Exporting data to: data/interim/libraries/sop/rt_prepared.tsv.gz\n#&gt; [2025-11-06 16:23:11.11122] [DEBUG] Dimensions: 1 rows x 4 columns (4 variables)\n#&gt; [2025-11-06 16:23:11.112189] [INFO] Successfully exported 1 rows to data/interim/libraries/sop/rt_prepared.tsv.gz\n#&gt; ✔ lib_rt completed [62ms, 182 B]\n#&gt; + par_ann_spe_fil_spe_raw dispatched\n#&gt; ✔ par_ann_spe_fil_spe_raw completed [0ms, 7.77 MB]\n#&gt; + lib_spe_exp_int_pre dispatched\n#&gt; [2025-11-06 16:23:11.547973] [INFO] Preparing spectral libraries\n#&gt; [2025-11-06 16:23:11.553035] [WARN] Your input file does not exist, returning empty lib instead.\n#&gt; [2025-11-06 16:23:12.823166] [DEBUG] Exporting 1 spectra to: data/interim/libraries/spectra/exp/internal_pos.rds\n#&gt; [2025-11-06 16:23:12.824312] [TRACE] Successfully exported spectra\n#&gt; [2025-11-06 16:23:12.825585] [DEBUG] Exporting 1 spectra to: data/interim/libraries/spectra/exp/internal_neg.rds\n#&gt; [2025-11-06 16:23:12.826462] [TRACE] Successfully exported spectra\n#&gt; [2025-11-06 16:23:12.827188] [INFO] Exporting data to: data/interim/libraries/sop/internal_prepared.tsv.gz\n#&gt; [2025-11-06 16:23:12.827864] [DEBUG] Dimensions: 1 rows x 5 columns (5 variables)\n#&gt; [2025-11-06 16:23:12.828851] [INFO] Successfully exported 1 rows to data/interim/libraries/sop/internal_prepared.tsv.gz\n#&gt; [2025-11-06 16:23:12.832511] [DEBUG] Loading user parameters from: params/user/prepare_libraries_spectra.yaml\n#&gt; [2025-11-06 16:23:12.83377] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_spectra.yaml\n#&gt; [2025-11-06 16:23:12.937453] [INFO] Exporting parameters to: data/interim/params/251106_162312_prepare_libraries_spectra.yaml\n#&gt; [2025-11-06 16:23:12.938648] [DEBUG] Successfully exported 2 parameters\n#&gt; ✔ lib_spe_exp_int_pre completed [1.4s, 155 B]\n#&gt; + input_features dispatched\n#&gt; ✔ input_features completed [0ms, 451.55 kB]\n#&gt; + lib_rt_sop dispatched\n#&gt; ✔ lib_rt_sop completed [0ms, 96 B]\n#&gt; + lib_rt_rts dispatched\n#&gt; ✔ lib_rt_rts completed [1ms, 86 B]\n#&gt; + input_spectra dispatched\n#&gt; ✔ input_spectra completed [1ms, 7.77 MB]\n#&gt; + lib_spe_exp_int_pre_sop dispatched\n#&gt; ✔ lib_spe_exp_int_pre_sop completed [0ms, 106 B]\n#&gt; + lib_spe_exp_int_pre_pos dispatched\n#&gt; ✔ lib_spe_exp_int_pre_pos completed [1ms, 599 B]\n#&gt; + lib_spe_exp_int_pre_neg dispatched\n#&gt; ✔ lib_spe_exp_int_pre_neg completed [0ms, 599 B]\n#&gt; + fea_pre dispatched\n#&gt; [2025-11-06 16:23:15.789649] [DEBUG] Loading user parameters from: params/user/prepare_features_tables.yaml\n#&gt; [2025-11-06 16:23:15.791037] [DEBUG] Loading user-specified parameters from: params/user/prepare_features_tables.yaml\n#&gt; [2025-11-06 16:23:15.810947] [INFO] Preparing features table from: data/source/example_features.csv\n#&gt; [2025-11-06 16:23:15.81178] [DEBUG] Retaining top 1 intensity samples per feature\n#&gt; [2025-11-06 16:23:15.812724] [TRACE] Loading features table\n#&gt; [2025-11-06 16:23:15.821723] [DEBUG] Loaded 5328 features\n#&gt; [2025-11-06 16:23:15.822573] [TRACE] Formatting feature table\n#&gt; [2025-11-06 16:23:15.82344] [TRACE] Detecting format: MZmine ('Peak area' or ':area'), SLAW ('quant_'), or SIRIUS ('Peak height')\n#&gt; [2025-11-06 16:23:15.827174] [TRACE] Standardizing column names\n#&gt; [2025-11-06 16:23:15.828337] [TRACE] Filtering to top 1 intensity samples per feature\n#&gt; [2025-11-06 16:23:15.910376] [INFO] Prepared 5328 feature-sample pairs\n#&gt; [2025-11-06 16:23:15.914544] [DEBUG] Loading user parameters from: params/user/prepare_features_tables.yaml\n#&gt; [2025-11-06 16:23:15.91568] [DEBUG] Loading user-specified parameters from: params/user/prepare_features_tables.yaml\n#&gt; [2025-11-06 16:23:15.936457] [INFO] Exporting parameters to: data/interim/params/251106_162315_prepare_features_tables.yaml\n#&gt; [2025-11-06 16:23:15.937569] [DEBUG] Successfully exported 3 parameters\n#&gt; [2025-11-06 16:23:15.938563] [DEBUG] Created directory: data/interim/features\n#&gt; [2025-11-06 16:23:15.939414] [INFO] Exporting data to: data/interim/features/example_features.tsv.gz\n#&gt; [2025-11-06 16:23:15.940311] [DEBUG] Dimensions: 5328 rows x 5 columns (5 variables)\n#&gt; [2025-11-06 16:23:15.953544] [INFO] Successfully exported 5328 rows to data/interim/features/example_features.tsv.gz\n#&gt; ✔ fea_pre completed [168ms, 95.63 kB]\n#&gt; + fea_edg_spe dispatched\n#&gt; [2025-11-06 16:23:16.302451] [INFO] Creating spectral similarity network edges\n#&gt; [2025-11-06 16:23:16.303821] [DEBUG] Parameters - Threshold: 0.7, Method: gnps\n#&gt; [2025-11-06 16:23:16.304648] [DEBUG] Tolerances - PPM: 10, Dalton: 0.01\n#&gt; [2025-11-06 16:23:16.305361] [INFO] Importing spectra from: data/source/example_spectra.mgf\n#&gt; [2025-11-06 16:23:16.306104] [DEBUG] Parameters: cutoff=0, dalton=0.01, ppm=10, polarity=all\n#&gt; [2025-11-06 16:23:16.316898] [TRACE] Detected file format: mgf\n#&gt; [2025-11-06 16:23:16.317916] [DEBUG] Reading MGF file...\n#&gt; [2025-11-06 16:23:16.344968] [INFO] Reading MGF file (7.41 MB) with optimized parser: data/source/example_spectra.mgf\n#&gt; [2025-11-06 16:23:16.345747] [DEBUG] Using MS level: 2\n#&gt; [2025-11-06 16:23:18.20394] [INFO] Processed 10000 spectra...\n#&gt; [2025-11-06 16:23:19.617871] [INFO] Total spectra read: 16282\n#&gt; [2025-11-06 16:23:19.618921] [TRACE] Combining spectrum data into DataFrame\n#&gt; [2025-11-06 16:23:25.816991] [TRACE] Mapping field names to standard spectra variables\n#&gt; [2025-11-06 16:23:25.827203] [TRACE] Finalizing DataFrame structure\n#&gt; [2025-11-06 16:23:26.09954] [INFO] Loaded 16282 spectra from file\n#&gt; [2025-11-06 16:23:26.119645] [DEBUG] Filtered to MS2 spectra: 16282 -&gt; 12195 spectra\n#&gt; [2025-11-06 16:23:26.120678] [DEBUG] Combining replicate spectra by FEATURE_ID\n#&gt; [2025-11-06 16:23:28.580162] [DEBUG] Combined replicates: 12195 -&gt; 4087 spectra\n#&gt; [2025-11-06 16:23:28.581204] [DEBUG] Sanitizing spectra (cutoff=0)\n#&gt; [2025-11-06 16:23:28.582013] [INFO] Sanitizing 4087 spectra (cutoff: 0)\n#&gt; [2025-11-06 16:23:28.596043] [DEBUG] Removed 247 spectra with &lt;= 2 peaks\n#&gt; [2025-11-06 16:23:28.602926] [INFO] Sanitization complete: 3840/4087 spectra retained (94%, 247 removed)\n#&gt; [2025-11-06 16:23:28.603745] [DEBUG] Sanitization complete: 4087 -&gt; 3840 spectra\n#&gt; [2025-11-06 16:23:28.604429] [INFO] Import complete: 3840 spectra ready for analysis\n#&gt; [2025-11-06 16:23:28.605152] [TRACE] Performing spectral comparison\n#&gt; [2025-11-06 16:23:28.605796] [TRACE] As we do not limit the precursors delta,\n#&gt; expect a (relatively) long processing time.\n#&gt; [2025-11-06 16:23:28.607973] [INFO] ============================================\n#&gt; [2025-11-06 16:23:28.608976] [INFO] = Take yourself a break, you deserve it.   =\n#&gt; [2025-11-06 16:23:28.610116] [INFO] ============================================\n#&gt; [2025-11-06 16:23:28.61078] [DEBUG] Calculating 7370880 pairwise similarities\n#&gt;  ■                                  1% |  ETA: 10m\n#&gt;  ■                                  1% |  ETA: 10m\n#&gt;  ■                                  1% |  ETA: 10m\n#&gt;  ■■                                 2% |  ETA: 10m\n#&gt;  ■■                                 2% |  ETA: 10m\n#&gt;  ■■                                 3% |  ETA: 10m\n#&gt;  ■■                                 3% |  ETA: 10m\n#&gt;  ■■                                 4% |  ETA: 10m\n#&gt;  ■■                                 4% |  ETA: 10m\n#&gt;  ■■                                 5% |  ETA: 10m\n#&gt;  ■■■                                5% |  ETA: 10m\n#&gt;  ■■■                                6% |  ETA: 10m\n#&gt;  ■■■                                6% |  ETA: 10m\n#&gt;  ■■■                                7% |  ETA: 10m\n#&gt;  ■■■                                7% |  ETA: 10m\n#&gt;  ■■■                                8% |  ETA: 10m\n#&gt;  ■■■                                8% |  ETA: 10m\n#&gt;  ■■■■                               9% |  ETA: 10m\n#&gt;  ■■■■                               9% |  ETA:  9m\n#&gt;  ■■■■                              10% |  ETA:  9m\n#&gt;  ■■■■                              10% |  ETA:  9m\n#&gt;  ■■■■                              11% |  ETA:  9m\n#&gt;  ■■■■                              11% |  ETA:  9m\n#&gt;  ■■■■■                             12% |  ETA:  9m\n#&gt;  ■■■■■                             12% |  ETA:  9m\n#&gt;  ■■■■■                             13% |  ETA:  9m\n#&gt;  ■■■■■                             13% |  ETA:  9m\n#&gt;  ■■■■■                             14% |  ETA:  9m\n#&gt;  ■■■■■                             14% |  ETA:  9m\n#&gt;  ■■■■■                             15% |  ETA:  9m\n#&gt;  ■■■■■■                            15% |  ETA:  8m\n#&gt;  ■■■■■■                            16% |  ETA:  8m\n#&gt;  ■■■■■■                            17% |  ETA:  8m\n#&gt;  ■■■■■■                            17% |  ETA:  8m\n#&gt;  ■■■■■■                            18% |  ETA:  8m\n#&gt;  ■■■■■■                            18% |  ETA:  8m\n#&gt;  ■■■■■■■                           19% |  ETA:  8m\n#&gt;  ■■■■■■■                           20% |  ETA:  8m\n#&gt;  ■■■■■■■                           20% |  ETA:  8m\n#&gt;  ■■■■■■■                           21% |  ETA:  8m\n#&gt;  ■■■■■■■                           21% |  ETA:  8m\n#&gt;  ■■■■■■■■                          22% |  ETA:  8m\n#&gt;  ■■■■■■■■                          22% |  ETA:  7m\n#&gt;  ■■■■■■■■                          23% |  ETA:  7m\n#&gt;  ■■■■■■■■                          24% |  ETA:  7m\n#&gt;  ■■■■■■■■                          24% |  ETA:  7m\n#&gt;  ■■■■■■■■                          25% |  ETA:  7m\n#&gt;  ■■■■■■■■■                         25% |  ETA:  7m\n#&gt;  ■■■■■■■■■                         26% |  ETA:  7m\n#&gt;  ■■■■■■■■■                         26% |  ETA:  7m\n#&gt;  ■■■■■■■■■                         27% |  ETA:  7m\n#&gt;  ■■■■■■■■■                         28% |  ETA:  7m\n#&gt;  ■■■■■■■■■■                        28% |  ETA:  7m\n#&gt;  ■■■■■■■■■■                        29% |  ETA:  7m\n#&gt;  ■■■■■■■■■■                        30% |  ETA:  6m\n#&gt;  ■■■■■■■■■■                        30% |  ETA:  6m\n#&gt;  ■■■■■■■■■■                        31% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■                       32% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■                       32% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■                       33% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■                       34% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■                       34% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      35% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      36% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      37% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      37% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      38% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■                     39% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■                     40% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■                     41% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■                     41% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■                    42% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■                    43% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■                    44% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■                    45% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■                   46% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■                   46% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■                   47% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■                   48% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■                  49% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■                  50% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■                  51% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■                 52% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■                 53% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■                 54% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■■                55% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■                56% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■                58% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               59% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               60% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               61% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              62% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              64% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             65% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             66% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             68% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■            70% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■            71% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■           73% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■          75% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■          77% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■         80% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■        82% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■       85% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■      89% |  ETA: 40s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■     94% |  ETA: 19s\n#&gt; [2025-11-06 16:28:47.60916] [INFO] Created 9223 edges passing thresholds\n#&gt; [2025-11-06 16:28:47.610204] [TRACE] Calculating features' entropy\n#&gt; [2025-11-06 16:28:47.652862] [TRACE] Counting features' number of peaks\n#&gt; [2025-11-06 16:28:47.665298] [TRACE] Found spectrum IDs in field: FEATURE_ID\n#&gt; [2025-11-06 16:28:47.703685] [DEBUG] Loading user parameters from: params/user/create_edges_spectra.yaml\n#&gt; [2025-11-06 16:28:47.705455] [DEBUG] Loading user-specified parameters from: params/user/create_edges_spectra.yaml\n#&gt; [2025-11-06 16:28:47.734643] [INFO] Exporting parameters to: data/interim/params/251106_162847_create_edges_spectra.yaml\n#&gt; [2025-11-06 16:28:47.735844] [DEBUG] Successfully exported 4 parameters\n#&gt; [2025-11-06 16:28:47.736812] [INFO] Exporting data to: data/interim/features/example_edgesSpectra.tsv\n#&gt; [2025-11-06 16:28:47.737739] [DEBUG] Dimensions: 11577 rows x 6 columns (6 variables)\n#&gt; [2025-11-06 16:28:47.741107] [INFO] Successfully exported 11577 rows to data/interim/features/example_edgesSpectra.tsv\n#&gt; ✔ fea_edg_spe completed [5m 31.4s, 533.82 kB]\n#&gt; + lib_sop_mer dispatched\n#&gt; [2025-11-06 16:28:48.129248] [INFO] Preparing merged structure-organism pairs library\n#&gt; [2025-11-06 16:28:48.132394] [DEBUG] Filter mode: FALSE\n#&gt; [2025-11-06 16:28:48.133586] [INFO] Filtering by phylum: Streptophyta\n#&gt; [2025-11-06 16:28:48.134538] [TRACE] Loading and concatenating prepared libraries\n#&gt; [2025-11-06 16:29:05.090181] [INFO] Splitting concatenated SOP library into standardized components\n#&gt; [2025-11-06 16:29:05.091352] [DEBUG] Input table has 2100467 rows\n#&gt; [2025-11-06 16:29:10.092623] [TRACE] Sanitizing structures\n#&gt; [2025-11-06 16:29:10.094058] [INFO] Processing SMILES strings with RDKit\n#&gt; [2025-11-06 16:29:10.095428] [TRACE] Loading Python SMILES processor from: /usr/local/lib/R/site-library/tima/python/process_smiles.py\n#&gt; Downloading uv...Done!\n#&gt; Downloading cpython-3.12.12-linux-x86_64-gnu (download) (31.8MiB)\n#&gt;  Downloading cpython-3.12.12-linux-x86_64-gnu (download)\n#&gt; Downloading numpy (15.9MiB)\n#&gt; Downloading pillow (6.7MiB)\n#&gt; Downloading rdkit (34.5MiB)\n#&gt;  Downloading pillow\n#&gt;  Downloading numpy\n#&gt;  Downloading rdkit\n#&gt; Installed 3 packages in 28ms\n#&gt; [2025-11-06 16:29:14.67132] [DEBUG] Found 1500062 unique SMILES strings to process\n#&gt; [2025-11-06 16:29:20.716986] [DEBUG] Loaded 1681964 cached SMILES from cache\n#&gt; [2025-11-06 16:29:24.202511] [INFO] Processing 61 new SMILES with RDKit\n#&gt; [16:29:24] Explicit valence for atom # 1 N, 3, is greater than permitted\n#&gt; [16:29:24] ERROR: Could not sanitize molecule on line 1\n#&gt; [16:29:24] ERROR: Explicit valence for atom # 1 N, 3, is greater than permitted\n#&gt; [16:29:24] Explicit valence for atom # 0 P, 11, is greater than permitted\n#&gt; [16:29:24] ERROR: Could not sanitize molecule on line 2\n#&gt; [16:29:24] ERROR: Explicit valence for atom # 0 P, 11, is greater than permitted\n#&gt; [16:29:24] Explicit valence for atom # 21 N, 4, is greater than permitted\n#&gt; [16:29:24] ERROR: Could not sanitize molecule on line 3\n#&gt; [16:29:24] ERROR: Explicit valence for atom # 21 N, 4, is greater than permitted\n#&gt; [16:29:24] Explicit valence for atom # 1 Cl, 4, is greater than permitted\n#&gt; [16:29:24] ERROR: Could not sanitize molecule on line 4\n#&gt; [16:29:24] ERROR: Explicit valence for atom # 1 Cl, 4, is greater than permitted\n#&gt; [16:29:24] Explicit valence for atom # 6 C, 5, is greater than permitted\n#&gt; [16:29:24] ERROR: Could not sanitize molecule on line 5\n#&gt; [16:29:24] ERROR: Explicit valence for atom # 6 C, 5, is greater than permitted\n#&gt; [16:29:24] Explicit valence for atom # 18 S, 7, is greater than permitted\n#&gt; [16:29:24] ERROR: Could not sanitize molecule on line 6\n#&gt; [16:29:24] ERROR: Explicit valence for atom # 18 S, 7, is greater than permitted\n#&gt; [16:29:24] SMILES Parse Error: syntax error while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n#&gt; [16:29:24] SMILES Parse Error: check for mistakes around position 76:\n#&gt; [16:29:24] C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C\n#&gt; [16:29:24] ~~~~~~~~~~~~~~~~~~~~^\n#&gt; [16:29:24] SMILES Parse Error: extra open parentheses while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n#&gt; [16:29:24] SMILES Parse Error: check for mistakes around position 32:\n#&gt; [16:29:24] C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2\n#&gt; [16:29:24] ~~~~~~~~~~~~~~~~~~~~^\n#&gt; [16:29:24] SMILES Parse Error: extra open parentheses while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n#&gt; [16:29:24] SMILES Parse Error: check for mistakes around position 49:\n#&gt; [16:29:24] )\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2\n#&gt; [16:29:24] ~~~~~~~~~~~~~~~~~~~~^\n#&gt; [16:29:24] SMILES Parse Error: extra open parentheses while parsing: OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1\n#&gt; [16:29:24] SMILES Parse Error: check for mistakes around position 66:\n#&gt; [16:29:24] )=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(\n#&gt; [16:29:24] ~~~~~~~~~~~~~~~~~~~~^\n#&gt; [16:29:24] SMILES Parse Error: Failed parsing SMILES 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1' for input: 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n#&gt; [16:29:24] ERROR: Smiles parse error on line 7\n#&gt; [16:29:24] ERROR: Cannot create molecule from : 'OC1=CC=CC(=C1)C-1=C2\\CCC(=N2)\\C(=C2/N\\C(\\C=C2)=C(/C2=N/C(/C=C2)=C(\\C2=CC=C\\-1N2)C1=CC(O)=CC=C1)C1=CC(O)=CC=C1)\\C1=CC(O)=CC=C1'\n#&gt; [2025-11-06 16:29:24.300545] [INFO] Successfully processed 54 SMILES\n#&gt; [2025-11-06 16:29:49.328324] [INFO] Led to 877903 referenced structure-organism pairs\n#&gt; [2025-11-06 16:29:52.30555] [INFO] Corresponding to 393389 unique stereoisomers (excluding structures without stereochemistry)...\n#&gt; [2025-11-06 16:29:54.296358] [INFO] ... and 1007790 unique structures without stereochemistry...\n#&gt; [2025-11-06 16:29:54.376955] [INFO] ... or 1184311 unique constitutional isomers (ignoring stereochemistry)\n#&gt; [2025-11-06 16:30:15.313021] [INFO] ... among 36800 unique organisms\n#&gt; [2025-11-06 16:30:15.333057] [TRACE] Keeping keys\n#&gt; [2025-11-06 16:30:15.337541] [TRACE] Keeping organisms\n#&gt; [2025-11-06 16:30:15.338548] [TRACE] Completing organisms taxonomy\n#&gt; [2025-11-06 16:30:15.401385] [INFO] Processing 919 organism name(s) for OTT taxonomy lookup\n#&gt; [2025-11-06 16:30:15.412829] [DEBUG] Cleaned to 919 unique organism name(s)\n#&gt; [2025-11-06 16:30:15.413629] [DEBUG] Testing Open Tree of Life API availability\n#&gt; [2025-11-06 16:30:15.820112] [DEBUG] OTT API is available, proceeding with taxonomy queries\n#&gt; [2025-11-06 16:30:15.821529] [INFO] Querying OTT API in 10 batches\n#&gt; [2025-11-06 16:30:20.156633] [DEBUG] Initial taxonomy queries completed\n#&gt; [2025-11-06 16:30:20.164135] [DEBUG] Getting taxonomy\n#&gt; [2025-11-06 16:30:21.620518] [INFO] Taxonomy retrieved!\n#&gt; [2025-11-06 16:30:21.693901] [INFO] Got OTTaxonomy!\n#&gt; [2025-11-06 16:30:21.705881] [TRACE] Keeping structures\n#&gt; [2025-11-06 16:30:21.709995] [DEBUG] Loading user parameters from: params/user/prepare_libraries_sop_merged.yaml\n#&gt; [2025-11-06 16:30:21.711079] [DEBUG] Loading user-specified parameters from: params/user/prepare_libraries_sop_merged.yaml\n#&gt; [2025-11-06 16:30:21.731915] [INFO] Exporting parameters to: data/interim/params/251106_163021_prepare_libraries_sop_merged.yaml\n#&gt; [2025-11-06 16:30:21.732907] [DEBUG] Successfully exported 2 parameters\n#&gt; [2025-11-06 16:30:21.733737] [INFO] Exporting data to: data/interim/libraries/sop/merged/keys.tsv.gz\n#&gt; [2025-11-06 16:30:21.734544] [DEBUG] Dimensions: 877903 rows x 4 columns (4 variables)\n#&gt; [2025-11-06 16:30:21.735343] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:30:22.886588] [INFO] Successfully exported 877903 rows to data/interim/libraries/sop/merged/keys.tsv.gz\n#&gt; [2025-11-06 16:30:22.888117] [DEBUG] Created directory: data/interim/libraries/sop/merged/organisms/taxonomies\n#&gt; [2025-11-06 16:30:22.889076] [INFO] Exporting data to: data/interim/libraries/sop/merged/organisms/taxonomies/ott.tsv.gz\n#&gt; [2025-11-06 16:30:22.890029] [DEBUG] Dimensions: 35894 rows x 12 columns (12 variables)\n#&gt; [2025-11-06 16:30:22.976912] [INFO] Successfully exported 35894 rows to data/interim/libraries/sop/merged/organisms/taxonomies/ott.tsv.gz\n#&gt; [2025-11-06 16:30:22.97827] [INFO] Exporting data to: data/interim/libraries/sop/merged/structures/stereo.tsv.gz\n#&gt; [2025-11-06 16:30:22.979293] [DEBUG] Dimensions: 1404391 rows x 4 columns (4 variables)\n#&gt; [2025-11-06 16:30:22.980337] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:30:26.399078] [INFO] Successfully exported 1404391 rows to data/interim/libraries/sop/merged/structures/stereo.tsv.gz\n#&gt; [2025-11-06 16:30:26.400512] [INFO] Exporting data to: data/interim/libraries/sop/merged/structures/metadata.tsv.gz\n#&gt; [2025-11-06 16:30:26.401545] [DEBUG] Dimensions: 1458341 rows x 4 columns (4 variables)\n#&gt; [2025-11-06 16:30:26.402615] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:30:28.079818] [INFO] Successfully exported 1458341 rows to data/interim/libraries/sop/merged/structures/metadata.tsv.gz\n#&gt; [2025-11-06 16:30:28.081254] [INFO] Exporting data to: data/interim/libraries/sop/merged/structures/names.tsv.gz\n#&gt; [2025-11-06 16:30:28.082278] [DEBUG] Dimensions: 423199 rows x 2 columns (2 variables)\n#&gt; [2025-11-06 16:30:28.083401] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:30:28.682182] [INFO] Successfully exported 423199 rows to data/interim/libraries/sop/merged/structures/names.tsv.gz\n#&gt; [2025-11-06 16:30:28.683697] [DEBUG] Created directory: data/interim/libraries/sop/merged/structures/taxonomies\n#&gt; [2025-11-06 16:30:28.684738] [INFO] Exporting data to: data/interim/libraries/sop/merged/structures/taxonomies/classyfire.tsv.gz\n#&gt; [2025-11-06 16:30:28.685687] [DEBUG] Dimensions: 146393 rows x 6 columns (6 variables)\n#&gt; [2025-11-06 16:30:28.686721] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:30:28.834182] [INFO] Successfully exported 146393 rows to data/interim/libraries/sop/merged/structures/taxonomies/classyfire.tsv.gz\n#&gt; [2025-11-06 16:30:28.835468] [INFO] Exporting data to: data/interim/libraries/sop/merged/structures/taxonomies/npc.tsv.gz\n#&gt; [2025-11-06 16:30:28.836452] [DEBUG] Dimensions: 141818 rows x 4 columns (4 variables)\n#&gt; [2025-11-06 16:30:28.837511] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:30:29.129473] [INFO] Successfully exported 141818 rows to data/interim/libraries/sop/merged/structures/taxonomies/npc.tsv.gz\n#&gt; ✔ lib_sop_mer completed [1m 41s, 250 B]\n#&gt; + ann_spe_pos dispatched\n#&gt; [2025-11-06 16:30:30.303724] [INFO] Starting spectral annotation in pos mode\n#&gt; [2025-11-06 16:30:30.305074] [DEBUG] Method: gnps, Threshold: 0, PPM: 10, Dalton: 0.01\n#&gt; [2025-11-06 16:30:30.306414] [DEBUG] Processing 5 spectral library/libraries\n#&gt; [2025-11-06 16:30:30.30741] [INFO] Starting spectral annotation in pos mode\n#&gt; [2025-11-06 16:30:30.308329] [DEBUG] Similarity threshold: 0, method: gnps\n#&gt; [2025-11-06 16:30:30.309211] [DEBUG] Tolerances: 10 ppm, 0.01 Da\n#&gt; [2025-11-06 16:30:30.310882] [DEBUG] Filtered libraries by polarity: 5 -&gt; 5\n#&gt; [2025-11-06 16:30:30.311661] [TRACE] Loading query spectra from: data/source/example_spectra.mgf\n#&gt; [2025-11-06 16:30:30.312547] [INFO] Importing spectra from: data/source/example_spectra.mgf\n#&gt; [2025-11-06 16:30:30.313404] [DEBUG] Parameters: cutoff=0, dalton=0.01, ppm=10, polarity=pos\n#&gt; [2025-11-06 16:30:30.314532] [TRACE] Detected file format: mgf\n#&gt; [2025-11-06 16:30:30.315386] [DEBUG] Reading MGF file...\n#&gt; [2025-11-06 16:30:30.3162] [INFO] Reading MGF file (7.41 MB) with optimized parser: data/source/example_spectra.mgf\n#&gt; [2025-11-06 16:30:30.316935] [DEBUG] Using MS level: 2\n#&gt; [2025-11-06 16:30:32.062907] [INFO] Processed 10000 spectra...\n#&gt; [2025-11-06 16:30:33.346509] [INFO] Total spectra read: 16282\n#&gt; [2025-11-06 16:30:33.347567] [TRACE] Combining spectrum data into DataFrame\n#&gt; [2025-11-06 16:30:39.035551] [TRACE] Mapping field names to standard spectra variables\n#&gt; [2025-11-06 16:30:39.045731] [TRACE] Finalizing DataFrame structure\n#&gt; [2025-11-06 16:30:39.216195] [INFO] Loaded 16282 spectra from file\n#&gt; [2025-11-06 16:30:39.229] [DEBUG] Filtered to MS2 spectra: 16282 -&gt; 12195 spectra\n#&gt; [2025-11-06 16:30:39.235509] [DEBUG] Filtered to pos polarity: 12195 -&gt; 12195 spectra\n#&gt; [2025-11-06 16:30:39.236382] [DEBUG] Combining replicate spectra by FEATURE_ID\n#&gt; [2025-11-06 16:30:40.065044] [DEBUG] Combined replicates: 12195 -&gt; 4087 spectra\n#&gt; [2025-11-06 16:30:40.066125] [DEBUG] Sanitizing spectra (cutoff=0)\n#&gt; [2025-11-06 16:30:40.066965] [INFO] Sanitizing 4087 spectra (cutoff: 0)\n#&gt; [2025-11-06 16:30:40.184269] [DEBUG] Removed 247 spectra with &lt;= 2 peaks\n#&gt; [2025-11-06 16:30:40.192194] [INFO] Sanitization complete: 3840/4087 spectra retained (94%, 247 removed)\n#&gt; [2025-11-06 16:30:40.19313] [DEBUG] Sanitization complete: 4087 -&gt; 3840 spectra\n#&gt; [2025-11-06 16:30:40.19391] [INFO] Import complete: 3840 spectra ready for analysis\n#&gt; [2025-11-06 16:30:40.194655] [TRACE] Loading spectral libraries\n#&gt; [2025-11-06 16:30:40.1954] [INFO] Importing spectra from: data/interim/libraries/spectra/is/wikidata_5607185_pos.rds\n#&gt; [2025-11-06 16:30:40.196089] [DEBUG] Parameters: cutoff=0, dalton=0.01, ppm=10, polarity=pos\n#&gt; [2025-11-06 16:30:40.19695] [TRACE] Detected file format: rds\n#&gt; [2025-11-06 16:30:40.197639] [DEBUG] Reading RDS file...\n#&gt; [2025-11-06 16:30:59.525201] [INFO] Loaded 998198 spectra from file\n#&gt; [2025-11-06 16:31:00.263527] [DEBUG] Filtered to pos polarity: 998198 -&gt; 998198 spectra\n#&gt; [2025-11-06 16:31:00.265076] [INFO] Import complete: 998198 spectra ready for analysis\n#&gt; [2025-11-06 16:31:00.266154] [INFO] Importing spectra from: data/interim/libraries/spectra/exp/internal_pos.rds\n#&gt; [2025-11-06 16:31:00.266989] [DEBUG] Parameters: cutoff=0, dalton=0.01, ppm=10, polarity=pos\n#&gt; [2025-11-06 16:31:00.268008] [TRACE] Detected file format: rds\n#&gt; [2025-11-06 16:31:00.268767] [DEBUG] Reading RDS file...\n#&gt; [2025-11-06 16:31:00.269602] [INFO] Loaded 1 spectra from file\n#&gt; [2025-11-06 16:31:00.271692] [DEBUG] Filtered to pos polarity: 1 -&gt; 0 spectra\n#&gt; [2025-11-06 16:31:00.272457] [INFO] Import complete: 0 spectra ready for analysis\n#&gt; [2025-11-06 16:31:00.273195] [INFO] Importing spectra from: data/interim/libraries/spectra/exp/gnps_11566051_pos.rds\n#&gt; [2025-11-06 16:31:00.273866] [DEBUG] Parameters: cutoff=0, dalton=0.01, ppm=10, polarity=pos\n#&gt; [2025-11-06 16:31:00.274685] [TRACE] Detected file format: rds\n#&gt; [2025-11-06 16:31:00.275329] [DEBUG] Reading RDS file...\n#&gt; [2025-11-06 16:31:06.152904] [INFO] Loaded 354789 spectra from file\n#&gt; [2025-11-06 16:31:06.380569] [DEBUG] Filtered to pos polarity: 354789 -&gt; 354788 spectra\n#&gt; [2025-11-06 16:31:06.381785] [INFO] Import complete: 354788 spectra ready for analysis\n#&gt; [2025-11-06 16:31:06.382583] [INFO] Importing spectra from: data/interim/libraries/spectra/exp/massbank_2025051_pos.rds\n#&gt; [2025-11-06 16:31:06.383248] [DEBUG] Parameters: cutoff=0, dalton=0.01, ppm=10, polarity=pos\n#&gt; [2025-11-06 16:31:06.384122] [TRACE] Detected file format: rds\n#&gt; [2025-11-06 16:31:06.384828] [DEBUG] Reading RDS file...\n#&gt; [2025-11-06 16:31:07.125183] [INFO] Loaded 66388 spectra from file\n#&gt; [2025-11-06 16:31:07.176381] [DEBUG] Filtered to pos polarity: 66388 -&gt; 66388 spectra\n#&gt; [2025-11-06 16:31:07.17759] [INFO] Import complete: 66388 spectra ready for analysis\n#&gt; [2025-11-06 16:31:07.178384] [INFO] Importing spectra from: data/interim/libraries/spectra/exp/merlin_13911806_pos.rds\n#&gt; [2025-11-06 16:31:07.179122] [DEBUG] Parameters: cutoff=0, dalton=0.01, ppm=10, polarity=pos\n#&gt; [2025-11-06 16:31:07.180028] [TRACE] Detected file format: rds\n#&gt; [2025-11-06 16:31:07.180739] [DEBUG] Reading RDS file...\n#&gt; [2025-11-06 16:31:11.816506] [INFO] Loaded 208280 spectra from file\n#&gt; [2025-11-06 16:31:12.014221] [DEBUG] Filtered to pos polarity: 208280 -&gt; 208273 spectra\n#&gt; [2025-11-06 16:31:12.015544] [INFO] Import complete: 208273 spectra ready for analysis\n#&gt; [2025-11-06 16:31:26.677624] [INFO] Annotating using following libraries\n#&gt; [2025-11-06 16:31:29.800048] [INFO]          library spectra unique_connectivities\n#&gt;  ISDB - Wikidata  998198                998198\n#&gt;             gnps  354788                 22675\n#&gt;           merlin  208273                 26197\n#&gt;         massbank   66388                  5901\n#&gt; [2025-11-06 16:31:29.802073] [TRACE] Found spectrum IDs in field: FEATURE_ID\n#&gt; [2025-11-06 16:31:29.821374] [TRACE] Reducing library size\n#&gt; [2025-11-06 16:31:30.892534] [TRACE] Annotating\n#&gt; [2025-11-06 16:31:30.893729] [INFO] Calculating entropy and similarity for 3840 spectra\n#&gt; [2025-11-06 16:31:30.894678] [DEBUG] Parameters - Method: gnps, Dalton: 0.01, PPM: 10\n#&gt;  ■                                  1% |  ETA:  6m\n#&gt;  ■■                                 2% |  ETA:  6m\n#&gt;  ■■                                 3% |  ETA:  5m\n#&gt;  ■■                                 4% |  ETA:  5m\n#&gt;  ■■■                                5% |  ETA:  5m\n#&gt;  ■■■                                6% |  ETA:  5m\n#&gt;  ■■■                                8% |  ETA:  4m\n#&gt;  ■■■■                               9% |  ETA:  4m\n#&gt;  ■■■■                              10% |  ETA:  4m\n#&gt;  ■■■■                              12% |  ETA:  4m\n#&gt;  ■■■■■                             13% |  ETA:  4m\n#&gt;  ■■■■■                             14% |  ETA:  4m\n#&gt;  ■■■■■■                            15% |  ETA:  4m\n#&gt;  ■■■■■■                            17% |  ETA:  4m\n#&gt;  ■■■■■■                            18% |  ETA:  3m\n#&gt;  ■■■■■■■                           19% |  ETA:  3m\n#&gt;  ■■■■■■■                           20% |  ETA:  3m\n#&gt;  ■■■■■■■                           22% |  ETA:  3m\n#&gt;  ■■■■■■■■                          23% |  ETA:  3m\n#&gt;  ■■■■■■■■                          24% |  ETA:  3m\n#&gt;  ■■■■■■■■■                         25% |  ETA:  3m\n#&gt;  ■■■■■■■■■                         27% |  ETA:  3m\n#&gt;  ■■■■■■■■■                         28% |  ETA:  3m\n#&gt;  ■■■■■■■■■■                        29% |  ETA:  3m\n#&gt;  ■■■■■■■■■■                        30% |  ETA:  3m\n#&gt;  ■■■■■■■■■■                        31% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■                       33% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■                       34% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■                      36% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■                      37% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■                      38% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■                     39% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■                     40% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■                     41% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■                    43% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■                    44% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■                   45% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■                   46% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■                   48% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■                  49% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■                  51% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■                 52% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■                 54% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■                56% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■                58% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               59% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               61% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              63% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              64% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             66% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             68% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■            70% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■           72% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■           74% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■          75% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■          77% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■         79% |  ETA: 46s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■         80% |  ETA: 43s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■        82% |  ETA: 39s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■        84% |  ETA: 35s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■       85% |  ETA: 32s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■       87% |  ETA: 29s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■      88% |  ETA: 25s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■      90% |  ETA: 22s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■      92% |  ETA: 18s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■     93% |  ETA: 14s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■     95% |  ETA: 11s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■    97% |  ETA:  7s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■    98% |  ETA:  4s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% |  ETA:  1s\n#&gt; [2025-11-06 16:35:04.014194] [TRACE] Filtering results above threshold only\n#&gt; [2025-11-06 16:35:04.400985] [INFO] 321348 Candidates were annotated on 3679 features, with at least 0 similarity score.\n#&gt; [2025-11-06 16:35:04.405525] [DEBUG] Loading user parameters from: params/user/annotate_spectra.yaml\n#&gt; [2025-11-06 16:35:04.406511] [DEBUG] Loading user-specified parameters from: params/user/annotate_spectra.yaml\n#&gt; [2025-11-06 16:35:04.437524] [INFO] Exporting parameters to: data/interim/params/251106_163504_annotate_spectra.yaml\n#&gt; [2025-11-06 16:35:04.438528] [DEBUG] Successfully exported 4 parameters\n#&gt; [2025-11-06 16:35:04.439257] [INFO] Exporting data to: data/interim/annotations/example_spectralMatches_pos.tsv.gz\n#&gt; [2025-11-06 16:35:04.439944] [DEBUG] Dimensions: 629774 rows x 14 columns (14 variables)\n#&gt; [2025-11-06 16:35:04.440697] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:35:06.257866] [INFO] Successfully exported 629774 rows to data/interim/annotations/example_spectralMatches_pos.tsv.gz\n#&gt; ✔ ann_spe_pos completed [4m 36s, 37.54 MB]\n#&gt; + ann_spe_neg dispatched\n#&gt; [2025-11-06 16:35:08.032172] [INFO] Starting spectral annotation in neg mode\n#&gt; [2025-11-06 16:35:08.122104] [DEBUG] Method: gnps, Threshold: 0, PPM: 10, Dalton: 0.01\n#&gt; [2025-11-06 16:35:08.259825] [DEBUG] Processing 5 spectral library/libraries\n#&gt; [2025-11-06 16:35:08.379855] [INFO] Starting spectral annotation in neg mode\n#&gt; [2025-11-06 16:35:08.380911] [DEBUG] Similarity threshold: 0, method: gnps\n#&gt; [2025-11-06 16:35:08.381859] [DEBUG] Tolerances: 10 ppm, 0.01 Da\n#&gt; [2025-11-06 16:35:08.383593] [DEBUG] Filtered libraries by polarity: 5 -&gt; 5\n#&gt; [2025-11-06 16:35:08.384372] [TRACE] Loading query spectra from: data/source/example_spectra.mgf\n#&gt; [2025-11-06 16:35:08.385113] [INFO] Importing spectra from: data/source/example_spectra.mgf\n#&gt; [2025-11-06 16:35:08.385847] [DEBUG] Parameters: cutoff=0, dalton=0.01, ppm=10, polarity=neg\n#&gt; [2025-11-06 16:35:08.386745] [TRACE] Detected file format: mgf\n#&gt; [2025-11-06 16:35:08.387442] [DEBUG] Reading MGF file...\n#&gt; [2025-11-06 16:35:08.388203] [INFO] Reading MGF file (7.41 MB) with optimized parser: data/source/example_spectra.mgf\n#&gt; [2025-11-06 16:35:08.388935] [DEBUG] Using MS level: 2\n#&gt; [2025-11-06 16:35:10.066618] [INFO] Processed 10000 spectra...\n#&gt; [2025-11-06 16:35:11.33289] [INFO] Total spectra read: 16282\n#&gt; [2025-11-06 16:35:11.333915] [TRACE] Combining spectrum data into DataFrame\n#&gt; [2025-11-06 16:35:16.575305] [TRACE] Mapping field names to standard spectra variables\n#&gt; [2025-11-06 16:35:16.585383] [TRACE] Finalizing DataFrame structure\n#&gt; [2025-11-06 16:35:16.726326] [INFO] Loaded 16282 spectra from file\n#&gt; [2025-11-06 16:35:16.741895] [DEBUG] Filtered to MS2 spectra: 16282 -&gt; 12195 spectra\n#&gt; [2025-11-06 16:35:16.743241] [DEBUG] Filtered to neg polarity: 12195 -&gt; 0 spectra\n#&gt; [2025-11-06 16:35:16.744031] [DEBUG] Combining replicate spectra by FEATURE_ID\n#&gt; [2025-11-06 16:35:16.7477] [DEBUG] Combined replicates: 0 -&gt; 0 spectra\n#&gt; [2025-11-06 16:35:16.74842] [DEBUG] Sanitizing spectra (cutoff=0)\n#&gt; [2025-11-06 16:35:16.749139] [INFO] Sanitizing 0 spectra (cutoff: 0)\n#&gt; [2025-11-06 16:35:16.754239] [INFO] Sanitization complete: 0/0 spectra retained (NaN%, 0 removed)\n#&gt; [2025-11-06 16:35:16.75504] [DEBUG] Sanitization complete: 0 -&gt; 0 spectra\n#&gt; [2025-11-06 16:35:16.755749] [INFO] Import complete: 0 spectra ready for analysis\n#&gt; [2025-11-06 16:35:16.756426] [WARN] No spectra matched the given polarity, returning an empty dataframe\n#&gt; [2025-11-06 16:35:16.76035] [DEBUG] Loading user parameters from: params/user/annotate_spectra.yaml\n#&gt; [2025-11-06 16:35:16.761317] [DEBUG] Loading user-specified parameters from: params/user/annotate_spectra.yaml\n#&gt; [2025-11-06 16:35:16.79177] [INFO] Exporting parameters to: data/interim/params/251106_163516_annotate_spectra.yaml\n#&gt; [2025-11-06 16:35:16.792722] [DEBUG] Successfully exported 4 parameters\n#&gt; [2025-11-06 16:35:16.793452] [INFO] Exporting data to: data/interim/annotations/example_spectralMatches_neg.tsv.gz\n#&gt; [2025-11-06 16:35:16.794143] [DEBUG] Dimensions: 1 rows x 14 columns (14 variables)\n#&gt; [2025-11-06 16:35:16.795732] [INFO] Successfully exported 1 rows to data/interim/annotations/example_spectralMatches_neg.tsv.gz\n#&gt; ✔ ann_spe_neg completed [8.8s, 187 B]\n#&gt; + edg_spe dispatched\n#&gt; ✔ edg_spe completed [0ms, 533.82 kB]\n#&gt; + lib_mer_key dispatched\n#&gt; ✔ lib_mer_key completed [0ms, 18.06 MB]\n#&gt; + lib_mer_str_met dispatched\n#&gt; ✔ lib_mer_str_met completed [1ms, 36.02 MB]\n#&gt; + lib_mer_str_nam dispatched\n#&gt; ✔ lib_mer_str_nam completed [0ms, 11.28 MB]\n#&gt; + lib_mer_str_stereo dispatched\n#&gt; ✔ lib_mer_str_stereo completed [0ms, 43.58 MB]\n#&gt; + lib_mer_str_tax_cla dispatched\n#&gt; ✔ lib_mer_str_tax_cla completed [0ms, 2.51 MB]\n#&gt; + lib_mer_str_tax_npc dispatched\n#&gt; ✔ lib_mer_str_tax_npc completed [1ms, 2.44 MB]\n#&gt; + lib_mer_org_tax_ott dispatched\n#&gt; ✔ lib_mer_org_tax_ott completed [0ms, 939.13 kB]\n#&gt; + ann_ms1_pre dispatched\n#&gt; [2025-11-06 16:35:20.743995] [INFO] Starting mass-based annotation in pos mode\n#&gt; [2025-11-06 16:35:20.745163] [DEBUG] Tolerances: 10 ppm, 0.02 min RT\n#&gt; [2025-11-06 16:35:20.746025] [TRACE] Loading features table from: data/interim/features/example_features.tsv.gz\n#&gt; [2025-11-06 16:35:20.784684] [INFO] Processing 5328 features for annotation\n#&gt; [2025-11-06 16:35:42.938034] [TRACE] Rounding 2 columns to 5 decimal places\n#&gt; [2025-11-06 16:35:43.130975] [TRACE] Filtering desired adducts and adding mz tolerance\n#&gt; [2025-11-06 16:35:43.312284] [INFO] Already 2112 adducts previously detected\n#&gt; [2025-11-06 16:35:43.323803] [TRACE] Calculating rt tolerance\n#&gt; [2025-11-06 16:35:43.326133] [TRACE] Joining within given rt tolerance\n#&gt; [2025-11-06 16:35:43.378317] [INFO] Here are the top 10 observed m/z differences inside the RT windows:\n#&gt; [2025-11-06 16:35:43.379193] [INFO]              bin   N\n#&gt;  (4.8501,5.0366] 352\n#&gt;  (21.822,22.009] 283\n#&gt;   (16.973,17.16] 208\n#&gt;  (17.906,18.092] 192\n#&gt;  (15.854,16.041] 172\n#&gt;    (39.914,40.1] 143\n#&gt;  (38.981,39.168] 137\n#&gt;  (34.878,35.065] 115\n#&gt;  (77.962,78.148] 114\n#&gt;  (1.8659,2.0524] 108\n#&gt; [2025-11-06 16:35:43.380549] [INFO] These differences may help identify potential preprocessing issues\n#&gt; [2025-11-06 16:35:43.383765] [TRACE] Forming adducts and clusters\n#&gt; [2025-11-06 16:35:43.754784] [TRACE] Calculating delta mz for single charge adducts and clusters\n#&gt; [2025-11-06 16:35:43.777828] [TRACE] Joining within given delta mz tolerance (neutral losses)\n#&gt; [2025-11-06 16:35:43.787133] [TRACE] Joining within given delta mz tolerance (adducts)\n#&gt; [2025-11-06 16:35:43.803798] [TRACE] Keeping initial and destination feature\n#&gt; [2025-11-06 16:35:43.815012] [TRACE] Joining with initial results (adducts)\n#&gt; [2025-11-06 16:35:43.821868] [TRACE] Joining with initial results (neutral losses)\n#&gt; [2025-11-06 16:35:45.838196] [TRACE] Joining within given mz tol to exact mass library\n#&gt; [2025-11-06 16:35:45.855655] [TRACE] Keeping unique exact masses and molecular formulas\n#&gt; [2025-11-06 16:35:46.561062] [TRACE] Joining exact masses with single charge adducts\n#&gt; [2025-11-06 16:35:46.562026] [TRACE] Getting back to M\n#&gt; [2025-11-06 16:35:46.574117] [TRACE] Calculating multicharged and in source dimers\n#&gt; [2025-11-06 16:36:15.308821] [TRACE] Joining within given rt tolerance\n#&gt; [2025-11-06 16:36:15.444563] [TRACE] Joining within given mz tol and filtering possible adducts\n#&gt; [2025-11-06 16:36:15.470445] [TRACE] Joining single adducts, in source dimers, and multicharged\n#&gt; [2025-11-06 16:36:15.572624] [TRACE] Adding chemical classification\n#&gt; [2025-11-06 16:36:17.736434] [INFO] MS1 annotation results: 48099 unique structures annotated across 4224 features\n#&gt; [2025-11-06 16:36:17.746835] [DEBUG] Loading user parameters from: params/user/annotate_masses.yaml\n#&gt; [2025-11-06 16:36:17.748104] [DEBUG] Loading user-specified parameters from: params/user/annotate_masses.yaml\n#&gt; [2025-11-06 16:36:17.786118] [INFO] Exporting parameters to: data/interim/params/251106_163617_annotate_masses.yaml\n#&gt; [2025-11-06 16:36:17.787121] [DEBUG] Successfully exported 4 parameters\n#&gt; [2025-11-06 16:36:17.787849] [INFO] Exporting data to: data/interim/features/example_edgesMasses.tsv\n#&gt; [2025-11-06 16:36:17.788514] [DEBUG] Dimensions: 2653 rows x 3 columns (3 variables)\n#&gt; [2025-11-06 16:36:17.789766] [INFO] Successfully exported 2653 rows to data/interim/features/example_edgesMasses.tsv\n#&gt; [2025-11-06 16:36:17.79056] [INFO] Exporting data to: data/interim/annotations/example_ms1Prepared.tsv.gz\n#&gt; [2025-11-06 16:36:17.791205] [DEBUG] Dimensions: 187762 rows x 18 columns (18 variables)\n#&gt; [2025-11-06 16:36:17.791929] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:36:18.520089] [INFO] Successfully exported 187762 rows to data/interim/annotations/example_ms1Prepared.tsv.gz\n#&gt; ✔ ann_ms1_pre completed [57.8s, 157 B]\n#&gt; + ann_spe_exp_gnp_pre dispatched\n#&gt; [2025-11-06 16:36:19.468384] [INFO] Preparing GNPS annotations\n#&gt; [2025-11-06 16:36:19.47354] [WARN] No GNPS annotations found, returning an empty file instead\n#&gt; [2025-11-06 16:36:19.478552] [DEBUG] Loading user parameters from: params/user/prepare_annotations_gnps.yaml\n#&gt; [2025-11-06 16:36:19.479541] [DEBUG] Loading user-specified parameters from: params/user/prepare_annotations_gnps.yaml\n#&gt; [2025-11-06 16:36:19.493342] [INFO] Exporting parameters to: data/interim/params/251106_163619_prepare_annotations_gnps.yaml\n#&gt; [2025-11-06 16:36:19.494233] [DEBUG] Successfully exported 1 parameters\n#&gt; [2025-11-06 16:36:19.494955] [INFO] Exporting data to: data/interim/annotations/example_gnpsPrepared.tsv.gz\n#&gt; [2025-11-06 16:36:19.495635] [DEBUG] Dimensions: 1 rows x 21 columns (21 variables)\n#&gt; [2025-11-06 16:36:19.496653] [INFO] Successfully exported 1 rows to data/interim/annotations/example_gnpsPrepared.tsv.gz\n#&gt; ✔ ann_spe_exp_gnp_pre completed [29ms, 237 B]\n#&gt; + ann_spe_pre dispatched\n#&gt; [2025-11-06 16:36:19.876657] [INFO] Preparing spectral matching annotations from 2 file(s)\n#&gt; [2025-11-06 16:36:19.878023] [TRACE] Loading and formatting spectral matches\n#&gt; [2025-11-06 16:36:21.71935] [TRACE] Selecting and standardizing annotation columns\n#&gt; [2025-11-06 16:36:21.720282] [DEBUG] Input: 629774 rows, 22 columns\n#&gt; [2025-11-06 16:36:23.992585] [TRACE] Rounding 2 columns to 5 decimal places\n#&gt; [2025-11-06 16:36:24.49871] [TRACE] Complementing structural metadata from reference libraries\n#&gt; [2025-11-06 16:36:24.499663] [DEBUG] Input: 629774 rows\n#&gt; [2025-11-06 16:36:30.76468] [TRACE] Stereo loaded\n#&gt; [2025-11-06 16:36:34.917963] [TRACE] Metadata loaded\n#&gt; [2025-11-06 16:36:42.568259] [TRACE] Names loaded\n#&gt; [2025-11-06 16:36:42.790572] [TRACE] Classyfire done\n#&gt; [2025-11-06 16:36:43.011785] [TRACE] NPClassifier done\n#&gt; [2025-11-06 16:36:43.80803] [TRACE] Metadata done\n#&gt; [2025-11-06 16:36:43.901023] [TRACE] Names done\n#&gt; [2025-11-06 16:36:58.289268] [TRACE] Output: 629774 rows, 22 columns\n#&gt; [2025-11-06 16:36:58.293422] [DEBUG] Loading user parameters from: params/user/prepare_annotations_spectra.yaml\n#&gt; [2025-11-06 16:36:58.294335] [DEBUG] Loading user-specified parameters from: params/user/prepare_annotations_spectra.yaml\n#&gt; [2025-11-06 16:36:58.305938] [INFO] Exporting parameters to: data/interim/params/251106_163658_prepare_annotations_spectra.yaml\n#&gt; [2025-11-06 16:36:58.306806] [DEBUG] Successfully exported 1 parameters\n#&gt; [2025-11-06 16:36:58.307501] [INFO] Exporting data to: data/interim/annotations/example_spectralMatchesPrepared.tsv.gz\n#&gt; [2025-11-06 16:36:58.30813] [DEBUG] Dimensions: 629774 rows x 22 columns (22 variables)\n#&gt; [2025-11-06 16:36:58.308852] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:37:01.124656] [INFO] Successfully exported 629774 rows to data/interim/annotations/example_spectralMatchesPrepared.tsv.gz\n#&gt; ✔ ann_spe_pre completed [41.2s, 60.86 MB]\n#&gt; + ann_sir_pre dispatched\n#&gt; [2025-11-06 16:37:02.255774] [INFO] Preparing SIRIUS 6 annotations\n#&gt; [2025-11-06 16:37:02.260947] [DEBUG] SIRIUS directory: data/interim/annotations/example_sirius.zip\n#&gt; [2025-11-06 16:37:02.261755] [TRACE] Loading SIRIUS results...\n#&gt; [2025-11-06 16:37:02.262389] [TRACE] Loading and formatting SIRIUS results...\n#&gt; [2025-11-06 16:37:02.263029] [TRACE] Reading 'canopus_formula_summary_all.tsv' from SIRIUS archive: data/interim/annotations/example_sirius.zip\n#&gt; [2025-11-06 16:37:02.267497] [DEBUG] Extracting file: example_sirius/canopus_formula_summary_all.tsv\n#&gt; [2025-11-06 16:37:02.311738] [TRACE] Successfully read 40 rows from archive\n#&gt; [2025-11-06 16:37:02.313733] [TRACE] ... CANOPUS loaded\n#&gt; [2025-11-06 16:37:02.314394] [TRACE] Reading 'formula_identifications_all.tsv' from SIRIUS archive: data/interim/annotations/example_sirius.zip\n#&gt; [2025-11-06 16:37:02.318064] [DEBUG] Extracting file: example_sirius/formula_identifications_all.tsv\n#&gt; [2025-11-06 16:37:02.321743] [TRACE] Successfully read 277 rows from archive\n#&gt; [2025-11-06 16:37:02.323439] [TRACE] ... formulas loaded\n#&gt; [2025-11-06 16:37:02.324095] [TRACE] Reading 'structure_identifications_all.tsv' from SIRIUS archive: data/interim/annotations/example_sirius.zip\n#&gt; [2025-11-06 16:37:02.327674] [DEBUG] Extracting file: example_sirius/structure_identifications_all.tsv\n#&gt; [2025-11-06 16:37:02.338628] [TRACE] Successfully read 479 rows from archive\n#&gt; [2025-11-06 16:37:02.339291] [TRACE] ... structures loaded\n#&gt; [2025-11-06 16:37:02.341454] [TRACE] Reading 'denovo_structure_identifications_all.tsv' from SIRIUS archive: data/interim/annotations/example_sirius.zip\n#&gt; [2025-11-06 16:37:02.345001] [DEBUG] Extracting file: example_sirius/denovo_structure_identifications_all.tsv\n#&gt; [2025-11-06 16:37:02.367118] [TRACE] Successfully read 2138 rows from archive\n#&gt; [2025-11-06 16:37:02.367807] [TRACE] ... de novo loaded\n#&gt; [2025-11-06 16:37:02.369104] [TRACE] Selecting CANOPUS columns for SIRIUS v6\n#&gt; [2025-11-06 16:37:02.372032] [TRACE] ... CANOPUS prepared\n#&gt; [2025-11-06 16:37:02.376403] [TRACE] ... formulas prepared\n#&gt; [2025-11-06 16:37:02.381816] [TRACE] ... structures prepared\n#&gt; [2025-11-06 16:37:02.385525] [TRACE] ... denovo prepared\n#&gt; [2025-11-06 16:37:02.396503] [TRACE] Everything joined together\n#&gt; [2025-11-06 16:37:02.398911] [TRACE] Selecting and standardizing annotation columns\n#&gt; [2025-11-06 16:37:02.399626] [DEBUG] Input: 479 rows, 39 columns\n#&gt; [2025-11-06 16:37:02.42187] [TRACE] Rounding 2 columns to 5 decimal places\n#&gt; [2025-11-06 16:37:02.428533] [TRACE] Complementing structural metadata from reference libraries\n#&gt; [2025-11-06 16:37:02.429173] [DEBUG] Input: 479 rows\n#&gt; [2025-11-06 16:37:05.013368] [TRACE] Stereo loaded\n#&gt; [2025-11-06 16:37:09.406656] [TRACE] Metadata loaded\n#&gt; [2025-11-06 16:37:15.607255] [TRACE] Names loaded\n#&gt; [2025-11-06 16:37:15.81379] [TRACE] Classyfire done\n#&gt; [2025-11-06 16:37:16.035389] [TRACE] NPClassifier done\n#&gt; [2025-11-06 16:37:16.324636] [TRACE] Metadata done\n#&gt; [2025-11-06 16:37:16.411795] [TRACE] Names done\n#&gt; [2025-11-06 16:37:20.096315] [TRACE] Output: 479 rows, 39 columns\n#&gt; [2025-11-06 16:37:20.097218] [TRACE] Splitting SIRIUS results\n#&gt; [2025-11-06 16:37:21.431136] [DEBUG] Loading user parameters from: params/user/prepare_annotations_sirius.yaml\n#&gt; [2025-11-06 16:37:21.432342] [DEBUG] Loading user-specified parameters from: params/user/prepare_annotations_sirius.yaml\n#&gt; [2025-11-06 16:37:21.449109] [INFO] Exporting parameters to: data/interim/params/251106_163721_prepare_annotations_sirius.yaml\n#&gt; [2025-11-06 16:37:21.450078] [DEBUG] Successfully exported 2 parameters\n#&gt; [2025-11-06 16:37:21.450819] [INFO] Exporting data to: data/interim/annotations/example_canopusPrepared.tsv.gz\n#&gt; [2025-11-06 16:37:21.451504] [DEBUG] Dimensions: 14 rows x 13 columns (13 variables)\n#&gt; [2025-11-06 16:37:21.452635] [INFO] Successfully exported 14 rows to data/interim/annotations/example_canopusPrepared.tsv.gz\n#&gt; [2025-11-06 16:37:21.453434] [INFO] Exporting data to: data/interim/annotations/example_formulaPrepared.tsv.gz\n#&gt; [2025-11-06 16:37:21.454136] [DEBUG] Dimensions: 16 rows x 8 columns (8 variables)\n#&gt; [2025-11-06 16:37:21.455136] [INFO] Successfully exported 16 rows to data/interim/annotations/example_formulaPrepared.tsv.gz\n#&gt; [2025-11-06 16:37:21.455915] [INFO] Exporting data to: data/interim/annotations/example_siriusPrepared.tsv.gz\n#&gt; [2025-11-06 16:37:21.456606] [DEBUG] Dimensions: 479 rows x 21 columns (21 variables)\n#&gt; [2025-11-06 16:37:21.460521] [INFO] Successfully exported 479 rows to data/interim/annotations/example_siriusPrepared.tsv.gz\n#&gt; ✔ ann_sir_pre completed [19.2s, 165 B]\n#&gt; + tax_pre dispatched\n#&gt; [2025-11-06 16:37:22.510577] [INFO] Preparing taxonomic assignments for features\n#&gt; [2025-11-06 16:37:22.515539] [INFO] Using metadata for organism assignments\n#&gt; [2025-11-06 16:37:22.516372] [TRACE] Loading feature table\n#&gt; [2025-11-06 16:37:22.554021] [TRACE] Loading metadata table\n#&gt; [2025-11-06 16:37:22.555137] [TRACE] Preparing organisms names\n#&gt; [2025-11-06 16:37:22.563516] [TRACE] Retrieving already computed Open Tree of Life Taxonomy\n#&gt; [2025-11-06 16:37:22.682608] [TRACE] Submitting the rest to OTL\n#&gt; [2025-11-06 16:37:22.684851] [INFO] Processing 2 organism name(s) for OTT taxonomy lookup\n#&gt; [2025-11-06 16:37:22.693635] [DEBUG] Cleaned to 2 unique organism name(s)\n#&gt; [2025-11-06 16:37:22.694317] [DEBUG] Testing Open Tree of Life API availability\n#&gt; [2025-11-06 16:37:22.934745] [DEBUG] OTT API is available, proceeding with taxonomy queries\n#&gt; [2025-11-06 16:37:22.935901] [INFO] Querying OTT API in 1 batches\n#&gt; [2025-11-06 16:37:23.167034] [DEBUG] Initial taxonomy queries completed\n#&gt; [2025-11-06 16:37:23.176995] [INFO] Retrying with blk\n#&gt; [2025-11-06 16:37:23.40037] [DEBUG] Getting taxonomy\n#&gt; [2025-11-06 16:37:23.520385] [INFO] Taxonomy retrieved!\n#&gt; [2025-11-06 16:37:23.544999] [INFO] Got OTTaxonomy!\n#&gt; [2025-11-06 16:37:23.545738] [TRACE] Joining all results\n#&gt; [2025-11-06 16:37:23.54843] [TRACE] Joining with metadata table\n#&gt; [2025-11-06 16:37:23.552086] [TRACE] Joining with cleaned taxonomy table\n#&gt; [2025-11-06 16:37:23.945712] [DEBUG] Loading user parameters from: params/user/prepare_taxa.yaml\n#&gt; [2025-11-06 16:37:23.946792] [DEBUG] Loading user-specified parameters from: params/user/prepare_taxa.yaml\n#&gt; [2025-11-06 16:37:23.972933] [INFO] Exporting parameters to: data/interim/params/251106_163723_prepare_taxa.yaml\n#&gt; [2025-11-06 16:37:23.973849] [DEBUG] Successfully exported 3 parameters\n#&gt; [2025-11-06 16:37:23.97464] [DEBUG] Created directory: data/interim/taxa\n#&gt; [2025-11-06 16:37:23.975327] [INFO] Exporting data to: data/interim/taxa/example_taxed.tsv.gz\n#&gt; [2025-11-06 16:37:23.975993] [DEBUG] Dimensions: 5328 rows x 12 columns (12 variables)\n#&gt; [2025-11-06 16:37:23.982286] [INFO] Successfully exported 5328 rows to data/interim/taxa/example_taxed.tsv.gz\n#&gt; ✔ tax_pre completed [1.5s, 19.70 kB]\n#&gt; + ann_ms1_pre_edg dispatched\n#&gt; ✔ ann_ms1_pre_edg completed [0ms, 81.71 kB]\n#&gt; + ann_ms1_pre_ann dispatched\n#&gt; ✔ ann_ms1_pre_ann completed [0ms, 10.81 MB]\n#&gt; + ann_sir_pre_can dispatched\n#&gt; ✔ ann_sir_pre_can completed [0ms, 784 B]\n#&gt; + ann_sir_pre_for dispatched\n#&gt; ✔ ann_sir_pre_for completed [0ms, 487 B]\n#&gt; + ann_sir_pre_str dispatched\n#&gt; ✔ ann_sir_pre_str completed [0ms, 24.42 kB]\n#&gt; + fea_edg_pre dispatched\n#&gt; [2025-11-06 16:37:26.319231] [INFO] Preparing molecular network edges\n#&gt; [2025-11-06 16:37:26.320231] [DEBUG] MS1 edges: data/interim/features/example_edgesMasses.tsv\n#&gt; [2025-11-06 16:37:26.320902] [DEBUG] Spectral edges: data/interim/features/example_edgesSpectra.tsv\n#&gt; [2025-11-06 16:37:26.321557] [TRACE] Loading edge tables\n#&gt; [2025-11-06 16:37:26.329556] [DEBUG] MS1 edges: 2653 rows\n#&gt; [2025-11-06 16:37:26.330243] [DEBUG] Spectral edges: 11577 rows\n#&gt; [2025-11-06 16:37:26.332698] [TRACE] Extracted entropy for 3840 features\n#&gt; [2025-11-06 16:37:26.333335] [TRACE] Combining and formatting edge tables\n#&gt; [2025-11-06 16:37:26.358338] [INFO] Prepared 17751 total edges\n#&gt; [2025-11-06 16:37:26.361894] [DEBUG] Loading user parameters from: params/user/prepare_features_edges.yaml\n#&gt; [2025-11-06 16:37:26.362722] [DEBUG] Loading user-specified parameters from: params/user/prepare_features_edges.yaml\n#&gt; [2025-11-06 16:37:26.37772] [INFO] Exporting parameters to: data/interim/params/251106_163726_prepare_features_edges.yaml\n#&gt; [2025-11-06 16:37:26.378572] [DEBUG] Successfully exported 2 parameters\n#&gt; [2025-11-06 16:37:26.379241] [INFO] Exporting data to: data/interim/features/example_edges.tsv\n#&gt; [2025-11-06 16:37:26.379877] [DEBUG] Dimensions: 17751 rows x 7 columns (7 variables)\n#&gt; [2025-11-06 16:37:26.38308] [INFO] Successfully exported 17751 rows to data/interim/features/example_edges.tsv\n#&gt; ✔ fea_edg_pre completed [65ms, 758.28 kB]\n#&gt; + ann_fil dispatched\n#&gt; [2025-11-06 16:37:26.758562] [INFO] Filtering annotations\n#&gt; [2025-11-06 16:37:26.759858] [DEBUG] RT tolerance: Inf minutes\n#&gt; [2025-11-06 16:37:26.760748] [TRACE] ... features\n#&gt; [2025-11-06 16:37:26.799598] [INFO] Processing 5328 unique features for annotation filtering\n#&gt; [2025-11-06 16:37:26.800309] [DEBUG] Loading 4 annotation file(s)\n#&gt; [2025-11-06 16:37:32.304172] [INFO] Removing MS1 annotations superseded by spectral matches\n#&gt; [2025-11-06 16:37:33.526496] [DEBUG] Found 630254 spectral annotations\n#&gt; [2025-11-06 16:37:35.025855] [INFO] Removed 79186 redundant MS1 annotations\n#&gt; [2025-11-06 16:37:35.026879] [INFO] Total annotations before RT filtering: 738830\n#&gt; [2025-11-06 16:37:35.027579] [TRACE] Loading retention time library\n#&gt; [2025-11-06 16:37:35.060889] [DEBUG] Loaded 1 retention time standards\n#&gt; [2025-11-06 16:37:36.401211] [INFO] Filtering annotations outside Inf min RT tolerance\n#&gt; [2025-11-06 16:37:40.176728] [INFO] Removed 0 annotations based on retention time tolerance\n#&gt; [2025-11-06 16:37:40.406331] [DEBUG] Loading user parameters from: params/user/filter_annotations.yaml\n#&gt; [2025-11-06 16:37:40.407441] [DEBUG] Loading user-specified parameters from: params/user/filter_annotations.yaml\n#&gt; [2025-11-06 16:37:40.425098] [INFO] Exporting parameters to: data/interim/params/251106_163740_filter_annotations.yaml\n#&gt; [2025-11-06 16:37:40.425991] [DEBUG] Successfully exported 2 parameters\n#&gt; [2025-11-06 16:37:40.426702] [INFO] Exporting data to: data/interim/annotations/example_annotationsFiltered.tsv.gz\n#&gt; [2025-11-06 16:37:40.427336] [DEBUG] Dimensions: 739349 rows x 28 columns (28 variables)\n#&gt; [2025-11-06 16:37:40.428057] [DEBUG] Large dataset detected, export may take some time...\n#&gt; [2025-11-06 16:37:43.39145] [INFO] Successfully exported 739349 rows to data/interim/annotations/example_annotationsFiltered.tsv.gz\n#&gt; ✔ ann_fil completed [16.6s, 56.07 MB]\n#&gt; + fea_com dispatched\n#&gt; [2025-11-06 16:37:44.183209] [INFO] Creating components from 1 edge file(s)\n#&gt; [2025-11-06 16:37:44.185645] [TRACE] Loading edge data\n#&gt; [2025-11-06 16:37:44.200878] [INFO] Loaded 15234 edges connecting 5909 unique features\n#&gt; [2025-11-06 16:37:44.201619] [TRACE] Building graph structure\n#&gt; [2025-11-06 16:37:44.205169] [TRACE] Identifying connected components\n#&gt; [2025-11-06 16:37:44.212654] [INFO] Found 2513 components\n#&gt; [2025-11-06 16:37:44.21334] [TRACE] Formatting component assignments\n#&gt; [2025-11-06 16:37:44.244143] [INFO] Component sizes - Min: 1, Max: 1586, Mean: 2.4\n#&gt; [2025-11-06 16:37:44.247699] [DEBUG] Loading user parameters from: params/user/create_components.yaml\n#&gt; [2025-11-06 16:37:44.248533] [DEBUG] Loading user-specified parameters from: params/user/create_components.yaml\n#&gt; [2025-11-06 16:37:44.259786] [INFO] Exporting parameters to: data/interim/params/251106_163744_create_components.yaml\n#&gt; [2025-11-06 16:37:44.260645] [DEBUG] Successfully exported 1 parameters\n#&gt; [2025-11-06 16:37:44.261324] [INFO] Exporting data to: data/interim/features/example_components.tsv\n#&gt; [2025-11-06 16:37:44.261986] [DEBUG] Dimensions: 5909 rows x 2 columns (2 variables)\n#&gt; [2025-11-06 16:37:44.263181] [INFO] Successfully exported 5909 rows to data/interim/features/example_components.tsv\n#&gt; [2025-11-06 16:37:44.263906] [INFO] Components written to: data/interim/features/example_components.tsv\n#&gt; ✔ fea_com completed [82ms, 51.39 kB]\n#&gt; + int_com dispatched\n#&gt; ✔ int_com completed [0ms, 51.39 kB]\n#&gt; + fea_com_pre dispatched\n#&gt; [2025-11-06 16:37:45.039426] [INFO] Preparing molecular network components from 1 file(s)\n#&gt; [2025-11-06 16:37:45.040545] [TRACE] Loading component tables\n#&gt; [2025-11-06 16:37:45.043342] [DEBUG] Loaded 5909 component assignments\n#&gt; [2025-11-06 16:37:45.045739] [INFO] Prepared 5909 unique feature-component assignments\n#&gt; [2025-11-06 16:37:45.04943] [DEBUG] Loading user parameters from: params/user/prepare_features_components.yaml\n#&gt; [2025-11-06 16:37:45.050299] [DEBUG] Loading user-specified parameters from: params/user/prepare_features_components.yaml\n#&gt; [2025-11-06 16:37:45.061911] [INFO] Exporting parameters to: data/interim/params/251106_163745_prepare_features_components.yaml\n#&gt; [2025-11-06 16:37:45.062766] [DEBUG] Successfully exported 1 parameters\n#&gt; [2025-11-06 16:37:45.063439] [INFO] Exporting data to: data/interim/features/example_componentsPrepared.tsv\n#&gt; [2025-11-06 16:37:45.064083] [DEBUG] Dimensions: 5909 rows x 2 columns (2 variables)\n#&gt; [2025-11-06 16:37:45.065312] [INFO] Successfully exported 5909 rows to data/interim/features/example_componentsPrepared.tsv\n#&gt; ✔ fea_com_pre completed [27ms, 51.38 kB]\n#&gt; + ann_pre dispatched\n#&gt; [2025-11-06 16:37:45.446472] [INFO] Starting annotation weighting and scoring\n#&gt; [2025-11-06 16:37:45.447601] [DEBUG] Weights - Spectral: 0.333, Chemical: 0.166, Biological: 0.5\n#&gt; [2025-11-06 16:37:45.448317] [DEBUG] Candidates - Neighbors: 16, Final: 1\n#&gt; [2025-11-06 16:37:45.448991] [TRACE] Loading input files...\n#&gt; [2025-11-06 16:37:45.449615] [TRACE] ... components\n#&gt; [2025-11-06 16:37:45.451669] [TRACE] ... edges\n#&gt; [2025-11-06 16:37:45.475719] [TRACE] ... structure-organism pairs\n#&gt; [2025-11-06 16:37:57.533539] [TRACE] ... canopus\n#&gt; [2025-11-06 16:37:57.564666] [TRACE] ... formula\n#&gt; [2025-11-06 16:37:57.594235] [TRACE] ... annotations\n#&gt; [2025-11-06 16:38:06.256862] [TRACE] Initial annotations:\n#&gt; [2025-11-06 16:38:06.815023] [INFO]  candidate_library      n\n#&gt;    ISDB - Wikidata 577751\n#&gt;           TIMA MS1  82324\n#&gt;               gnps  25161\n#&gt;             merlin  23150\n#&gt;           massbank   3591\n#&gt;             SIRIUS    479\n#&gt; [2025-11-06 16:38:06.843506] [TRACE] Re-arranging annotations\n#&gt; [2025-11-06 16:38:14.283116] [TRACE] Adding biological organism metadata\n#&gt; [2025-11-06 16:38:16.645682] [TRACE] Performing taxonomically informed scoring\n#&gt; [2025-11-06 16:38:16.647556] [INFO] Weighting 695299 annotations by biological source\n#&gt; [2025-11-06 16:38:16.648357] [INFO] Weighting annotations by biological source\n#&gt; [2025-11-06 16:38:16.649061] [DEBUG] Weights - Spectral: 0.333, Biological: 0.5\n#&gt; [2025-11-06 16:38:16.649736] [TRACE] Filtering structure-organism pairs\n#&gt; [2025-11-06 16:38:17.509785] [DEBUG] Filtered to 628630 structure-organism pairs\n#&gt; [2025-11-06 16:38:17.51079] [TRACE] Preparing annotation table\n#&gt; [2025-11-06 16:38:18.762617] [TRACE] Calculating biological score at all levels ...\n#&gt; [2025-11-06 16:38:18.763611] [TRACE] ... domain\n#&gt; [2025-11-06 16:38:18.774565] [TRACE] ... kingdom\n#&gt; [2025-11-06 16:38:18.784593] [TRACE] ... phylum\n#&gt; [2025-11-06 16:38:18.79463] [TRACE] ... class\n#&gt; [2025-11-06 16:38:18.805388] [TRACE] ... order\n#&gt; [2025-11-06 16:38:18.820871] [TRACE] ... family\n#&gt; [2025-11-06 16:38:18.846052] [TRACE] ... tribe\n#&gt; [2025-11-06 16:38:18.86245] [TRACE] ... genus\n#&gt; [2025-11-06 16:38:18.940006] [TRACE] ... species\n#&gt; [2025-11-06 16:38:19.218287] [TRACE] ... varietas\n#&gt; [2025-11-06 16:38:19.230134] [TRACE] Keeping best biological score\n#&gt; [2025-11-06 16:38:23.514825] [INFO] Taxonomically informed metabolite annotation reranked:\n#&gt; Kingdom level: 41238 structures\n#&gt; Phylum level:  40784 structures\n#&gt; Class level:   35057 structures\n#&gt; Order level:   9353 structures\n#&gt; Family level:  7515 structures\n#&gt; Tribe level:   1184 structures\n#&gt; Genus level:   919 structures\n#&gt; Species level: 402 structures\n#&gt; Variety level: 0 structures\n#&gt; [2025-11-06 16:38:23.515842] [TRACE] Extracting distinct structure-taxonomy pairs\n#&gt; [2025-11-06 16:38:23.840894] [TRACE] Calculating chemical consistency for features with at least 2 neighbors\n#&gt; [2025-11-06 16:38:23.872996] [DEBUG] Found 7933 valid edges for consistency calculation\n#&gt; [2025-11-06 16:38:24.046694] [TRACE] Calculating consistency scores across network edges\n#&gt; [2025-11-06 16:38:24.047712] [TRACE] ... at the (classyfire) kingdom level\n#&gt; [2025-11-06 16:38:24.187027] [TRACE] ... at the (NPC) pathway level\n#&gt; [2025-11-06 16:38:24.414587] [TRACE] ... at the (classyfire) superclass level\n#&gt; [2025-11-06 16:38:24.645141] [TRACE] ... at the (NPC) superclass level\n#&gt; [2025-11-06 16:38:25.016037] [TRACE] ... at the (classyfire) class level\n#&gt; [2025-11-06 16:38:25.88424] [TRACE] ... at the (NPC) class level\n#&gt; [2025-11-06 16:38:26.776054] [TRACE] ... at the (classyfire) parent level\n#&gt; [2025-11-06 16:38:27.53066] [TRACE] Splitting already computed predictions\n#&gt; [2025-11-06 16:38:30.247972] [TRACE] Joining all except -1 together\n#&gt; [2025-11-06 16:38:38.199537] [TRACE] Adding already computed predictions back\n#&gt; [2025-11-06 16:38:44.180403] [INFO] Weighting 695285 annotations by chemical consistency\n#&gt; [2025-11-06 16:38:44.181441] [DEBUG] Weights - spectral: 0.333, biological: 0.5, chemical: 0.166\n#&gt; [2025-11-06 16:38:44.182207] [DEBUG] Weights - Spectral: 0.333, Biological: 0.5, Chemical: 0.166\n#&gt; [2025-11-06 16:38:44.36175] [TRACE] Calculating chemical score at all levels ...\n#&gt; [2025-11-06 16:38:44.362866] [TRACE] ... (classyfire) kingdom\n#&gt; [2025-11-06 16:38:44.37149] [TRACE] ... (NPC) pathway\n#&gt; [2025-11-06 16:38:44.385569] [TRACE] ... (classyfire) superclass\n#&gt; [2025-11-06 16:38:44.401446] [TRACE] ... (NPC) superclass\n#&gt; [2025-11-06 16:38:44.423727] [TRACE] ... (classyfire) class\n#&gt; [2025-11-06 16:38:44.450445] [TRACE] ... (NPC) class\n#&gt; [2025-11-06 16:38:44.496242] [TRACE] ... (classyfire) parent\n#&gt; [2025-11-06 16:38:44.547554] [TRACE] ... keeping best chemical score\n#&gt; [2025-11-06 16:38:45.5739] [INFO] Chemically informed scoring reranked:\n#&gt; Classyfire hierarchy:\n#&gt;   Kingdom level:    40464 structures\n#&gt;   Superclass level: 28047 structures\n#&gt;   Class level:      28047 structures\n#&gt;   Parent level:     23927 structures\n#&gt; NPClassifier hierarchy:\n#&gt;   Pathway level:    31334 structures\n#&gt;   Superclass level: 23948 structures\n#&gt;   Class level:      23948 structures\n#&gt; (Note: WITHOUT consistency score filtering - for later predictions)\n#&gt; [2025-11-06 16:38:45.575002] [INFO] Cleaning chemically weighted annotations\n#&gt; [2025-11-06 16:38:45.575656] [DEBUG] Keeping top 1 candidates per feature\n#&gt; [2025-11-06 16:38:45.576269] [DEBUG] Using best_percentile: 0.9 for consistent filtering\n#&gt; [2025-11-06 16:38:45.5769] [DEBUG] Options - High confidence: TRUE, Remove ties: FALSE, Summarize: FALSE\n#&gt; [2025-11-06 16:38:45.577524] [INFO] Filtering top 1 candidates and keeping only MS1 candidates with minimum 0 biological score OR 0 chemical score \n#&gt; [2025-11-06 16:38:54.434458] [TRACE] Minimizing annotation results\n#&gt; [2025-11-06 16:38:54.435379] [DEBUG] Thresholds - Classes: 0.6, Compounds: 0.4, Percentile: 0.9\n#&gt; [2025-11-06 16:38:58.058347] [TRACE] Filtering for high-confidence candidates\n#&gt; [2025-11-06 16:38:58.066586] [TRACE] Applying RT error filter (max: 0.1 min)\n#&gt; [2025-11-06 16:38:58.070309] [INFO] Removed 693032 low-confidence candidates (99.7% of 695221 total)\n#&gt; [2025-11-06 16:38:58.071098] [INFO] 2189 high-confidence candidates remaining (0.3%)\n#&gt; [2025-11-06 16:38:58.072721] [TRACE] Applying 0.9 percentile filter to match minimize_results\n#&gt; [2025-11-06 16:38:58.081189] [TRACE] Processing full results\n#&gt; [2025-11-06 16:38:58.081903] [INFO] Summarizing annotation results\n#&gt; [2025-11-06 16:38:58.082525] [DEBUG] Options - Remove ties: FALSE, Summarize: FALSE\n#&gt; [2025-11-06 16:38:58.08313] [TRACE] Adding feature metadata and simplifying columns\n#&gt; [2025-11-06 16:39:02.881282] [TRACE] Selecting columns to export\n#&gt; [2025-11-06 16:39:02.958586] [TRACE] Adding consensus again to droped candidates\n#&gt; [2025-11-06 16:39:09.478089] [TRACE] Processing filtered results\n#&gt; [2025-11-06 16:39:09.479119] [INFO] Summarizing annotation results\n#&gt; [2025-11-06 16:39:09.480039] [DEBUG] Options - Remove ties: FALSE, Summarize: FALSE\n#&gt; [2025-11-06 16:39:09.480923] [TRACE] Adding feature metadata and simplifying columns\n#&gt; [2025-11-06 16:39:13.472916] [TRACE] Selecting columns to export\n#&gt; [2025-11-06 16:39:13.535261] [TRACE] Adding consensus again to droped candidates\n#&gt; [2025-11-06 16:39:21.365992] [DEBUG] Created directory: data/processed/20251106_163921_example\n#&gt; [2025-11-06 16:39:21.367046] [INFO] Exporting parameters to: data/processed/20251106_163921_example/251106_163921_prepare_params.yaml\n#&gt; [2025-11-06 16:39:21.368116] [DEBUG] Successfully exported 4 parameters\n#&gt; [2025-11-06 16:39:21.387991] [INFO] Exporting parameters to: data/processed/20251106_163921_example/251106_163921_prepare_params_advanced.yaml\n#&gt; [2025-11-06 16:39:21.389356] [DEBUG] Successfully exported 11 parameters\n#&gt; [2025-11-06 16:39:21.390207] [INFO] Exporting data to: data/processed/20251106_163921_example/example_results_mini.tsv\n#&gt; [2025-11-06 16:39:21.391059] [DEBUG] Dimensions: 5328 rows x 16 columns (16 variables)\n#&gt; [2025-11-06 16:39:21.394809] [INFO] Successfully exported 5328 rows to data/processed/20251106_163921_example/example_results_mini.tsv\n#&gt; [2025-11-06 16:39:21.395721] [INFO] Exporting data to: data/processed/20251106_163921_example/example_results_filtered.tsv\n#&gt; [2025-11-06 16:39:21.396583] [DEBUG] Dimensions: 5835 rows x 59 columns (59 variables)\n#&gt; [2025-11-06 16:39:21.402602] [INFO] Successfully exported 5835 rows to data/processed/20251106_163921_example/example_results_filtered.tsv\n#&gt; [2025-11-06 16:39:21.40351] [INFO] Exporting data to: data/processed/20251106_163921_example/example_results.tsv\n#&gt; [2025-11-06 16:39:21.404353] [DEBUG] Dimensions: 6652 rows x 59 columns (59 variables)\n#&gt; [2025-11-06 16:39:21.411075] [INFO] Successfully exported 6652 rows to data/processed/20251106_163921_example/example_results.tsv\n#&gt; ✔ ann_pre completed [1m 36s, 3.97 MB]\n#&gt; ✔ ended pipeline [21m 48s, 126 completed, 0 skipped]\n#&gt; There were 14 warnings (use warnings() to see them)\n#&gt; Processing complete. Total molecules processed: 54\n\nThe final exported file is formatted in order to be easily imported in Cytoscape to further explore your data!\nWe hope you enjoyed using TIMA and are pleased to hear from you!\nFor any remark or suggestion, please fill an issue or feel free to contact us directly.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{rutz2025,\n  author = {Rutz, Adriano},\n  title = {3 {Performing} {Taxonomically} {Informed} {Metabolite}\n    {Annotation}},\n  date = {2025-11-06},\n  url = {https://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/III-processing.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRutz, Adriano. 2025. “3 Performing Taxonomically Informed\nMetabolite Annotation.” November 6, 2025. https://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/III-processing.html."
  },
  {
    "objectID": "vignettes/tima.html",
    "href": "vignettes/tima.html",
    "title": "General comments about the infrastructure",
    "section": "",
    "text": "This vignette describes the philosophy behind the infrastructure of TIMA."
  },
  {
    "objectID": "vignettes/tima.html#philosophy",
    "href": "vignettes/tima.html#philosophy",
    "title": "General comments about the infrastructure",
    "section": "Philosophy",
    "text": "Philosophy\nOur main goals were flexibility and reproducibility.\n\nFlexibility\nTo ensure flexibility, we tried to split the process in as much tiny parts as needed. So you can decide whether to skip an optional part, add your own processing, etc. We tried to cover most use cases, but of course they are not exhaustive. If you feel like something useful to other users is missing, please fill an issue.\n\n\nReproducibility\nAfter some time using TIMA, you will probably wonder: “What was the parameters I used to generate this file?” … Or a collaborator might ask you to share your data and parameters. Writing them down each time might be time-consuming and not really in line with modern computational approaches. Therefore, we chose to implement all parameters of all steps (almost…) as YAML files. They are human-readable and can be used in batches. If you do not like YAML, parameters of each step can also be given as command line arguments. They will then be saved as YAML you will be able to share.\nTo ensure optimal reproducibility and avoiding re-computing endlessly steps that did not change, we decided to build a {targets} pipeline. Each step of the whole pipeline will be described next."
  },
  {
    "objectID": "vignettes/tima.html#use",
    "href": "vignettes/tima.html#use",
    "title": "General comments about the infrastructure",
    "section": "Use",
    "text": "Use\nAll coming steps admit you already installed tima:\n\ninstall.packages(\n  \"tima\",\n  repos = c(\n    \"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\",\n    \"https://cloud.r-project.org\"\n  )\n)\ntima::install()\ntima::get_example_files()\n\nWe now recommend you to read the following vignettes:\n\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/I-gathering.html\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/II-preparing.html\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/III-processing.html\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/IV-benchmarking.html\n\n\ntl;dr\nIf you do not feel like going through all the steps, then just do 🚀:\n\ntima::run_app()\n\nIf you do not even need a GUI ☠️:\n\ntima::tima_full()\n\nIn case you just want to change some small parameters between jobs, a convenience function is available:\n\ntima::change_params_small(\n  fil_pat = \"myExamplePattern\",\n  fil_fea_raw = \"myExampleDir/myExampleFeatures.csv\",\n  fil_met_raw = \"myExampleDir2SomeWhereElse/myOptionalMetadata.tsv\",\n  fil_sir_raw = \"myExampleDir3/myAwesomeSiriusProject.zip\",\n  fil_spe_raw = \"myBeautifulSpectra.mgf\",\n  ms_pol = \"pos\",\n  org_tax = \"Gentiana lutea\",\n  hig_con = TRUE,\n  summarize = FALSE\n)"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. &lt;https://fsf.org/&gt;\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\nThe GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow.\n\n\n\n\n\n“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\n\nThe work must carry prominent notices stating that you modified it, and giving a relevant date.\n\n\nThe work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\n\n\nYou must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\n\n\nIf the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\n\n\nConvey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\n\n\nConvey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\n\n\nConvey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\n\nDisclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\n\n\nRequiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\n\n\nProhibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\n\n\nLimiting the use for publicity purposes of names of licensors or authors of the material; or\n\n\nDeclining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\n\n\nRequiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS\n\n\n\n\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "LICENSE.html#preamble",
    "href": "LICENSE.html#preamble",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow."
  },
  {
    "objectID": "LICENSE.html#terms-and-conditions",
    "href": "LICENSE.html#terms-and-conditions",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\n\nThe work must carry prominent notices stating that you modified it, and giving a relevant date.\n\n\nThe work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\n\n\nYou must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\n\n\nIf the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\n\n\nConvey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\n\n\nConvey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\n\n\nConvey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\n\nDisclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\n\n\nRequiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\n\n\nProhibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\n\n\nLimiting the use for publicity purposes of names of licensors or authors of the material; or\n\n\nDeclining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\n\n\nRequiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS"
  },
  {
    "objectID": "LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "href": "LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tima ",
    "section": "",
    "text": "The initial work is available at https://doi.org/10.3389/fpls.2019.01329, with many improvements made since then. The workflow is illustrated below.\n\n\nThis repository contains everything needed to perform Taxonomically Informed Metabolite Annotation.\n\n\nHere is what you minimally need:\n\nA feature list (.csv) (see example features)\nA spectral file corresponding to the feature list (.mgf) (see example spectra)\nThe biological source(s) of the sample(s) you are annotating (.csv) (see example metadata) (File is optional if only a single organism)\n\nOptionally, you may want to add:\n\nAn in-house structure-organism pairs library (we provide LOTUS as starting point for each user)\nYour own manual or automated annotations (we currently support annotations coming from SIRIUS (with some limitations))\n\n\n\n\nAs the package is not (yet) available on CRAN, you will need to install with:\ninstall.packages(\n  \"tima\",\n  repos = c(\n    \"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\",\n    \"https://cloud.r-project.org\"\n  )\n)\nThen, you should be able to install the rest with:\ntima::install()\nNormally, everything you need should then be installed (as tested in here). If for some reason, some packages were not installed, try to install them manually. To avoid such issues, we offer a containerized version (see Docker).\nOnce installed, you are ready to go through our documentation, with the major steps detailed.\nIn case you do not have your data ready, you can obtain some example data using:\ntima::get_example_files()\nOnce you are done, you can open a small GUI to adapt your parameters and launch your job:\ntima::run_app()\nThis command will open a small app in your default browser.\n\n\nA container is also available, together with a small compose file. Main commands are below:\ndocker pull adafede/tima-r\n# docker build . -t adafede/tima-r\ndocker run --user tima-user -v \"$(pwd)/.tima/data:/home/tima-user/.tima/data\" -p 3838:3838 adafede/tima-r Rscript -e \"tima::run_app()\"\n# docker run --user tima-user -v \"$(pwd)/.tima/data:/home/tima-user/.tima/data\" adafede/tima-r Rscript -e \"tima::tima_full()\"\n\n\n\n\nAccording to which steps you used, please give credit to the authors of the tools/resources used.\n\n\nGeneral: https://doi.org/10.3389/fpls.2019.01329\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5797920\n\n\n\nGeneral: https://doi.org/10.7554/eLife.70780\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5794106\n\n\n\nGeneral: https://doi.org/10.1021/acs.analchem.5b04804\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5607185\n\n\n\nGeneral: https://doi.org/10.1038/nbt.3597\n\n\n\nGeneral: https://doi.org/10.1038/s41592-019-0344-8\n\nCSI:FingerId: https://doi.org/10.1073/pnas.1509788112\nZODIAC: https://doi.org/10.1038/s42256-020-00234-6\nCANOPUS: https://doi.org/10.1038/s41587-020-0740-8\nCOSMIC: https://doi.org/10.1038/s41587-021-01045-9\n\n\n\n\n\nECMDB 2.0: https://doi.org/10.1093/nar/gkv1060\nHMDB 5.0: https://doi.org/10.1093/nar/gkab1062\nMassBank: https://doi.org/10.5281/zenodo.3378723\nMerlin: https://doi.org/10.5281/zenodo.13911806\nNPClassifier: https://doi.org/10.1021/acs.jnatprod.1c00399\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\n\narchive\n1.1.12\nHester and Csárdi (2025)\n\n\nbase\n4.5.2\nR Core Team (2025)\n\n\nBiocManager\n1.30.26\nMorgan and Ramos (2025)\n\n\nBiocParallel\n1.44.0\nWang et al. (2025)\n\n\nBiocVersion\n3.22.0\nMorgan (2025)\n\n\ndocopt\n0.7.2\nde Jonge (2025)\n\n\nDT\n0.34.0\nXie et al. (2025)\n\n\nfs\n1.6.6\nHester, Wickham, and Csárdi (2025)\n\n\ngt\n1.1.0\nIannone et al. (2025)\n\n\nhttr2\n1.2.1\nWickham (2025)\n\n\nigraph\n2.2.1\nCsárdi and Nepusz (2006); Antonov et al. (2023); Csárdi et al. (2025)\n\n\nIRanges\n2.44.0\nLawrence et al. (2013)\n\n\nknitr\n1.50\nXie (2014); Xie (2015); Xie (2025)\n\n\nlogger\n0.4.1\nDaróczi and Wickham (2025)\n\n\nMetaboCoreUtils\n1.18.0\nRainer et al. (2022a)\n\n\nMsBackendMgf\n1.18.0\nGatto, Rainer, and Gibb (2025)\n\n\nMsBackendMsp\n1.14.0\nRainer et al. (2022b)\n\n\nMsCoreUtils\n1.21.0\nRainer et al. (2022c)\n\n\nmsentropy\n0.1.4\nLi (2023)\n\n\nreticulate\n1.44.0\nUshey, Allaire, and Tang (2025)\n\n\nrmarkdown\n2.30\nXie, Allaire, and Grolemund (2018); Xie, Dervieux, and Riederer (2020); Allaire et al. (2025)\n\n\nrotl\n3.1.0\nMichonneau, Brown, and Winter (2016); OpenTreeOfLife et al. (2019)\n\n\nshiny\n1.11.1\nChang et al. (2025)\n\n\nshinybusy\n0.3.3\nMeyer and Perrier (2024)\n\n\nshinyhelper\n0.3.2\nMason-Thom (2019)\n\n\nshinyjs\n2.1.0\nAttali (2021)\n\n\nshinytest2\n0.4.1\nSchloerke (2025)\n\n\nshinyvalidate\n0.1.3\nSievert, Iannone, and Cheng (2023)\n\n\nshinyWidgets\n0.9.0\nPerrier, Meyer, and Granjon (2025)\n\n\nSpectra\n1.19.11\nRainer et al. (2022d)\n\n\nstringi\n1.8.7\nGagolewski (2022)\n\n\ntargets\n1.11.4\nLandau (2021)\n\n\ntestthat\n3.2.3\nWickham (2011)\n\n\ntidyfst\n1.8.2\nHuang and Zhao (2020)\n\n\ntidyselect\n1.2.1\nHenry and Wickham (2024)\n\n\ntidytable\n0.11.2\nFairbanks (2024)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\ntima\n2.12.0\nRutz et al. (2019); Rutz and Allard (2025)\n\n\nvisNetwork\n2.1.4\nAlmende B.V. and Contributors and Thieurmel (2025)\n\n\nyaml\n2.3.10\nGarbett et al. (2024)"
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "tima ",
    "section": "",
    "text": "Here is what you minimally need:\n\nA feature list (.csv) (see example features)\nA spectral file corresponding to the feature list (.mgf) (see example spectra)\nThe biological source(s) of the sample(s) you are annotating (.csv) (see example metadata) (File is optional if only a single organism)\n\nOptionally, you may want to add:\n\nAn in-house structure-organism pairs library (we provide LOTUS as starting point for each user)\nYour own manual or automated annotations (we currently support annotations coming from SIRIUS (with some limitations))"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "tima ",
    "section": "",
    "text": "As the package is not (yet) available on CRAN, you will need to install with:\ninstall.packages(\n  \"tima\",\n  repos = c(\n    \"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\",\n    \"https://cloud.r-project.org\"\n  )\n)\nThen, you should be able to install the rest with:\ntima::install()\nNormally, everything you need should then be installed (as tested in here). If for some reason, some packages were not installed, try to install them manually. To avoid such issues, we offer a containerized version (see Docker).\nOnce installed, you are ready to go through our documentation, with the major steps detailed.\nIn case you do not have your data ready, you can obtain some example data using:\ntima::get_example_files()\nOnce you are done, you can open a small GUI to adapt your parameters and launch your job:\ntima::run_app()\nThis command will open a small app in your default browser.\n\n\nA container is also available, together with a small compose file. Main commands are below:\ndocker pull adafede/tima-r\n# docker build . -t adafede/tima-r\ndocker run --user tima-user -v \"$(pwd)/.tima/data:/home/tima-user/.tima/data\" -p 3838:3838 adafede/tima-r Rscript -e \"tima::run_app()\"\n# docker run --user tima-user -v \"$(pwd)/.tima/data:/home/tima-user/.tima/data\" adafede/tima-r Rscript -e \"tima::tima_full()\""
  },
  {
    "objectID": "index.html#main-citations",
    "href": "index.html#main-citations",
    "title": "tima ",
    "section": "",
    "text": "According to which steps you used, please give credit to the authors of the tools/resources used.\n\n\nGeneral: https://doi.org/10.3389/fpls.2019.01329\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5797920\n\n\n\nGeneral: https://doi.org/10.7554/eLife.70780\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5794106\n\n\n\nGeneral: https://doi.org/10.1021/acs.analchem.5b04804\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5607185\n\n\n\nGeneral: https://doi.org/10.1038/nbt.3597\n\n\n\nGeneral: https://doi.org/10.1038/s41592-019-0344-8\n\nCSI:FingerId: https://doi.org/10.1073/pnas.1509788112\nZODIAC: https://doi.org/10.1038/s42256-020-00234-6\nCANOPUS: https://doi.org/10.1038/s41587-020-0740-8\nCOSMIC: https://doi.org/10.1038/s41587-021-01045-9\n\n\n\n\n\nECMDB 2.0: https://doi.org/10.1093/nar/gkv1060\nHMDB 5.0: https://doi.org/10.1093/nar/gkab1062\nMassBank: https://doi.org/10.5281/zenodo.3378723\nMerlin: https://doi.org/10.5281/zenodo.13911806\nNPClassifier: https://doi.org/10.1021/acs.jnatprod.1c00399"
  },
  {
    "objectID": "index.html#additional-software-credits",
    "href": "index.html#additional-software-credits",
    "title": "tima ",
    "section": "",
    "text": "Package\nVersion\nCitation\n\n\n\n\narchive\n1.1.12\nHester and Csárdi (2025)\n\n\nbase\n4.5.2\nR Core Team (2025)\n\n\nBiocManager\n1.30.26\nMorgan and Ramos (2025)\n\n\nBiocParallel\n1.44.0\nWang et al. (2025)\n\n\nBiocVersion\n3.22.0\nMorgan (2025)\n\n\ndocopt\n0.7.2\nde Jonge (2025)\n\n\nDT\n0.34.0\nXie et al. (2025)\n\n\nfs\n1.6.6\nHester, Wickham, and Csárdi (2025)\n\n\ngt\n1.1.0\nIannone et al. (2025)\n\n\nhttr2\n1.2.1\nWickham (2025)\n\n\nigraph\n2.2.1\nCsárdi and Nepusz (2006); Antonov et al. (2023); Csárdi et al. (2025)\n\n\nIRanges\n2.44.0\nLawrence et al. (2013)\n\n\nknitr\n1.50\nXie (2014); Xie (2015); Xie (2025)\n\n\nlogger\n0.4.1\nDaróczi and Wickham (2025)\n\n\nMetaboCoreUtils\n1.18.0\nRainer et al. (2022a)\n\n\nMsBackendMgf\n1.18.0\nGatto, Rainer, and Gibb (2025)\n\n\nMsBackendMsp\n1.14.0\nRainer et al. (2022b)\n\n\nMsCoreUtils\n1.21.0\nRainer et al. (2022c)\n\n\nmsentropy\n0.1.4\nLi (2023)\n\n\nreticulate\n1.44.0\nUshey, Allaire, and Tang (2025)\n\n\nrmarkdown\n2.30\nXie, Allaire, and Grolemund (2018); Xie, Dervieux, and Riederer (2020); Allaire et al. (2025)\n\n\nrotl\n3.1.0\nMichonneau, Brown, and Winter (2016); OpenTreeOfLife et al. (2019)\n\n\nshiny\n1.11.1\nChang et al. (2025)\n\n\nshinybusy\n0.3.3\nMeyer and Perrier (2024)\n\n\nshinyhelper\n0.3.2\nMason-Thom (2019)\n\n\nshinyjs\n2.1.0\nAttali (2021)\n\n\nshinytest2\n0.4.1\nSchloerke (2025)\n\n\nshinyvalidate\n0.1.3\nSievert, Iannone, and Cheng (2023)\n\n\nshinyWidgets\n0.9.0\nPerrier, Meyer, and Granjon (2025)\n\n\nSpectra\n1.19.11\nRainer et al. (2022d)\n\n\nstringi\n1.8.7\nGagolewski (2022)\n\n\ntargets\n1.11.4\nLandau (2021)\n\n\ntestthat\n3.2.3\nWickham (2011)\n\n\ntidyfst\n1.8.2\nHuang and Zhao (2020)\n\n\ntidyselect\n1.2.1\nHenry and Wickham (2024)\n\n\ntidytable\n0.11.2\nFairbanks (2024)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\ntima\n2.12.0\nRutz et al. (2019); Rutz and Allard (2025)\n\n\nvisNetwork\n2.1.4\nAlmende B.V. and Contributors and Thieurmel (2025)\n\n\nyaml\n2.3.10\nGarbett et al. (2024)"
  },
  {
    "objectID": "vignettes/articles/IV-benchmarking.html",
    "href": "vignettes/articles/IV-benchmarking.html",
    "title": "4 Benchmarking Performance",
    "section": "",
    "text": "This vignette simply shows the actual performance of TIMA.\nThe benchmarking dataset was built using https://zenodo.org/record/5186176.\nIt contained positive and negative MS2 spectra of multiple ion species ([M+H]+, [M+Na]+, [M+H4N]+, …) coming from different mass spectrometers.\nIn positive mode, It was filtered to 27,789 spectra, representing 17,822 structures without stereo. Of those, only 15,005 spectra (54.0%) corresponded to structures present in the library we used to annotate.\nIn negative mode, It was filtered to 12,060 spectra, representing 9,112 structures without stereo. Of those, only 6,282 spectra (52.1%) corresponded to structures present in the library we used to annotate."
  },
  {
    "objectID": "vignettes/articles/IV-benchmarking.html#best-100-candidates",
    "href": "vignettes/articles/IV-benchmarking.html#best-100-candidates",
    "title": "4 Benchmarking Performance",
    "section": "Best 100 candidates",
    "text": "Best 100 candidates\n\nPositive\n\n\n\n\nNegative"
  },
  {
    "objectID": "vignettes/articles/IV-benchmarking.html#best-25-candidates-zoomed",
    "href": "vignettes/articles/IV-benchmarking.html#best-25-candidates-zoomed",
    "title": "4 Benchmarking Performance",
    "section": "Best 25 candidates (zoomed)",
    "text": "Best 25 candidates (zoomed)\n\nPositive\n\n\n\n\nNegative"
  },
  {
    "objectID": "vignettes/articles/IV-benchmarking.html#candidates-distribution",
    "href": "vignettes/articles/IV-benchmarking.html#candidates-distribution",
    "title": "4 Benchmarking Performance",
    "section": "Candidates distribution",
    "text": "Candidates distribution\n\nPositive\n\n\n\n\nNegative"
  },
  {
    "objectID": "vignettes/articles/II-preparing.html",
    "href": "vignettes/articles/II-preparing.html",
    "title": "2 Preparing inputs",
    "section": "",
    "text": "This vignette describes the main steps of the annotation process."
  },
  {
    "objectID": "vignettes/articles/II-preparing.html#structural-annotations-of-your-features",
    "href": "vignettes/articles/II-preparing.html#structural-annotations-of-your-features",
    "title": "2 Preparing inputs",
    "section": "Structural annotations of your features",
    "text": "Structural annotations of your features\nFor the moment, we support 3 different types of annotations:\n\n\nInternal MS1 exact mass-based library search\nInternal MS2 library search (experimental and in silico)\nSIRIUS\n\nIf needed, you can get an example of what your minimal feature table should look like by running:\n\ntima::get_example_files(example = \"features\")\n\nThen, you can prepare your features for the next steps:\n\ntima::prepare_features_tables()\n#&gt; [1] \"data/interim/features/example_features.tsv.gz\"\n\n\nMS1-based\nThese annotations are of the lowest possible quality. However, they allow to annotate unusual adducts, in-source fragments thanks to different small tricks implemented. Try to really restrict the adduct list and structure-organism pairs you want to consider as possibilities explode rapidly.\n\ntima::annotate_masses()\n#&gt; $annotations\n#&gt; [1] \"data/interim/annotations/example_ms1Prepared.tsv.gz\"\n#&gt; \n#&gt; $edges\n#&gt; [1] \"data/interim/features/example_edgesMasses.tsv\"\n\n\n\nSpectral\n\ntima::annotate_spectra()\n#&gt;  ■                                  1% |  ETA:  6m\n#&gt;  ■■                                 2% |  ETA:  6m\n#&gt;  ■■                                 3% |  ETA:  5m\n#&gt;  ■■                                 4% |  ETA:  5m\n#&gt;  ■■■                                5% |  ETA:  5m\n#&gt;  ■■■                                6% |  ETA:  5m\n#&gt;  ■■■                                8% |  ETA:  4m\n#&gt;  ■■■■                               9% |  ETA:  4m\n#&gt;  ■■■■                              10% |  ETA:  4m\n#&gt;  ■■■■■                             12% |  ETA:  4m\n#&gt;  ■■■■■                             13% |  ETA:  4m\n#&gt;  ■■■■■                             14% |  ETA:  4m\n#&gt;  ■■■■■■                            16% |  ETA:  3m\n#&gt;  ■■■■■■                            17% |  ETA:  3m\n#&gt;  ■■■■■■                            18% |  ETA:  3m\n#&gt;  ■■■■■■■                           19% |  ETA:  3m\n#&gt;  ■■■■■■■                           20% |  ETA:  3m\n#&gt;  ■■■■■■■■                          22% |  ETA:  3m\n#&gt;  ■■■■■■■■                          23% |  ETA:  3m\n#&gt;  ■■■■■■■■                          25% |  ETA:  3m\n#&gt;  ■■■■■■■■■                         26% |  ETA:  3m\n#&gt;  ■■■■■■■■■                         27% |  ETA:  3m\n#&gt;  ■■■■■■■■■■                        29% |  ETA:  3m\n#&gt;  ■■■■■■■■■■                        30% |  ETA:  3m\n#&gt;  ■■■■■■■■■■                        31% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■                       32% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■                       34% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■                       35% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■                      37% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■                      38% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■                     39% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■                     40% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■                     41% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■                    43% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■                    44% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■                   46% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■                   47% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■                   48% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■                  49% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■                  51% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■                 52% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■                 54% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■                55% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■                57% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               59% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               60% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              62% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              63% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              65% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             66% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             68% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■            70% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■            72% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■           73% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■           74% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■          76% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■          78% |  ETA: 50s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■         79% |  ETA: 46s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■         81% |  ETA: 43s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■        82% |  ETA: 39s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■        84% |  ETA: 36s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■        85% |  ETA: 33s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■       86% |  ETA: 31s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■       88% |  ETA: 27s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■      89% |  ETA: 25s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■      90% |  ETA: 22s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■      91% |  ETA: 19s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■     92% |  ETA: 17s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■     94% |  ETA: 13s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■    95% |  ETA: 10s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■    97% |  ETA:  7s\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■   99% |  ETA:  3s\n#&gt; [1] \"data/interim/annotations/example_spectralMatches.tsv.gz\"\ntima::prepare_annotations_spectra()\n#&gt; [1] \"data/interim/annotations/example_spectralMatchesPrepared.tsv.gz\"\n\nThere are multiple similarities available, including spectral entropy from https://doi.org/10.1038/s41592-021-01331-z for matching.\n\nGNPS\nOptional\n\ntima::prepare_annotations_gnps()\n#&gt; [1] \"data/interim/annotations/example_gnpsPrepared.tsv.gz\"\n\n\n\n\nFingerprint-based\n\nSirius\nAs SIRIUS jobs are long to perform, we provide example SIRIUS workspaces (both SIRIUS 5 and 6). Note that spectral matches from SIRIUS are not supported for now. They have been generated on the 20 first lines of the example MGF with the following command:\n\n# this is run on SIRIUS 6\nsirius \\\n--noCite \\\n--input=data/source/example_spectra_mini.mgf \\\n--output=data/interim/annotations/example_sirius.sirius/ \\\n--maxmz=800 \\\nconfig \\\n--AlgorithmProfile=orbitrap \\\n--StructureSearchDB=BIO \\\n--Timeout.secondsPerTree=10 \\\n--Timeout.secondsPerInstance=10 \\\nformulas \\\nzodiac \\\nfingerprints \\\nclasses \\\nstructures \\\ndenovo-structures \\\nsummaries \\\n--chemvista \\\n--feature-quality-summary \\\n--full-summary\n\n# this is run on SIRIUS 5\nsirius \\\n--noCite \\\n--input data/source/example_spectra_mini.mgf \\\n--output data/interim/annotations/example_sirius/ \\\n--maxmz 800 \\\nconfig \\\n--AlgorithmProfile orbitrap \\\n--StructureSearchDB BIO \\\n--Timeout.secondsPerTree 10 \\\n--Timeout.secondsPerInstance 10 \\\nformula \\\nzodiac \\\nfingerprint \\\nstructure \\\ncompound-classes \\\nwrite-summaries \\\n--full-summary\n\nThese parameters were not optimized and were only used to give an example output. If you are using the cli, do not forget to generate the summaries with the --full-summary option, or if you use the gui, generate them by clicking the corresponding icon. You can get an example running:\n\ntima:::get_example_sirius()\n\nThe sirius workspace should ideally have yourPattern_sirius as name and be placed in data/interim/annotations (else it will not be found by default except you provide the right path).\n\ntima::prepare_annotations_sirius()\n#&gt;                                                   canopus \n#&gt; \"data/interim/annotations/example_canopusPrepared.tsv.gz\" \n#&gt;                                                   formula \n#&gt; \"data/interim/annotations/example_formulaPrepared.tsv.gz\" \n#&gt;                                                structural \n#&gt;  \"data/interim/annotations/example_siriusPrepared.tsv.gz\"\n\nIf you want to know how we attempt to combine the CSI score with other ones, see R/transform_score_sirius_csi.R Note that starting from SIRIUS 6, the approx confidence score is the one considered, and not the exact one.\n\n\n\n\n\n\nAnnotations are now prepared and can be used for further processing. Your features are not only informed with structural information but also, chemical class information. The latter might be corresponding or not to the chemical class of your annotated structure, depending on the consistency of your annotations."
  },
  {
    "objectID": "vignettes/articles/II-preparing.html#chemical-class-annotation-of-your-features",
    "href": "vignettes/articles/II-preparing.html#chemical-class-annotation-of-your-features",
    "title": "2 Preparing inputs",
    "section": "Chemical class annotation of your features",
    "text": "Chemical class annotation of your features\nWithin our workflow, we offer a new way to attribute chemical classes to your features. It is analog to Network Annotation Propagation, but uses the edges of your network instead of the clusters. This makes more sense in our view, as also recently illustrated by CANOPUS.\n\nWe are currently also working on CANOPUS integration for chemical class annotation but this implies way heavier computations and we want to offer our users a fast solution.\n\nGenerating a network\nA network is generated during the process. The edges are created based on the spectral entropy similarity calculated between your spectra (see https://doi.org/10.1038/s41592-021-01331-z).\n\ntima::create_edges_spectra()\n#&gt;  ■                                  0% |  ETA: 14m\n#&gt;  ■                                  1% |  ETA: 13m\n#&gt;  ■                                  1% |  ETA: 13m\n#&gt;  ■                                  2% |  ETA: 13m\n#&gt;  ■■                                 2% |  ETA: 13m\n#&gt;  ■■                                 2% |  ETA: 14m\n#&gt;  ■■                                 3% |  ETA: 14m\n#&gt;  ■■                                 3% |  ETA: 13m\n#&gt;  ■■                                 3% |  ETA: 13m\n#&gt;  ■■                                 4% |  ETA: 13m\n#&gt;  ■■                                 4% |  ETA: 13m\n#&gt;  ■■                                 5% |  ETA: 12m\n#&gt;  ■■■                                5% |  ETA: 12m\n#&gt;  ■■■                                6% |  ETA: 12m\n#&gt;  ■■■                                6% |  ETA: 12m\n#&gt;  ■■■                                7% |  ETA: 12m\n#&gt;  ■■■                                7% |  ETA: 11m\n#&gt;  ■■■                                7% |  ETA: 11m\n#&gt;  ■■■                                8% |  ETA: 11m\n#&gt;  ■■■■                               8% |  ETA: 11m\n#&gt;  ■■■■                               9% |  ETA: 11m\n#&gt;  ■■■■                               9% |  ETA: 11m\n#&gt;  ■■■■                              10% |  ETA: 11m\n#&gt;  ■■■■                              10% |  ETA: 11m\n#&gt;  ■■■■                              11% |  ETA: 10m\n#&gt;  ■■■■                              11% |  ETA: 10m\n#&gt;  ■■■■■                             12% |  ETA: 10m\n#&gt;  ■■■■■                             12% |  ETA: 10m\n#&gt;  ■■■■■                             13% |  ETA: 10m\n#&gt;  ■■■■■                             13% |  ETA: 10m\n#&gt;  ■■■■■                             14% |  ETA: 10m\n#&gt;  ■■■■■                             14% |  ETA: 10m\n#&gt;  ■■■■■                             15% |  ETA: 10m\n#&gt;  ■■■■■■                            15% |  ETA: 10m\n#&gt;  ■■■■■■                            16% |  ETA:  9m\n#&gt;  ■■■■■■                            16% |  ETA:  9m\n#&gt;  ■■■■■■                            17% |  ETA:  9m\n#&gt;  ■■■■■■                            17% |  ETA:  9m\n#&gt;  ■■■■■■                            18% |  ETA:  9m\n#&gt;  ■■■■■■                            18% |  ETA:  9m\n#&gt;  ■■■■■■■                           19% |  ETA:  9m\n#&gt;  ■■■■■■■                           19% |  ETA:  9m\n#&gt;  ■■■■■■■                           20% |  ETA:  9m\n#&gt;  ■■■■■■■                           20% |  ETA:  9m\n#&gt;  ■■■■■■■                           21% |  ETA:  9m\n#&gt;  ■■■■■■■                           21% |  ETA:  8m\n#&gt;  ■■■■■■■■                          22% |  ETA:  8m\n#&gt;  ■■■■■■■■                          22% |  ETA:  8m\n#&gt;  ■■■■■■■■                          23% |  ETA:  8m\n#&gt;  ■■■■■■■■                          24% |  ETA:  8m\n#&gt;  ■■■■■■■■                          24% |  ETA:  8m\n#&gt;  ■■■■■■■■                          25% |  ETA:  8m\n#&gt;  ■■■■■■■■■                         25% |  ETA:  8m\n#&gt;  ■■■■■■■■■                         26% |  ETA:  8m\n#&gt;  ■■■■■■■■■                         26% |  ETA:  8m\n#&gt;  ■■■■■■■■■                         27% |  ETA:  8m\n#&gt;  ■■■■■■■■■                         28% |  ETA:  8m\n#&gt;  ■■■■■■■■■                         28% |  ETA:  7m\n#&gt;  ■■■■■■■■■■                        29% |  ETA:  7m\n#&gt;  ■■■■■■■■■■                        29% |  ETA:  7m\n#&gt;  ■■■■■■■■■■                        30% |  ETA:  7m\n#&gt;  ■■■■■■■■■■                        31% |  ETA:  7m\n#&gt;  ■■■■■■■■■■                        31% |  ETA:  7m\n#&gt;  ■■■■■■■■■■■                       32% |  ETA:  7m\n#&gt;  ■■■■■■■■■■■                       32% |  ETA:  7m\n#&gt;  ■■■■■■■■■■■                       33% |  ETA:  7m\n#&gt;  ■■■■■■■■■■■                       34% |  ETA:  7m\n#&gt;  ■■■■■■■■■■■                       34% |  ETA:  7m\n#&gt;  ■■■■■■■■■■■                       35% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      36% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      36% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      37% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      38% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■                      38% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■■                     39% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■■                     40% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■■                     40% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■■                     41% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■■                     42% |  ETA:  6m\n#&gt;  ■■■■■■■■■■■■■■                    42% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■                    43% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■                    44% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■                    45% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■                   45% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■                   46% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■                   47% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■                   48% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■■                  49% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■■                  49% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■■                  50% |  ETA:  5m\n#&gt;  ■■■■■■■■■■■■■■■■                  51% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■                 52% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■                 53% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■                 54% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■■                55% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■■                57% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■■                58% |  ETA:  4m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               59% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■               61% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              62% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              63% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■              65% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             67% |  ETA:  3m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■             68% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■            70% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■            72% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■           74% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■          76% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■          78% |  ETA:  2m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■         81% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■        83% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■       87% |  ETA:  1m\n#&gt;  ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■     92% |  ETA: 30s\n#&gt; [1] \"data/interim/features/example_edgesSpectra.tsv\"\n\nPrepare the edges:\n\ntima::prepare_features_edges()\n#&gt; [1] \"data/interim/features/example_edges.tsv\"\n\nCreate and prepare the components:\n\ntima::create_components()\n#&gt; [1] \"data/interim/features/example_components.tsv\"\n\n\ntima::prepare_features_components()\n#&gt; [1] \"data/interim/features/example_componentsPrepared.tsv\""
  },
  {
    "objectID": "vignettes/articles/II-preparing.html#biological-source-annotation",
    "href": "vignettes/articles/II-preparing.html#biological-source-annotation",
    "title": "2 Preparing inputs",
    "section": "Biological source annotation",
    "text": "Biological source annotation\nThis step allows you to attribute biological source information to your features. If all your features come from a single extract, it will attribute the biological source of your extract to all your features. If you have multiple extracts aligned, it will take the n (according to your parameters) highest intensities of your aligned feature table and attribute the biological source of corresponding extracts. \n\ntima::prepare_taxa()\n#&gt; [1] \"data/interim/taxa/example_taxed.tsv.gz\""
  },
  {
    "objectID": "vignettes/articles/II-preparing.html#filter-annotations-based-on-retention-time",
    "href": "vignettes/articles/II-preparing.html#filter-annotations-based-on-retention-time",
    "title": "2 Preparing inputs",
    "section": "Filter annotations (based on retention time)",
    "text": "Filter annotations (based on retention time)\nThis step allows you to filter out the annotation of all the tools used, based on your own internal (experimental or predicted) retention times library. It is optional. If you do not have one, it will simply group the annotations of all tools.\n\ntima::filter_annotations()\n#&gt; [1] \"data/interim/annotations/example_annotationsFiltered.tsv.gz\"\n\nYou are almost there! See already all the steps accomplished!\n\n\n\n\n\n\nWe now recommend you to read the next vignette."
  },
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "tima",
    "section": "",
    "text": "tima\n\n\ntima 2.12.0\n\nAdded a minimal output\nAdded a parameter to limit the numbers of neighbors used for chemical consistency calculation (#193)\nAdded MERLIN spectral libraries (#190)\nAdded RDKit-based structures processing through reticulate (#19)\nBreaking Change: .RDS spectra are now stored more efficiently. To avoid errors, delete any .RDS files created before version 2.12.0\nExternalized spectral libraries preparation to SpectRalLibRaRies\nIntroduced similarity method argument (entropy and GNPS for now)\nImplemented GNPS similarity method in C\nImproved high confidence filtering\nImproved logs using logger (#189)\nKeep (only) best molecular formula and canopus annotations from SIRIUS\nNew ISDB version with 1 million compounds (see https://doi.org/10.5281/zenodo.14887271)\nRefactored MS1 annotation step to work per sample (#194)\nSwitched documentation from pkgdown to altdoc\nUpdated to Massbank version 2025.05.1\nUpdated minimal R version to 4.4.0 (and related Bioconductor dependencies)\n\n\n\ntima 2.11.1 (unreleased)\n\nAdded SIRIUS feature tables support (#185)\nAdded .rar compression support for SIRIUS workspaces (#186)\n\n\n\ntima 2.11.0\n\nAdded convenience function to change small parameters (#177)\nAdded demo files download to the app\nBetter packaging\nImproved documentation\nFixed all CRAN warnings\nFixed some edge cases in spectra import\nReduced dependencies\nReduced exports\nRemoved CompoundDb dependency as it was causing too many issues\nRemoved pak install and switched to r-universe\nReplaced internal functions by Spectra equivalents (#166)\nShinylive version available at https://taxonomicallyinformedannotation.github.io/tima-shinylive\nSimplified install and vignettes\nSwitched from base::lapply to purrr::map\n\n\n\ntima 2.10.0\n\nAdded alt text to vignettes\nAdded the possibility to add internal libraries through the GUI (#159)\nAdded the possibility to filter confident annotations only (#140)\nAdded number of peaks in spectrum\nBrought back some older dependencies to be compatible with oldrel\nChanged package name, usethis update\nClearer handling of SIRIUS scores (#146, #147)\nExposed more parameters to the GUI (#159)\nFacilitated install, no need to clone the directory anymore\nFinally made it to the r-universe\nFixed adducts and removed nitrogen rule\nFixed number of matched peaks\nImproved imports\nReduced warnings\nUpdated benchmarking steps\n\n\n\ntima 2.9.6\n\nAdded light-switch thanks to pkgdown 2.1.0.\nAttempt to simplify installation\nFixed library/adducts confusion (#123)\nFixed some incorrect adduct differences annotations\nRefactored adducts / neutral losses / dimers annotation to allow for more flexibility (#141, #144)\n\n\n\ntima 2.9.5\n\nDo not re-package if already the latest version\nSIRIUS 6 default and compatible (keeping SIRIUS 5 backward compatibility)\nUpdated to Massbank version 2024.06\n\n\n\ntima 2.9.4\n\nAutomated update\nAdded an option to remove ties (#134)\nAdded some details for SIRIUS, added manual workspace addition (#132)\nAdditional preprocessing (reduction) of noisy spectra\nDependencies update\nDocker updates (#131)\nHandle cases when same (feature_id, mslevel) pairs are present within an MGF (#133)\nImproved documentation\nNew working directory at $HOME/.tima\nUpdated R and Bioconductor versions\n\n\n\ntima 2.9.3\n\nAllowed for SIRIUS jobs containing only summaries\nAllowed for underscores in job pattern\nChanged some default values (less stringent)\nDependencies update\nMigrated app testing to shinytest2\nRemoved further some inconsistent MS1 annotations\nRemoved tests dependencies by default\n\n\n\ntima 2.9.2\n\nAdded Nitrogen rule to filter out some annotations\nBetter handling of partial downloads (#118)\nDependencies update (mainly targets 1.5.1, will invalidate previous targets)\nFixed some port issues in Shiny (#122)\nRemoved completely empty columns from final output to avoid confusion (#120)\n\n\n\ntima 2.9.1\n\nAdded Waystation action\nAdded structures from spectral libraries to SOP library (#113)\nExposed all parameters (#107, #108)\nFixed for Zenodo API\nHMDB structures support\nOptimized grep/gsub by adding perl=TRUE or fixed=TRUE\nUpdated to Massbank version 2023.11\nUpdated SIRIUS preparation (#74, #115)\n\n\n\ntima 2.9.0\n\nAdded compounds names as parameter\nAdded MassBank spectral library (#77)\nAllowed files outside data/source (#89)\nAdded RT library as annotation library (#86)\nBetter handling of download errors\nFixed Docker mount path\nImproved naming (#91)\nInternal variables refactoring\nMultiple Shiny fixes and tests addition (#60)\nMultiple fixes (#71, #81, #82)\nNew adducts (#79, #80)\nRefactored adducts, clusters and neutral losses\nRefactored biological and chemical score\nRefactored RT matching (#76)\nRefactored Sirius scores (#92)\nRemoved GNPS dependency by default\n\n\n\ntima 2.8.2\n\nAdded spectral entropy\nAdded MS1 only possibility\nAdded Fluorine adduct\nChanged from pbmclapply to pblapply\nDocumentation improvement\nFixed empty chemical classes\nFixed not classified taxa\nGitHub Actions improvement\nrenv removal\nPerformance improvement by replacing the tidyverse by the fastverse (in progress)\nReduced warnings (CRAN and jscpd)\n\n\n\ntima 2.8.1\n\nAdapted tests\nAdded retry parameter to get_organism_taxonomy_ott\nDependencies update\nMinor fixes\nMoved /params and paths.yaml to /inst as more standard. (see https://r-pkgs.org/misc.html#other-directories)\nPerformance improvement by replacing the tidyverse by the fastverse (in progress)\nReplaced extdata loading\n\n\n\ntima 2.8.0\n\nAdded GUI prototype\nStarted using renv\n\n\n\ntima 2.7.4\n\nClearer vocabulary\nECMDB support\nEdges (mass and spectra-based) and components are generated if not present.\nFixed case when no GNPS job ID\nFurther Targets improvements\nLot of fixes\nParameters refactoring\nRe-introduced Classyfire support.\nRetention time matching additionally to MS2 if RT present in library\nSteps refactoring\n\n\n\ntima 2.7.3\n\nImproved calculations over redundant formulas\nMinor fixes\nParameters refactoring\nSpectral matching update (see https://github.com/rformassspectrometry/MetaboAnnotation/issues/93)\nTargets implementation\n\n\n\ntima 2.7.2\n\nBenchmark update (including negative mode)\nImproved parameters documentation\nMinor fixes\nSpectral comparison + intensity filtering update\nSwitched r-base Docker image to bioconductor with ARM support\n\n\n\ntima 2.7.1\n\nAdded MONA helpers\nAdded parallelization on process_spectra\nAdded sqlite storing for spectra\nImproved code documentation\nImproved testing time\nMinor fixes\n\n\n\ntima 2.7.0\n\nAdded HMDB helpers for both taxo and ISDB\nAdded MS2 annotation capability (kudos @jorainer for the awesome Spectra suite)\nMinor fixes\n\n\n\ntima 2.6.0\n\nAdded Docker container\nChanged data architecture\nMinor fixes\n\n\n\ntima 2.5.6\n\nDependencies removal (e.g. metabo-store)\nMinor fixes\nPartial functions cleanup\n\n\n\ntima 2.5.5\n\nAutomation and parameters improvement\nMinor fixes\n\n\n\ntima 2.5.4\n\nMinor fixes\nMetadata completion improvement\nMolecular formula and adducts formalism improvement\n\n\n\ntima 2.5.3\n\nImports improvements\nLOTUS update\n\n\n\ntima 2.5.2\n\nPackaging improvements\n\n\n\ntima 2.5.1\n\nImproved support for SIRIUS (with new summaries)\n\n\n\ntima 2.5.0\n\nLOTUS update\nMinor fixes\n\n\n\ntima 2.4.0\n\nAdded chemical names and xlogp to output (#33)\nAdded support for case when no consensus is found (#30)\nImproved output (#34)\nMinor fixes\n\n\n\ntima 2.3.0\n\nAdded support for annotation without MN (#28)\nAdded support for multi tool annotations (#27)\nAdded support for classical MN GNPS jobs (#25)\nAdded support for new version of LOTUS\nGeneral improvements for manual inputs\nImproved tests code coverage\nMinor fixes\nUpdated adducts\n\n\n\ntima 2.2.2\n\nAdditional benchmark figure (Candidates distribution)\nMinor fixes\n\n\n\ntima 2.2.1\n\nMinor version name fixes\n\n\n\ntima 2.2.0\n\nAdded benchmark (here)\nVarious fixes\n\n\n\ntima 2.1.0\n\nFixes, deletion of binary dependencies.\n\n\n\ntima 2.0.0\n\nInitial version."
  },
  {
    "objectID": "man/prepare_features_tables.html",
    "href": "man/prepare_features_tables.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares LC-MS feature tables by standardizing column names, filtering to top intensity samples per feature, and formatting data for downstream analysis. Supports multiple input formats (MZmine, SLAW, SIRIUS).\n\n\n\nprepare_features_tables(\n  features = get_params(step = \"prepare_features_tables\")\\$files\\$features\\$raw,\n  output = get_params(step = \"prepare_features_tables\")\\$files\\$features\\$prepared,\n  candidates = get_params(step = \"prepare_features_tables\")\\$annotations\\$canidates\\$samples,\n  name_adduct = get_params(step = \"prepare_features_tables\")\\$names\\$adduct,\n  name_features = get_params(step = \"prepare_features_tables\")\\$names\\$features,\n  name_rt = get_params(step = \"prepare_features_tables\")\\$names\\$rt\\$features,\n  name_mz = get_params(step = \"prepare_features_tables\")\\$names\\$precursor\n)\n\n\n\n\n\n\n\nfeatures\n\n\nCharacter string path to raw features file\n\n\n\n\noutput\n\n\nCharacter string path where prepared features should be saved\n\n\n\n\ncandidates\n\n\nInteger number of top-intensity samples to retain per feature (recommended: ≤5 to reduce data size while keeping representative samples)\n\n\n\n\nname_adduct\n\n\nCharacter string name of the adduct column in input\n\n\n\n\nname_features\n\n\nCharacter string name of the feature ID column in input\n\n\n\n\nname_rt\n\n\nCharacter string name of the retention time column in input\n\n\n\n\nname_mz\n\n\nCharacter string name of the m/z column in input\n\n\n\n\n\n\nCharacter string path to the prepared feature table\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$features,\n  export = get_params(step = \"prepare_features_tables\")$files$features$raw\n)\nprepare_features_tables()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_tables"
    ]
  },
  {
    "objectID": "man/prepare_features_tables.html#prepare-features-table",
    "href": "man/prepare_features_tables.html#prepare-features-table",
    "title": "tima",
    "section": "",
    "text": "This function prepares LC-MS feature tables by standardizing column names, filtering to top intensity samples per feature, and formatting data for downstream analysis. Supports multiple input formats (MZmine, SLAW, SIRIUS).\n\n\n\nprepare_features_tables(\n  features = get_params(step = \"prepare_features_tables\")\\$files\\$features\\$raw,\n  output = get_params(step = \"prepare_features_tables\")\\$files\\$features\\$prepared,\n  candidates = get_params(step = \"prepare_features_tables\")\\$annotations\\$canidates\\$samples,\n  name_adduct = get_params(step = \"prepare_features_tables\")\\$names\\$adduct,\n  name_features = get_params(step = \"prepare_features_tables\")\\$names\\$features,\n  name_rt = get_params(step = \"prepare_features_tables\")\\$names\\$rt\\$features,\n  name_mz = get_params(step = \"prepare_features_tables\")\\$names\\$precursor\n)\n\n\n\n\n\n\n\nfeatures\n\n\nCharacter string path to raw features file\n\n\n\n\noutput\n\n\nCharacter string path where prepared features should be saved\n\n\n\n\ncandidates\n\n\nInteger number of top-intensity samples to retain per feature (recommended: ≤5 to reduce data size while keeping representative samples)\n\n\n\n\nname_adduct\n\n\nCharacter string name of the adduct column in input\n\n\n\n\nname_features\n\n\nCharacter string name of the feature ID column in input\n\n\n\n\nname_rt\n\n\nCharacter string name of the retention time column in input\n\n\n\n\nname_mz\n\n\nCharacter string name of the m/z column in input\n\n\n\n\n\n\nCharacter string path to the prepared feature table\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$features,\n  export = get_params(step = \"prepare_features_tables\")$files$features$raw\n)\nprepare_features_tables()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_tables"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_merged.html",
    "href": "man/prepare_libraries_sop_merged.html",
    "title": "tima",
    "section": "",
    "text": "This function merges all structure-organism pair libraries (LOTUS, HMDB, ECMDB, etc.) into a single comprehensive library. Can optionally filter by taxonomic level to create biologically-focused subsets. Also splits structures into separate metadata tables.\n\n\n\nprepare_libraries_sop_merged(\n  files = get_params(step = \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$prepared,\n  filter = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$mode,\n  level = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$level,\n  value = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$value,\n  cache = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$processed,\n  output_key = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  output_org_tax_ott = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  output_str_stereo = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  output_str_met = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  output_str_nam = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  output_str_tax_cla = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  output_str_tax_npc = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\nfiles\n\n\nCharacter vector or list of paths to prepared library files\n\n\n\n\nfilter\n\n\nLogical whether to filter the merged library by taxonomy\n\n\n\n\nlevel\n\n\nCharacter string taxonomic rank for filtering (kingdom, phylum, family, genus, etc.)\n\n\n\n\nvalue\n\n\nCharacter string taxon name(s) to keep (can use | for multiple, e.g., ‘Gentianaceae|Apocynaceae’)\n\n\n\n\ncache\n\n\nCharacter string path to cache directory for processed SMILES\n\n\n\n\noutput_key\n\n\nCharacter string path for output keys file\n\n\n\n\noutput_org_tax_ott\n\n\nCharacter string path for organisms taxonomy (OTT) file\n\n\n\n\noutput_str_stereo\n\n\nCharacter string path for structures stereochemistry file\n\n\n\n\noutput_str_met\n\n\nCharacter string path for structures metadata file\n\n\n\n\noutput_str_nam\n\n\nCharacter string path for structures names file\n\n\n\n\noutput_str_tax_cla\n\n\nCharacter string path for ClassyFire taxonomy file\n\n\n\n\noutput_str_tax_npc\n\n\nCharacter string path for NPClassifier taxonomy file\n\n\n\n\n\n\nCreates merged library by combining all available SOP sources, optionally filtering by taxonomic criteria (e.g., only Gentianaceae). Splits output into structures metadata, names, taxonomy, and organisms.\n\n\n\nCharacter string path to the prepared merged SOP library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nfiles &lt;- get_params(step = \"prepare_libraries_sop_merged\")$files$libraries$sop$prepared$lotus |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nget_file(url = paste0(dir, files), export = files)\nprepare_libraries_sop_merged(files = files)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_merged"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_merged.html#prepare-merged-structure-organism-pairs-libraries",
    "href": "man/prepare_libraries_sop_merged.html#prepare-merged-structure-organism-pairs-libraries",
    "title": "tima",
    "section": "",
    "text": "This function merges all structure-organism pair libraries (LOTUS, HMDB, ECMDB, etc.) into a single comprehensive library. Can optionally filter by taxonomic level to create biologically-focused subsets. Also splits structures into separate metadata tables.\n\n\n\nprepare_libraries_sop_merged(\n  files = get_params(step = \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$prepared,\n  filter = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$mode,\n  level = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$level,\n  value = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$value,\n  cache = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$processed,\n  output_key = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  output_org_tax_ott = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  output_str_stereo = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  output_str_met = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  output_str_nam = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  output_str_tax_cla = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  output_str_tax_npc = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\nfiles\n\n\nCharacter vector or list of paths to prepared library files\n\n\n\n\nfilter\n\n\nLogical whether to filter the merged library by taxonomy\n\n\n\n\nlevel\n\n\nCharacter string taxonomic rank for filtering (kingdom, phylum, family, genus, etc.)\n\n\n\n\nvalue\n\n\nCharacter string taxon name(s) to keep (can use | for multiple, e.g., ‘Gentianaceae|Apocynaceae’)\n\n\n\n\ncache\n\n\nCharacter string path to cache directory for processed SMILES\n\n\n\n\noutput_key\n\n\nCharacter string path for output keys file\n\n\n\n\noutput_org_tax_ott\n\n\nCharacter string path for organisms taxonomy (OTT) file\n\n\n\n\noutput_str_stereo\n\n\nCharacter string path for structures stereochemistry file\n\n\n\n\noutput_str_met\n\n\nCharacter string path for structures metadata file\n\n\n\n\noutput_str_nam\n\n\nCharacter string path for structures names file\n\n\n\n\noutput_str_tax_cla\n\n\nCharacter string path for ClassyFire taxonomy file\n\n\n\n\noutput_str_tax_npc\n\n\nCharacter string path for NPClassifier taxonomy file\n\n\n\n\n\n\nCreates merged library by combining all available SOP sources, optionally filtering by taxonomic criteria (e.g., only Gentianaceae). Splits output into structures metadata, names, taxonomy, and organisms.\n\n\n\nCharacter string path to the prepared merged SOP library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nfiles &lt;- get_params(step = \"prepare_libraries_sop_merged\")$files$libraries$sop$prepared$lotus |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nget_file(url = paste0(dir, files), export = files)\nprepare_libraries_sop_merged(files = files)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_merged"
    ]
  },
  {
    "objectID": "man/pre_harmonize_names_sirius.html",
    "href": "man/pre_harmonize_names_sirius.html",
    "title": "tima",
    "section": "",
    "text": "This function pre-processes SIRIUS column names by removing suffixes that appear after a forward slash. SIRIUS sometimes appends additional information after a slash (e.g., \"columnName/suffix\"), and this function removes everything from the slash onwards.\n\n\n\npre_harmonize_names_sirius(x)\n\n\n\n\n\n\n\nx\n\n\nCharacter string containing a SIRIUS column name to pre-harmonize\n\n\n\n\n\n\nCharacter string with everything from and including the first forward slash (/) removed, leaving only the base column name\n\n\n\n\nlibrary(\"tima\")\n\npre_harmonize_names_sirius(\"column_name/suffix\") # Returns \"column_name\"\npre_harmonize_names_sirius(\"simple_name\") # Returns \"simple_name\"\npre_harmonize_names_sirius(\"name/extra/info\") # Returns \"name\""
  },
  {
    "objectID": "man/pre_harmonize_names_sirius.html#pre-harmonize-names-sirius",
    "href": "man/pre_harmonize_names_sirius.html#pre-harmonize-names-sirius",
    "title": "tima",
    "section": "",
    "text": "This function pre-processes SIRIUS column names by removing suffixes that appear after a forward slash. SIRIUS sometimes appends additional information after a slash (e.g., \"columnName/suffix\"), and this function removes everything from the slash onwards.\n\n\n\npre_harmonize_names_sirius(x)\n\n\n\n\n\n\n\nx\n\n\nCharacter string containing a SIRIUS column name to pre-harmonize\n\n\n\n\n\n\nCharacter string with everything from and including the first forward slash (/) removed, leaving only the base column name\n\n\n\n\nlibrary(\"tima\")\n\npre_harmonize_names_sirius(\"column_name/suffix\") # Returns \"column_name\"\npre_harmonize_names_sirius(\"simple_name\") # Returns \"simple_name\"\npre_harmonize_names_sirius(\"name/extra/info\") # Returns \"name\""
  },
  {
    "objectID": "man/weight_chemo.html",
    "href": "man/weight_chemo.html",
    "title": "tima",
    "section": "",
    "text": "This function weights biologically weighted annotations according to their chemical consistency by comparing chemical taxonomy (ClassyFire, NPClassifier) across molecular network neighbors. Higher chemical consistency within network components results in higher chemical scores.\n\n\n\nweight_chemo(\n  annot_table_wei_bio_clean = get(\"annot_table_wei_bio_clean\", envir = parent.frame()),\n  weight_spectral = get(\"weight_spectral\", envir = parent.frame()),\n  weight_biological = get(\"weight_biological\", envir = parent.frame()),\n  weight_chemical = get(\"weight_chemical\", envir = parent.frame()),\n  score_chemical_cla_kingdom = get(\"score_chemical_cla_kingdom\", envir = parent.frame()),\n  score_chemical_cla_superclass = get(\"score_chemical_cla_superclass\", envir =\n    parent.frame()),\n  score_chemical_cla_class = get(\"score_chemical_cla_class\", envir = parent.frame()),\n  score_chemical_cla_parent = get(\"score_chemical_cla_parent\", envir = parent.frame()),\n  score_chemical_npc_pathway = get(\"score_chemical_npc_pathway\", envir = parent.frame()),\n  score_chemical_npc_superclass = get(\"score_chemical_npc_superclass\", envir =\n    parent.frame()),\n  score_chemical_npc_class = get(\"score_chemical_npc_class\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_bio_clean\n\n\nData frame with cleaned biologically weighted annotations\n\n\n\n\nweight_spectral\n\n\nNumeric weight for spectral similarity score (0-1)\n\n\n\n\nweight_biological\n\n\nNumeric weight for biological source score (0-1)\n\n\n\n\nweight_chemical\n\n\nNumeric weight for chemical consistency score (0-1)\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nNumeric score for ClassyFire kingdom match\n\n\n\n\nscore_chemical_cla_superclass\n\n\nNumeric score for ClassyFire superclass match\n\n\n\n\nscore_chemical_cla_class\n\n\nNumeric score for ClassyFire class match\n\n\n\n\nscore_chemical_cla_parent\n\n\nNumeric score for ClassyFire parent match (highest)\n\n\n\n\nscore_chemical_npc_pathway\n\n\nNumeric score for NPC pathway match\n\n\n\n\nscore_chemical_npc_superclass\n\n\nNumeric score for NPC superclass match\n\n\n\n\nscore_chemical_npc_class\n\n\nNumeric score for NPC class match (highest)\n\n\n\n\n\n\nData frame containing chemically weighted annotations with chemical consistency scores and final weighted scores\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/weight_chemo.html#weight-chemo",
    "href": "man/weight_chemo.html#weight-chemo",
    "title": "tima",
    "section": "",
    "text": "This function weights biologically weighted annotations according to their chemical consistency by comparing chemical taxonomy (ClassyFire, NPClassifier) across molecular network neighbors. Higher chemical consistency within network components results in higher chemical scores.\n\n\n\nweight_chemo(\n  annot_table_wei_bio_clean = get(\"annot_table_wei_bio_clean\", envir = parent.frame()),\n  weight_spectral = get(\"weight_spectral\", envir = parent.frame()),\n  weight_biological = get(\"weight_biological\", envir = parent.frame()),\n  weight_chemical = get(\"weight_chemical\", envir = parent.frame()),\n  score_chemical_cla_kingdom = get(\"score_chemical_cla_kingdom\", envir = parent.frame()),\n  score_chemical_cla_superclass = get(\"score_chemical_cla_superclass\", envir =\n    parent.frame()),\n  score_chemical_cla_class = get(\"score_chemical_cla_class\", envir = parent.frame()),\n  score_chemical_cla_parent = get(\"score_chemical_cla_parent\", envir = parent.frame()),\n  score_chemical_npc_pathway = get(\"score_chemical_npc_pathway\", envir = parent.frame()),\n  score_chemical_npc_superclass = get(\"score_chemical_npc_superclass\", envir =\n    parent.frame()),\n  score_chemical_npc_class = get(\"score_chemical_npc_class\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_bio_clean\n\n\nData frame with cleaned biologically weighted annotations\n\n\n\n\nweight_spectral\n\n\nNumeric weight for spectral similarity score (0-1)\n\n\n\n\nweight_biological\n\n\nNumeric weight for biological source score (0-1)\n\n\n\n\nweight_chemical\n\n\nNumeric weight for chemical consistency score (0-1)\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nNumeric score for ClassyFire kingdom match\n\n\n\n\nscore_chemical_cla_superclass\n\n\nNumeric score for ClassyFire superclass match\n\n\n\n\nscore_chemical_cla_class\n\n\nNumeric score for ClassyFire class match\n\n\n\n\nscore_chemical_cla_parent\n\n\nNumeric score for ClassyFire parent match (highest)\n\n\n\n\nscore_chemical_npc_pathway\n\n\nNumeric score for NPC pathway match\n\n\n\n\nscore_chemical_npc_superclass\n\n\nNumeric score for NPC superclass match\n\n\n\n\nscore_chemical_npc_class\n\n\nNumeric score for NPC class match (highest)\n\n\n\n\n\n\nData frame containing chemically weighted annotations with chemical consistency scores and final weighted scores\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/export_spectra_rds.html",
    "href": "man/export_spectra_rds.html",
    "title": "tima",
    "section": "",
    "text": "This function exports a Spectra object to an RDS file format for efficient storage and later retrieval. Only exports spectra with valid compound IDs.\n\n\n\nexport_spectra_rds(file, spectra)\n\n\n\n\n\n\n\nfile\n\n\nCharacter string path where spectra will be exported as RDS file\n\n\n\n\nspectra\n\n\nSpectra object containing spectral data to export\n\n\n\n\n\n\nNULL (invisibly). Saves spectra to file as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/export_spectra_rds.html#export-spectra-rds",
    "href": "man/export_spectra_rds.html#export-spectra-rds",
    "title": "tima",
    "section": "",
    "text": "This function exports a Spectra object to an RDS file format for efficient storage and later retrieval. Only exports spectra with valid compound IDs.\n\n\n\nexport_spectra_rds(file, spectra)\n\n\n\n\n\n\n\nfile\n\n\nCharacter string path where spectra will be exported as RDS file\n\n\n\n\nspectra\n\n\nSpectra object containing spectral data to export\n\n\n\n\n\n\nNULL (invisibly). Saves spectra to file as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/benchmark_taxize_spectra.html",
    "href": "man/benchmark_taxize_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function adds taxonomic information to benchmark features by linking them to structure-organism pairs and organism taxonomy data. For features with multiple organism associations, one is randomly selected.\n\n\n\nbenchmark_taxize_spectra(input, keys, org_tax_ott, output)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to the initial features file\n\n\n\n\nkeys\n\n\nCharacter string path to the structure-organism pair (SOP) keys file\n\n\n\n\norg_tax_ott\n\n\nCharacter string path to the organism taxonomy (OTT) file\n\n\n\n\noutput\n\n\nCharacter string path for the taxed benchmark output file\n\n\n\n\n\n\nBenchmark data often lacks clean taxonomic assignments. This function performs a controlled joining process with random sampling for ambiguous cases.\n\n\n\nCharacter string path to the taxed benchmark file\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/benchmark_taxize_spectra.html#benchmark-taxize-spectra",
    "href": "man/benchmark_taxize_spectra.html#benchmark-taxize-spectra",
    "title": "tima",
    "section": "",
    "text": "This function adds taxonomic information to benchmark features by linking them to structure-organism pairs and organism taxonomy data. For features with multiple organism associations, one is randomly selected.\n\n\n\nbenchmark_taxize_spectra(input, keys, org_tax_ott, output)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to the initial features file\n\n\n\n\nkeys\n\n\nCharacter string path to the structure-organism pair (SOP) keys file\n\n\n\n\norg_tax_ott\n\n\nCharacter string path to the organism taxonomy (OTT) file\n\n\n\n\noutput\n\n\nCharacter string path for the taxed benchmark output file\n\n\n\n\n\n\nBenchmark data often lacks clean taxonomic assignments. This function performs a controlled joining process with random sampling for ambiguous cases.\n\n\n\nCharacter string path to the taxed benchmark file\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/run_app.html",
    "href": "man/run_app.html",
    "title": "tima",
    "section": "",
    "text": "This function launches the TIMA Shiny web application for interactive metabolite annotation. It automatically detects if running inside a Docker container and adjusts network settings accordingly.\n\n\n\nrun_app(host = \"127.0.0.1\", port = 3838, browser = TRUE)\n\n\n\n\n\n\n\nhost\n\n\nCharacter string specifying the host/IP address to listen on. Default: \"127.0.0.1\" (localhost). Use \"0.0.0.0\" to allow external connections.\n\n\n\n\nport\n\n\nInteger port number to listen on. Default: 3838\n\n\n\n\nbrowser\n\n\nLogical whether to automatically launch a web browser when starting the app. Default: TRUE. Automatically set to FALSE in Docker.\n\n\n\n\n\n\nNULL (invisibly). Launches the Shiny app as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Launch app on localhost\nrun_app()\n\n# Launch on custom port\nrun_app(port = 8080)\n\n# Allow external connections\nrun_app(host = \"0.0.0.0\", port = 3838)",
    "crumbs": [
      "Get started",
      "Functions",
      "run_app"
    ]
  },
  {
    "objectID": "man/run_app.html#run-app",
    "href": "man/run_app.html#run-app",
    "title": "tima",
    "section": "",
    "text": "This function launches the TIMA Shiny web application for interactive metabolite annotation. It automatically detects if running inside a Docker container and adjusts network settings accordingly.\n\n\n\nrun_app(host = \"127.0.0.1\", port = 3838, browser = TRUE)\n\n\n\n\n\n\n\nhost\n\n\nCharacter string specifying the host/IP address to listen on. Default: \"127.0.0.1\" (localhost). Use \"0.0.0.0\" to allow external connections.\n\n\n\n\nport\n\n\nInteger port number to listen on. Default: 3838\n\n\n\n\nbrowser\n\n\nLogical whether to automatically launch a web browser when starting the app. Default: TRUE. Automatically set to FALSE in Docker.\n\n\n\n\n\n\nNULL (invisibly). Launches the Shiny app as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Launch app on localhost\nrun_app()\n\n# Launch on custom port\nrun_app(port = 8080)\n\n# Allow external connections\nrun_app(host = \"0.0.0.0\", port = 3838)",
    "crumbs": [
      "Get started",
      "Functions",
      "run_app"
    ]
  },
  {
    "objectID": "man/create_edges_spectra.html",
    "href": "man/create_edges_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function creates molecular network edges based on MS2 fragmentation spectra similarity. Compares all spectra against each other using spectral similarity metrics to identify related features.\n\n\n\ncreate_edges_spectra(\n  input = get_params(step = \"create_edges_spectra\")\\$files\\$spectral\\$raw,\n  output = get_params(step =\n    \"create_edges_spectra\")\\$files\\$networks\\$spectral\\$edges\\$raw\\$spectral,\n  name_source = get_params(step = \"create_edges_spectra\")\\$names\\$source,\n  name_target = get_params(step = \"create_edges_spectra\")\\$names\\$target,\n  method = get_params(step = \"create_edges_spectra\")\\$similarities\\$methods\\$edges,\n  threshold = get_params(step = \"create_edges_spectra\")\\$similarities\\$thresholds\\$edges,\n  matched_peaks = get_params(step =\n    \"create_edges_spectra\")\\$similarities\\$thresholds\\$matched_peaks,\n  ppm = get_params(step = \"create_edges_spectra\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms2,\n  dalton = get_params(step = \"create_edges_spectra\")\\$ms\\$tolerances\\$mass\\$dalton\\$ms2,\n  qutoff = get_params(step = \"create_edges_spectra\")\\$ms\\$thresholds\\$ms2\\$intensity\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path or list of paths to query MGF file(s) containing spectra\n\n\n\n\noutput\n\n\nCharacter string path for output edges file\n\n\n\n\nname_source\n\n\nCharacter string name of source feature column\n\n\n\n\nname_target\n\n\nCharacter string name of target feature column\n\n\n\n\nmethod\n\n\nCharacter string similarity method to use\n\n\n\n\nthreshold\n\n\nNumeric minimum similarity threshold (0-1) to report edge\n\n\n\n\nmatched_peaks\n\n\nInteger minimum number of matched peaks required\n\n\n\n\nppm\n\n\nNumeric relative mass tolerance in ppm\n\n\n\n\ndalton\n\n\nNumeric absolute mass tolerance in Daltons\n\n\n\n\nqutoff\n\n\nNumeric intensity cutoff below which MS2 fragments are removed\n\n\n\n\n\n\nCharacter string path to the created spectral edges file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_params(step = \"create_edges_spectra\")$files$spectral$raw\n)\ncreate_edges_spectra()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "create_edges_spectra"
    ]
  },
  {
    "objectID": "man/create_edges_spectra.html#create-edges-spectra",
    "href": "man/create_edges_spectra.html#create-edges-spectra",
    "title": "tima",
    "section": "",
    "text": "This function creates molecular network edges based on MS2 fragmentation spectra similarity. Compares all spectra against each other using spectral similarity metrics to identify related features.\n\n\n\ncreate_edges_spectra(\n  input = get_params(step = \"create_edges_spectra\")\\$files\\$spectral\\$raw,\n  output = get_params(step =\n    \"create_edges_spectra\")\\$files\\$networks\\$spectral\\$edges\\$raw\\$spectral,\n  name_source = get_params(step = \"create_edges_spectra\")\\$names\\$source,\n  name_target = get_params(step = \"create_edges_spectra\")\\$names\\$target,\n  method = get_params(step = \"create_edges_spectra\")\\$similarities\\$methods\\$edges,\n  threshold = get_params(step = \"create_edges_spectra\")\\$similarities\\$thresholds\\$edges,\n  matched_peaks = get_params(step =\n    \"create_edges_spectra\")\\$similarities\\$thresholds\\$matched_peaks,\n  ppm = get_params(step = \"create_edges_spectra\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms2,\n  dalton = get_params(step = \"create_edges_spectra\")\\$ms\\$tolerances\\$mass\\$dalton\\$ms2,\n  qutoff = get_params(step = \"create_edges_spectra\")\\$ms\\$thresholds\\$ms2\\$intensity\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path or list of paths to query MGF file(s) containing spectra\n\n\n\n\noutput\n\n\nCharacter string path for output edges file\n\n\n\n\nname_source\n\n\nCharacter string name of source feature column\n\n\n\n\nname_target\n\n\nCharacter string name of target feature column\n\n\n\n\nmethod\n\n\nCharacter string similarity method to use\n\n\n\n\nthreshold\n\n\nNumeric minimum similarity threshold (0-1) to report edge\n\n\n\n\nmatched_peaks\n\n\nInteger minimum number of matched peaks required\n\n\n\n\nppm\n\n\nNumeric relative mass tolerance in ppm\n\n\n\n\ndalton\n\n\nNumeric absolute mass tolerance in Daltons\n\n\n\n\nqutoff\n\n\nNumeric intensity cutoff below which MS2 fragments are removed\n\n\n\n\n\n\nCharacter string path to the created spectral edges file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_params(step = \"create_edges_spectra\")$files$spectral$raw\n)\ncreate_edges_spectra()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "create_edges_spectra"
    ]
  },
  {
    "objectID": "man/complement_metadata_structures.html",
    "href": "man/complement_metadata_structures.html",
    "title": "tima",
    "section": "",
    "text": "This function complements structural metadata by joining stereochemistry, metadata, names, and chemical taxonomy information from reference libraries. Enriches annotation results with comprehensive structure information.\n\n\n\ncomplement_metadata_structures(\n  df,\n  str_stereo = get(\"str_stereo\", envir = parent.frame()),\n  str_met = get(\"str_met\", envir = parent.frame()),\n  str_nam = get(\"str_nam\", envir = parent.frame()),\n  str_tax_cla = get(\"str_tax_cla\", envir = parent.frame()),\n  str_tax_npc = get(\"str_tax_npc\", envir = parent.frame())\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame with structural metadata to be complemented\n\n\n\n\nstr_stereo\n\n\nCharacter string path to file with structure stereochemistry\n\n\n\n\nstr_met\n\n\nCharacter string path to file with structure metadata\n\n\n\n\nstr_nam\n\n\nCharacter string path to file with structure names\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to file with ClassyFire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to file with NPClassifier taxonomy\n\n\n\n\n\n\nData frame with enriched structural metadata\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/complement_metadata_structures.html#complement-metadata-of-structures",
    "href": "man/complement_metadata_structures.html#complement-metadata-of-structures",
    "title": "tima",
    "section": "",
    "text": "This function complements structural metadata by joining stereochemistry, metadata, names, and chemical taxonomy information from reference libraries. Enriches annotation results with comprehensive structure information.\n\n\n\ncomplement_metadata_structures(\n  df,\n  str_stereo = get(\"str_stereo\", envir = parent.frame()),\n  str_met = get(\"str_met\", envir = parent.frame()),\n  str_nam = get(\"str_nam\", envir = parent.frame()),\n  str_tax_cla = get(\"str_tax_cla\", envir = parent.frame()),\n  str_tax_npc = get(\"str_tax_npc\", envir = parent.frame())\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame with structural metadata to be complemented\n\n\n\n\nstr_stereo\n\n\nCharacter string path to file with structure stereochemistry\n\n\n\n\nstr_met\n\n\nCharacter string path to file with structure metadata\n\n\n\n\nstr_nam\n\n\nCharacter string path to file with structure names\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to file with ClassyFire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to file with NPClassifier taxonomy\n\n\n\n\n\n\nData frame with enriched structural metadata\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/fake_sop_columns.html",
    "href": "man/fake_sop_columns.html",
    "title": "tima",
    "section": "",
    "text": "This function creates an empty structure-organism pair (SOP) dataframe template with all standard column names and NA values. Used as a placeholder when actual SOP data is unavailable.\n\n\n\nfake_sop_columns()\n\n\n\n\nA single-row data frame with standard SOP columns filled with NA_character_ values\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/fake_sop_columns.html#fake-sop-columns",
    "href": "man/fake_sop_columns.html#fake-sop-columns",
    "title": "tima",
    "section": "",
    "text": "This function creates an empty structure-organism pair (SOP) dataframe template with all standard column names and NA values. Used as a placeholder when actual SOP data is unavailable.\n\n\n\nfake_sop_columns()\n\n\n\n\nA single-row data frame with standard SOP columns filled with NA_character_ values\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/split_tables_sop.html",
    "href": "man/split_tables_sop.html",
    "title": "tima",
    "section": "",
    "text": "This function splits a concatenated structure-organism pairs (SOP) table into separate standardized tables for structures, organisms, and their relationships. It processes SMILES strings, standardizes chemical structures, and creates normalized reference tables.\n\n\n\nsplit_tables_sop(table, cache)\n\n\n\n\n\n\n\ntable\n\n\nData frame containing combined structure-organism pair data with columns for structures (SMILES, InChI, names), organisms, and references\n\n\n\n\ncache\n\n\nCharacter string path to cache file where previously processed SMILES are stored, or NULL to skip caching\n\n\n\n\n\n\nA list of normalized data frames:\n\n\n\ntable_keys\n\n\nStructure-organism pairs with reference DOIs\n\n\n\n\ntable_structures_stereo\n\n\nStructure stereochemistry information\n\n\n\n\ntable_organisms\n\n\nOrganism taxonomy information\n\n\n\n\ntable_structural\n\n\nProcessed and standardized structure data\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/split_tables_sop.html#split-structure-organism-pairs-table",
    "href": "man/split_tables_sop.html#split-structure-organism-pairs-table",
    "title": "tima",
    "section": "",
    "text": "This function splits a concatenated structure-organism pairs (SOP) table into separate standardized tables for structures, organisms, and their relationships. It processes SMILES strings, standardizes chemical structures, and creates normalized reference tables.\n\n\n\nsplit_tables_sop(table, cache)\n\n\n\n\n\n\n\ntable\n\n\nData frame containing combined structure-organism pair data with columns for structures (SMILES, InChI, names), organisms, and references\n\n\n\n\ncache\n\n\nCharacter string path to cache file where previously processed SMILES are stored, or NULL to skip caching\n\n\n\n\n\n\nA list of normalized data frames:\n\n\n\ntable_keys\n\n\nStructure-organism pairs with reference DOIs\n\n\n\n\ntable_structures_stereo\n\n\nStructure stereochemistry information\n\n\n\n\ntable_organisms\n\n\nOrganism taxonomy information\n\n\n\n\ntable_structural\n\n\nProcessed and standardized structure data\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/harmonize_adducts.html",
    "href": "man/harmonize_adducts.html",
    "title": "tima",
    "section": "",
    "text": "This function harmonizes adduct definitions by replacing various adduct notations with standardized forms according to a translation table.\n\n\n\nharmonize_adducts(df, adducts_colname = \"adduct\", adducts_translations)\n\n\n\n\n\n\n\ndf\n\n\nDataframe containing adduct column to harmonize\n\n\n\n\nadducts_colname\n\n\nCharacter string name of the adduct column (default: \"adduct\")\n\n\n\n\nadducts_translations\n\n\nNamed character vector mapping original adduct notations (names) to standardized forms (values)\n\n\n\n\n\n\nThe dataframe with harmonized adduct column\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/harmonize_adducts.html#harmonize-adducts",
    "href": "man/harmonize_adducts.html#harmonize-adducts",
    "title": "tima",
    "section": "",
    "text": "This function harmonizes adduct definitions by replacing various adduct notations with standardized forms according to a translation table.\n\n\n\nharmonize_adducts(df, adducts_colname = \"adduct\", adducts_translations)\n\n\n\n\n\n\n\ndf\n\n\nDataframe containing adduct column to harmonize\n\n\n\n\nadducts_colname\n\n\nCharacter string name of the adduct column (default: \"adduct\")\n\n\n\n\nadducts_translations\n\n\nNamed character vector mapping original adduct notations (names) to standardized forms (values)\n\n\n\n\n\n\nThe dataframe with harmonized adduct column\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/load_yaml_files.html",
    "href": "man/load_yaml_files.html",
    "title": "tima",
    "section": "",
    "text": "This function loads YAML parameter files, preferring user-specified parameters over defaults when available. It combines default/user params with prepare_params files.\n\n\n\nload_yaml_files()\n\n\n\n\nA list containing:\n\n\n\nyamls_params\n\n\nNamed list of parsed YAML content\n\n\n\n\nyaml_files\n\n\nCharacter vector of file paths\n\n\n\n\nyaml_names\n\n\nCharacter vector of parameter names\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/load_yaml_files.html#load-yaml-files",
    "href": "man/load_yaml_files.html#load-yaml-files",
    "title": "tima",
    "section": "",
    "text": "This function loads YAML parameter files, preferring user-specified parameters over defaults when available. It combines default/user params with prepare_params files.\n\n\n\nload_yaml_files()\n\n\n\n\nA list containing:\n\n\n\nyamls_params\n\n\nNamed list of parsed YAML content\n\n\n\n\nyaml_files\n\n\nCharacter vector of file paths\n\n\n\n\nyaml_names\n\n\nCharacter vector of parameter names\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/change_params_small.html",
    "href": "man/change_params_small.html",
    "title": "tima",
    "section": "",
    "text": "This function helps changing convenience parameters.\n\n\n\nchange_params_small(\n  fil_pat = NULL,\n  fil_fea_raw = NULL,\n  fil_met_raw = NULL,\n  fil_sir_raw = NULL,\n  fil_spe_raw = NULL,\n  ms_pol = NULL,\n  org_tax = NULL,\n  hig_con = NULL,\n  summarize = NULL\n)\n\n\n\n\n\n\n\nfil_pat\n\n\nThe pattern identifying your whole job. You can put whatever you want. STRING\n\n\n\n\nfil_fea_raw\n\n\nThe path to the file containing your features’ intensities. Can be generated by mzmine, SLAW, or SIRIUS. STRING\n\n\n\n\nfil_met_raw\n\n\nThe path to the file containing your metadata. If your experiment contains a single taxon, you can provide it below instead. STRING\n\n\n\n\nfil_sir_raw\n\n\nThe directory containing the sirius annotations. STRING\n\n\n\n\nfil_spe_raw\n\n\nThe path to the file containing your features’ spectra. Can contain MS1 and/or MS2 spectra. STRING\n\n\n\n\nms_pol\n\n\nThe polarity used. Must be either \"pos\" or \"neg\". STRING\n\n\n\n\norg_tax\n\n\nIf your experiment contains a single taxon, its scientific name. \"Homo sapiens\". STRING\n\n\n\n\nhig_con\n\n\nFilter high confidence candidates only. BOOLEAN\n\n\n\n\nsummarize\n\n\nSummarize all candidates per feature to a single row. BOOLEAN\n\n\n\n\n\n\nYAML file with changed parameters.\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ntima::change_params_small(\n  fil_pat = \"myExamplePattern\",\n  fil_fea_raw = \"myExampleDir/myExampleFeatures.csv\",\n  fil_met_raw = \"myExampleDir2SomeWhereElse/myOptionalMetadata.tsv\",\n  fil_sir_raw = \"myExampleDir3/myAwesomeSiriusProject.zip\",\n  fil_spe_raw = \"myBeautifulSpectra.mgf\",\n  ms_pol = \"pos\",\n  org_tax = \"Gentiana lutea\",\n  hig_con = TRUE,\n  summarize = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "change_params_small"
    ]
  },
  {
    "objectID": "man/change_params_small.html#change-params-small",
    "href": "man/change_params_small.html#change-params-small",
    "title": "tima",
    "section": "",
    "text": "This function helps changing convenience parameters.\n\n\n\nchange_params_small(\n  fil_pat = NULL,\n  fil_fea_raw = NULL,\n  fil_met_raw = NULL,\n  fil_sir_raw = NULL,\n  fil_spe_raw = NULL,\n  ms_pol = NULL,\n  org_tax = NULL,\n  hig_con = NULL,\n  summarize = NULL\n)\n\n\n\n\n\n\n\nfil_pat\n\n\nThe pattern identifying your whole job. You can put whatever you want. STRING\n\n\n\n\nfil_fea_raw\n\n\nThe path to the file containing your features’ intensities. Can be generated by mzmine, SLAW, or SIRIUS. STRING\n\n\n\n\nfil_met_raw\n\n\nThe path to the file containing your metadata. If your experiment contains a single taxon, you can provide it below instead. STRING\n\n\n\n\nfil_sir_raw\n\n\nThe directory containing the sirius annotations. STRING\n\n\n\n\nfil_spe_raw\n\n\nThe path to the file containing your features’ spectra. Can contain MS1 and/or MS2 spectra. STRING\n\n\n\n\nms_pol\n\n\nThe polarity used. Must be either \"pos\" or \"neg\". STRING\n\n\n\n\norg_tax\n\n\nIf your experiment contains a single taxon, its scientific name. \"Homo sapiens\". STRING\n\n\n\n\nhig_con\n\n\nFilter high confidence candidates only. BOOLEAN\n\n\n\n\nsummarize\n\n\nSummarize all candidates per feature to a single row. BOOLEAN\n\n\n\n\n\n\nYAML file with changed parameters.\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ntima::change_params_small(\n  fil_pat = \"myExamplePattern\",\n  fil_fea_raw = \"myExampleDir/myExampleFeatures.csv\",\n  fil_met_raw = \"myExampleDir2SomeWhereElse/myOptionalMetadata.tsv\",\n  fil_sir_raw = \"myExampleDir3/myAwesomeSiriusProject.zip\",\n  fil_spe_raw = \"myBeautifulSpectra.mgf\",\n  ms_pol = \"pos\",\n  org_tax = \"Gentiana lutea\",\n  hig_con = TRUE,\n  summarize = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "change_params_small"
    ]
  },
  {
    "objectID": "man/filter_annotations.html",
    "href": "man/filter_annotations.html",
    "title": "tima",
    "section": "",
    "text": "This function filters initial annotations by removing MS1-only annotations that also have spectral matches, and optionally filtering by retention time tolerance when RT libraries are available.\n\n\n\nfilter_annotations(\n  annotations = get_params(step =\n    \"filter_annotations\")\\$files\\$annotations\\$prepared\\$structural,\n  features = get_params(step = \"filter_annotations\")\\$files\\$features\\$prepared,\n  rts = get_params(step = \"filter_annotations\")\\$files\\$libraries\\$temporal\\$prepared,\n  output = get_params(step = \"filter_annotations\")\\$files\\$annotations\\$filtered,\n  tolerance_rt = get_params(step = \"filter_annotations\")\\$ms\\$tolerances\\$rt\\$library\n)\n\n\n\n\n\n\n\nannotations\n\n\nCharacter vector or list of paths to prepared annotation files\n\n\n\n\nfeatures\n\n\nCharacter string path to prepared features file\n\n\n\n\nrts\n\n\nCharacter string path to prepared retention time library (optional)\n\n\n\n\noutput\n\n\nCharacter string path for filtered annotations output\n\n\n\n\ntolerance_rt\n\n\nNumeric RT tolerance in minutes for library matching\n\n\n\n\n\n\nCharacter string path to the filtered annotations file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nannotations &lt;- get_params(step = \"filter_annotations\")$files$annotations$prepared$structural[[2]] |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nfeatures &lt;- get_params(step = \"filter_annotations\")$files$features$prepared |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nrts &lt;- get_params(step = \"filter_annotations\")$files$libraries$temporal$prepared |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nget_file(url = paste0(dir, annotations), export = annotations)\nget_file(url = paste0(dir, features), export = features)\nget_file(url = paste0(dir, rts), export = rts)\nfilter_annotations(\n  annotations = annotations,\n  features = features,\n  rts = rts\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "filter_annotations"
    ]
  },
  {
    "objectID": "man/filter_annotations.html#filter-annotations",
    "href": "man/filter_annotations.html#filter-annotations",
    "title": "tima",
    "section": "",
    "text": "This function filters initial annotations by removing MS1-only annotations that also have spectral matches, and optionally filtering by retention time tolerance when RT libraries are available.\n\n\n\nfilter_annotations(\n  annotations = get_params(step =\n    \"filter_annotations\")\\$files\\$annotations\\$prepared\\$structural,\n  features = get_params(step = \"filter_annotations\")\\$files\\$features\\$prepared,\n  rts = get_params(step = \"filter_annotations\")\\$files\\$libraries\\$temporal\\$prepared,\n  output = get_params(step = \"filter_annotations\")\\$files\\$annotations\\$filtered,\n  tolerance_rt = get_params(step = \"filter_annotations\")\\$ms\\$tolerances\\$rt\\$library\n)\n\n\n\n\n\n\n\nannotations\n\n\nCharacter vector or list of paths to prepared annotation files\n\n\n\n\nfeatures\n\n\nCharacter string path to prepared features file\n\n\n\n\nrts\n\n\nCharacter string path to prepared retention time library (optional)\n\n\n\n\noutput\n\n\nCharacter string path for filtered annotations output\n\n\n\n\ntolerance_rt\n\n\nNumeric RT tolerance in minutes for library matching\n\n\n\n\n\n\nCharacter string path to the filtered annotations file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nannotations &lt;- get_params(step = \"filter_annotations\")$files$annotations$prepared$structural[[2]] |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nfeatures &lt;- get_params(step = \"filter_annotations\")$files$features$prepared |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nrts &lt;- get_params(step = \"filter_annotations\")$files$libraries$temporal$prepared |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nget_file(url = paste0(dir, annotations), export = annotations)\nget_file(url = paste0(dir, features), export = features)\nget_file(url = paste0(dir, rts), export = rts)\nfilter_annotations(\n  annotations = annotations,\n  features = features,\n  rts = rts\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "filter_annotations"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_lotus.html",
    "href": "man/prepare_libraries_sop_lotus.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares the LOTUS (LOng-lasting, cUraTed collection of cOnnectivity daTa for natural products) structure-organism pairs database. It standardizes columns, extracts 2D InChIKeys, rounds numeric values, and removes duplicates.\n\n\n\nprepare_libraries_sop_lotus(\n  input = get_params(step = \"prepare_libraries_sop_lotus\")\\$files\\$libraries\\$sop\\$raw\\$lotus,\n  output = get_params(step =\n    \"prepare_libraries_sop_lotus\")\\$files\\$libraries\\$sop\\$prepared\\$lotus\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to the raw LOTUS data file\n\n\n\n\noutput\n\n\nCharacter string path for the prepared output file\n\n\n\n\n\n\nCharacter string path to the prepared structure-organism pairs library file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_lotus()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_lotus"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_lotus.html#prepare-libraries-of-structure-organism-pairs-lotus",
    "href": "man/prepare_libraries_sop_lotus.html#prepare-libraries-of-structure-organism-pairs-lotus",
    "title": "tima",
    "section": "",
    "text": "This function prepares the LOTUS (LOng-lasting, cUraTed collection of cOnnectivity daTa for natural products) structure-organism pairs database. It standardizes columns, extracts 2D InChIKeys, rounds numeric values, and removes duplicates.\n\n\n\nprepare_libraries_sop_lotus(\n  input = get_params(step = \"prepare_libraries_sop_lotus\")\\$files\\$libraries\\$sop\\$raw\\$lotus,\n  output = get_params(step =\n    \"prepare_libraries_sop_lotus\")\\$files\\$libraries\\$sop\\$prepared\\$lotus\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to the raw LOTUS data file\n\n\n\n\noutput\n\n\nCharacter string path for the prepared output file\n\n\n\n\n\n\nCharacter string path to the prepared structure-organism pairs library file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_lotus()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_lotus"
    ]
  },
  {
    "objectID": "man/weight_bio.html",
    "href": "man/weight_bio.html",
    "title": "tima",
    "section": "",
    "text": "This function weights MS annotations according to their biological source by comparing the taxonomic hierarchy of candidate structures’ reported organisms with the sample’s organism taxonomy. Higher taxonomic similarity results in higher biological scores.\n\n\n\nweight_bio(\n  annotation_table_taxed = get(\"annotation_table_taxed\", envir = parent.frame()),\n  structure_organism_pairs_table = get(\"structure_organism_pairs_table\", envir =\n    parent.frame()),\n  weight_spectral = get(\"weight_spectral\", envir = parent.frame()),\n  weight_biological = get(\"weight_biological\", envir = parent.frame()),\n  score_biological_domain = get(\"score_biological_domain\", envir = parent.frame()),\n  score_biological_kingdom = get(\"score_biological_kingdom\", envir = parent.frame()),\n  score_biological_phylum = get(\"score_biological_phylum\", envir = parent.frame()),\n  score_biological_class = get(\"score_biological_class\", envir = parent.frame()),\n  score_biological_order = get(\"score_biological_order\", envir = parent.frame()),\n  score_biological_family = get(\"score_biological_family\", envir = parent.frame()),\n  score_biological_tribe = get(\"score_biological_tribe\", envir = parent.frame()),\n  score_biological_genus = get(\"score_biological_genus\", envir = parent.frame()),\n  score_biological_species = get(\"score_biological_species\", envir = parent.frame()),\n  score_biological_variety = get(\"score_biological_variety\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannotation_table_taxed\n\n\nData frame containing initial annotations with sample taxonomy information\n\n\n\n\nstructure_organism_pairs_table\n\n\nData frame containing structure-organism pairs with complete taxonomic hierarchies\n\n\n\n\nweight_spectral\n\n\nNumeric weight for spectral similarity score (0-1)\n\n\n\n\nweight_biological\n\n\nNumeric weight for biological source score (0-1)\n\n\n\n\nscore_biological_domain\n\n\nNumeric score for domain-level taxonomic match\n\n\n\n\nscore_biological_kingdom\n\n\nNumeric score for kingdom-level match\n\n\n\n\nscore_biological_phylum\n\n\nNumeric score for phylum-level match\n\n\n\n\nscore_biological_class\n\n\nNumeric score for class-level match\n\n\n\n\nscore_biological_order\n\n\nNumeric score for order-level match\n\n\n\n\nscore_biological_family\n\n\nNumeric score for family-level match\n\n\n\n\nscore_biological_tribe\n\n\nNumeric score for tribe-level match\n\n\n\n\nscore_biological_genus\n\n\nNumeric score for genus-level match\n\n\n\n\nscore_biological_species\n\n\nNumeric score for species-level match\n\n\n\n\nscore_biological_variety\n\n\nNumeric score for variety-level match (highest)\n\n\n\n\n\n\nData frame containing biologically weighted annotations with biological scores and combined weighted scores\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/weight_bio.html#weight-bio",
    "href": "man/weight_bio.html#weight-bio",
    "title": "tima",
    "section": "",
    "text": "This function weights MS annotations according to their biological source by comparing the taxonomic hierarchy of candidate structures’ reported organisms with the sample’s organism taxonomy. Higher taxonomic similarity results in higher biological scores.\n\n\n\nweight_bio(\n  annotation_table_taxed = get(\"annotation_table_taxed\", envir = parent.frame()),\n  structure_organism_pairs_table = get(\"structure_organism_pairs_table\", envir =\n    parent.frame()),\n  weight_spectral = get(\"weight_spectral\", envir = parent.frame()),\n  weight_biological = get(\"weight_biological\", envir = parent.frame()),\n  score_biological_domain = get(\"score_biological_domain\", envir = parent.frame()),\n  score_biological_kingdom = get(\"score_biological_kingdom\", envir = parent.frame()),\n  score_biological_phylum = get(\"score_biological_phylum\", envir = parent.frame()),\n  score_biological_class = get(\"score_biological_class\", envir = parent.frame()),\n  score_biological_order = get(\"score_biological_order\", envir = parent.frame()),\n  score_biological_family = get(\"score_biological_family\", envir = parent.frame()),\n  score_biological_tribe = get(\"score_biological_tribe\", envir = parent.frame()),\n  score_biological_genus = get(\"score_biological_genus\", envir = parent.frame()),\n  score_biological_species = get(\"score_biological_species\", envir = parent.frame()),\n  score_biological_variety = get(\"score_biological_variety\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannotation_table_taxed\n\n\nData frame containing initial annotations with sample taxonomy information\n\n\n\n\nstructure_organism_pairs_table\n\n\nData frame containing structure-organism pairs with complete taxonomic hierarchies\n\n\n\n\nweight_spectral\n\n\nNumeric weight for spectral similarity score (0-1)\n\n\n\n\nweight_biological\n\n\nNumeric weight for biological source score (0-1)\n\n\n\n\nscore_biological_domain\n\n\nNumeric score for domain-level taxonomic match\n\n\n\n\nscore_biological_kingdom\n\n\nNumeric score for kingdom-level match\n\n\n\n\nscore_biological_phylum\n\n\nNumeric score for phylum-level match\n\n\n\n\nscore_biological_class\n\n\nNumeric score for class-level match\n\n\n\n\nscore_biological_order\n\n\nNumeric score for order-level match\n\n\n\n\nscore_biological_family\n\n\nNumeric score for family-level match\n\n\n\n\nscore_biological_tribe\n\n\nNumeric score for tribe-level match\n\n\n\n\nscore_biological_genus\n\n\nNumeric score for genus-level match\n\n\n\n\nscore_biological_species\n\n\nNumeric score for species-level match\n\n\n\n\nscore_biological_variety\n\n\nNumeric score for variety-level match (highest)\n\n\n\n\n\n\nData frame containing biologically weighted annotations with biological scores and combined weighted scores\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/parse_cli_params.html",
    "href": "man/parse_cli_params.html",
    "title": "tima",
    "section": "",
    "text": "This function parses command-line interface (CLI) arguments and merges them into the parameter configuration structure. It uses a mapping system to translate CLI argument names to their corresponding nested paths in the parameters list, applying appropriate type conversions.\n\n\n\nparse_cli_params(arguments, parameters)\n\n\n\n\n\n\n\narguments\n\n\nNamed list of CLI arguments from docopt or similar parser\n\n\n\n\nparameters\n\n\nNested list of default parameters to be updated with CLI values\n\n\n\n\n\n\nUpdated parameters list with CLI arguments merged in, maintaining the nested structure and applying type conversions as specified in the mappings\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/parse_cli_params.html#parse-cli-parameters",
    "href": "man/parse_cli_params.html#parse-cli-parameters",
    "title": "tima",
    "section": "",
    "text": "This function parses command-line interface (CLI) arguments and merges them into the parameter configuration structure. It uses a mapping system to translate CLI argument names to their corresponding nested paths in the parameters list, applying appropriate type conversions.\n\n\n\nparse_cli_params(arguments, parameters)\n\n\n\n\n\n\n\narguments\n\n\nNamed list of CLI arguments from docopt or similar parser\n\n\n\n\nparameters\n\n\nNested list of default parameters to be updated with CLI values\n\n\n\n\n\n\nUpdated parameters list with CLI arguments merged in, maintaining the nested structure and applying type conversions as specified in the mappings\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/replace_id.html",
    "href": "man/replace_id.html",
    "title": "tima",
    "section": "",
    "text": "This function replaces the default ID in file paths with user-specified values. It handles both GNPS job IDs and custom filename patterns, with special handling for example files.\n\n\n\nreplace_id(\n  x,\n  user_filename = get_params(step = \"prepare_params\")\\$files\\$pattern,\n  user_gnps = get_params(step = \"prepare_params\")\\$gnps\\$id,\n  example_gnps = get_default_paths()\\$gnps\\$example\n)\n\n\n\n\n\n\n\nx\n\n\nCharacter string containing the file path with default ID\n\n\n\n\nuser_filename\n\n\nCharacter string for a custom filename pattern\n\n\n\n\nuser_gnps\n\n\nCharacter string for a GNPS job ID (if NULL, uses user_filename)\n\n\n\n\nexample_gnps\n\n\nCharacter string for the example GNPS job ID to detect\n\n\n\n\n\n\nCharacter string with the ID replaced according to user specifications\n\n\n\n\nlibrary(\"tima\")\n\nreplace_id(\n  x = \"example/123456_features.tsv\",\n  user_gnps = NULL,\n  user_filename = \"Foo\"\n)"
  },
  {
    "objectID": "man/replace_id.html#replace-id-in-file-paths",
    "href": "man/replace_id.html#replace-id-in-file-paths",
    "title": "tima",
    "section": "",
    "text": "This function replaces the default ID in file paths with user-specified values. It handles both GNPS job IDs and custom filename patterns, with special handling for example files.\n\n\n\nreplace_id(\n  x,\n  user_filename = get_params(step = \"prepare_params\")\\$files\\$pattern,\n  user_gnps = get_params(step = \"prepare_params\")\\$gnps\\$id,\n  example_gnps = get_default_paths()\\$gnps\\$example\n)\n\n\n\n\n\n\n\nx\n\n\nCharacter string containing the file path with default ID\n\n\n\n\nuser_filename\n\n\nCharacter string for a custom filename pattern\n\n\n\n\nuser_gnps\n\n\nCharacter string for a GNPS job ID (if NULL, uses user_filename)\n\n\n\n\nexample_gnps\n\n\nCharacter string for the example GNPS job ID to detect\n\n\n\n\n\n\nCharacter string with the ID replaced according to user specifications\n\n\n\n\nlibrary(\"tima\")\n\nreplace_id(\n  x = \"example/123456_features.tsv\",\n  user_gnps = NULL,\n  user_filename = \"Foo\"\n)"
  },
  {
    "objectID": "man/create_components.html",
    "href": "man/create_components.html",
    "title": "tima",
    "section": "",
    "text": "This function creates network components (connected subgraphs) from edge lists using igraph. Each component represents a set of features that are connected through spectral similarity or other relationships.\n\n\n\ncreate_components(\n  input = get_params(step = \"create_components\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  output = get_params(step = \"create_components\")\\$files\\$networks\\$spectral\\$components\\$raw\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of file path(s) containing edge data. Files should have feature_source and feature_target columns.\n\n\n\n\noutput\n\n\nCharacter string path for the output components file\n\n\n\n\n\n\nCharacter string path to the created components file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo, data_interim)\nget_file(\n  url = paste0(dir, \"features/example_edges.tsv\"),\n  export = get_params(step = \"create_components\")$files$networks$spectral$edges$prepared\n)\ncreate_components()\nunlink(\"data\", recursive = TRUE)"
  },
  {
    "objectID": "man/create_components.html#create-components",
    "href": "man/create_components.html#create-components",
    "title": "tima",
    "section": "",
    "text": "This function creates network components (connected subgraphs) from edge lists using igraph. Each component represents a set of features that are connected through spectral similarity or other relationships.\n\n\n\ncreate_components(\n  input = get_params(step = \"create_components\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  output = get_params(step = \"create_components\")\\$files\\$networks\\$spectral\\$components\\$raw\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of file path(s) containing edge data. Files should have feature_source and feature_target columns.\n\n\n\n\noutput\n\n\nCharacter string path for the output components file\n\n\n\n\n\n\nCharacter string path to the created components file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo, data_interim)\nget_file(\n  url = paste0(dir, \"features/example_edges.tsv\"),\n  export = get_params(step = \"create_components\")$files$networks$spectral$edges$prepared\n)\ncreate_components()\nunlink(\"data\", recursive = TRUE)"
  },
  {
    "objectID": "man/get_organism_taxonomy_ott.html",
    "href": "man/get_organism_taxonomy_ott.html",
    "title": "tima",
    "section": "",
    "text": "This function retrieves taxonomic information from the Open Tree of Life (OTT) taxonomy service. It cleans organism names, queries the OTT API, and returns structured taxonomic data including OTT IDs and hierarchical classifications.\n\n\n\nget_organism_taxonomy_ott(\n  df,\n  url = \"https://api.opentreeoflife.org/v3/taxonomy/about\",\n  retry = TRUE\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing organism names in a column named \"organism\"\n\n\n\n\nurl\n\n\nCharacter string URL of the OTT API endpoint (default: production API, can be changed for testing)\n\n\n\n\nretry\n\n\nLogical indicating whether to retry failed queries using only the generic epithet (genus name) when full species names fail (default: TRUE)\n\n\n\n\n\n\nData frame with taxonomic information including OTT IDs, ranks, and taxonomic hierarchy. Returns empty template if API is unavailable.\n\n\n\n\nlibrary(\"tima\")\n\n# Single organism\ndf &lt;- data.frame(organism = \"Homo sapiens\")\ntaxonomy &lt;- get_organism_taxonomy_ott(df)\n\n# Multiple organisms\ndf &lt;- data.frame(organism = c(\"Homo sapiens\", \"Arabidopsis thaliana\"))\ntaxonomy &lt;- get_organism_taxonomy_ott(df)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_organism_taxonomy_ott"
    ]
  },
  {
    "objectID": "man/get_organism_taxonomy_ott.html#get-organism-taxonomy-open-tree-of-life-taxonomy",
    "href": "man/get_organism_taxonomy_ott.html#get-organism-taxonomy-open-tree-of-life-taxonomy",
    "title": "tima",
    "section": "",
    "text": "This function retrieves taxonomic information from the Open Tree of Life (OTT) taxonomy service. It cleans organism names, queries the OTT API, and returns structured taxonomic data including OTT IDs and hierarchical classifications.\n\n\n\nget_organism_taxonomy_ott(\n  df,\n  url = \"https://api.opentreeoflife.org/v3/taxonomy/about\",\n  retry = TRUE\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing organism names in a column named \"organism\"\n\n\n\n\nurl\n\n\nCharacter string URL of the OTT API endpoint (default: production API, can be changed for testing)\n\n\n\n\nretry\n\n\nLogical indicating whether to retry failed queries using only the generic epithet (genus name) when full species names fail (default: TRUE)\n\n\n\n\n\n\nData frame with taxonomic information including OTT IDs, ranks, and taxonomic hierarchy. Returns empty template if API is unavailable.\n\n\n\n\nlibrary(\"tima\")\n\n# Single organism\ndf &lt;- data.frame(organism = \"Homo sapiens\")\ntaxonomy &lt;- get_organism_taxonomy_ott(df)\n\n# Multiple organisms\ndf &lt;- data.frame(organism = c(\"Homo sapiens\", \"Arabidopsis thaliana\"))\ntaxonomy &lt;- get_organism_taxonomy_ott(df)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_organism_taxonomy_ott"
    ]
  },
  {
    "objectID": "man/decorate_chemo.html",
    "href": "man/decorate_chemo.html",
    "title": "tima",
    "section": "",
    "text": "This function logs summary statistics about chemically weighted annotations, showing how many structures were reranked at each chemical classification level. Uses cascading filters where each level builds on the previous (higher specificity requires passing lower levels). The function validates required columns and handles empty inputs gracefully.\n\n\n\ndecorate_chemo(\n  annot_table_wei_chemo = get(\"annot_table_wei_chemo\", envir = parent.frame()),\n  score_chemical_cla_kingdom = get(\"score_chemical_cla_kingdom\", envir = parent.frame()),\n  score_chemical_cla_superclass = get(\"score_chemical_cla_superclass\", envir =\n    parent.frame()),\n  score_chemical_cla_class = get(\"score_chemical_cla_class\", envir = parent.frame()),\n  score_chemical_cla_parent = get(\"score_chemical_cla_parent\", envir = parent.frame()),\n  score_chemical_npc_pathway = get(\"score_chemical_npc_pathway\", envir = parent.frame()),\n  score_chemical_npc_superclass = get(\"score_chemical_npc_superclass\", envir =\n    parent.frame()),\n  score_chemical_npc_class = get(\"score_chemical_npc_class\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_chemo\n\n\nData frame containing chemically weighted annotations\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nNumeric minimum score for Classyfire kingdom\n\n\n\n\nscore_chemical_cla_superclass\n\n\nNumeric minimum score for Classyfire superclass\n\n\n\n\nscore_chemical_cla_class\n\n\nNumeric minimum score for Classyfire class\n\n\n\n\nscore_chemical_cla_parent\n\n\nNumeric minimum score for Classyfire parent\n\n\n\n\nscore_chemical_npc_pathway\n\n\nNumeric minimum score for NPClassifier pathway\n\n\n\n\nscore_chemical_npc_superclass\n\n\nNumeric minimum score for NPClassifier superclass\n\n\n\n\nscore_chemical_npc_class\n\n\nNumeric minimum score for NPClassifier class\n\n\n\n\n\n\nThe input annotation table (unchanged), for use in pipelines\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/decorate_chemo.html#decorate-chemo",
    "href": "man/decorate_chemo.html#decorate-chemo",
    "title": "tima",
    "section": "",
    "text": "This function logs summary statistics about chemically weighted annotations, showing how many structures were reranked at each chemical classification level. Uses cascading filters where each level builds on the previous (higher specificity requires passing lower levels). The function validates required columns and handles empty inputs gracefully.\n\n\n\ndecorate_chemo(\n  annot_table_wei_chemo = get(\"annot_table_wei_chemo\", envir = parent.frame()),\n  score_chemical_cla_kingdom = get(\"score_chemical_cla_kingdom\", envir = parent.frame()),\n  score_chemical_cla_superclass = get(\"score_chemical_cla_superclass\", envir =\n    parent.frame()),\n  score_chemical_cla_class = get(\"score_chemical_cla_class\", envir = parent.frame()),\n  score_chemical_cla_parent = get(\"score_chemical_cla_parent\", envir = parent.frame()),\n  score_chemical_npc_pathway = get(\"score_chemical_npc_pathway\", envir = parent.frame()),\n  score_chemical_npc_superclass = get(\"score_chemical_npc_superclass\", envir =\n    parent.frame()),\n  score_chemical_npc_class = get(\"score_chemical_npc_class\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nannot_table_wei_chemo\n\n\nData frame containing chemically weighted annotations\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nNumeric minimum score for Classyfire kingdom\n\n\n\n\nscore_chemical_cla_superclass\n\n\nNumeric minimum score for Classyfire superclass\n\n\n\n\nscore_chemical_cla_class\n\n\nNumeric minimum score for Classyfire class\n\n\n\n\nscore_chemical_cla_parent\n\n\nNumeric minimum score for Classyfire parent\n\n\n\n\nscore_chemical_npc_pathway\n\n\nNumeric minimum score for NPClassifier pathway\n\n\n\n\nscore_chemical_npc_superclass\n\n\nNumeric minimum score for NPClassifier superclass\n\n\n\n\nscore_chemical_npc_class\n\n\nNumeric minimum score for NPClassifier class\n\n\n\n\n\n\nThe input annotation table (unchanged), for use in pipelines\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/transform_score_sirius_csi.html",
    "href": "man/transform_score_sirius_csi.html",
    "title": "tima",
    "section": "",
    "text": "This function transforms SIRIUS CSI (Compound Structure Identification) scores using a sigmoid function. The transformation maps raw scores to a 0-1 range for better interpretability.\n\n\n\ntransform_score_sirius_csi(csi_score = NULL, K = 50, scale = 10)\n\n\n\n\n\n\n\ncsi_score\n\n\nNumeric SIRIUS CSI score (can be negative, NA, NULL, or absent)\n\n\n\n\nK\n\n\nNumeric shift parameter to adjust the sigmoid center (default: 50)\n\n\n\n\nscale\n\n\nNumeric scale parameter controlling sigmoid steepness (default: 10)\n\n\n\n\n\n\nThis is an experimental transformation not officially approved by SIRIUS developers. The sigmoid function is: 1 / (1 + exp(-(score + K) / scale))\n\n\n\nNumeric transformed score in the range (0, 1), or NA if input is NA/NULL/absent\n\n\n\n\nlibrary(\"tima\")\n\n# Transform a single score\ntransform_score_sirius_csi(csi_score = -20)\n\n# Transform with custom parameters\ntransform_score_sirius_csi(csi_score = -20, K = 30, scale = 5)\n\n# Vectorized transformation\nscores &lt;- c(-50, -20, 0, 20, 50)\ntransform_score_sirius_csi(csi_score = scores)\n\n# Handle NA values\nscores_with_na &lt;- c(-20, NA, 0, 20)\ntransform_score_sirius_csi(csi_score = scores_with_na)\n\n# Handle missing/absent score\ntransform_score_sirius_csi()"
  },
  {
    "objectID": "man/transform_score_sirius_csi.html#transform-sirius-csi-score",
    "href": "man/transform_score_sirius_csi.html#transform-sirius-csi-score",
    "title": "tima",
    "section": "",
    "text": "This function transforms SIRIUS CSI (Compound Structure Identification) scores using a sigmoid function. The transformation maps raw scores to a 0-1 range for better interpretability.\n\n\n\ntransform_score_sirius_csi(csi_score = NULL, K = 50, scale = 10)\n\n\n\n\n\n\n\ncsi_score\n\n\nNumeric SIRIUS CSI score (can be negative, NA, NULL, or absent)\n\n\n\n\nK\n\n\nNumeric shift parameter to adjust the sigmoid center (default: 50)\n\n\n\n\nscale\n\n\nNumeric scale parameter controlling sigmoid steepness (default: 10)\n\n\n\n\n\n\nThis is an experimental transformation not officially approved by SIRIUS developers. The sigmoid function is: 1 / (1 + exp(-(score + K) / scale))\n\n\n\nNumeric transformed score in the range (0, 1), or NA if input is NA/NULL/absent\n\n\n\n\nlibrary(\"tima\")\n\n# Transform a single score\ntransform_score_sirius_csi(csi_score = -20)\n\n# Transform with custom parameters\ntransform_score_sirius_csi(csi_score = -20, K = 30, scale = 5)\n\n# Vectorized transformation\nscores &lt;- c(-50, -20, 0, 20, 50)\ntransform_score_sirius_csi(csi_score = scores)\n\n# Handle NA values\nscores_with_na &lt;- c(-20, NA, 0, 20)\ntransform_score_sirius_csi(csi_score = scores_with_na)\n\n# Handle missing/absent score\ntransform_score_sirius_csi()"
  },
  {
    "objectID": "man/join_gnps_wrapper.html",
    "href": "man/join_gnps_wrapper.html",
    "title": "tima",
    "section": "",
    "text": "Performs GNPS-style peak matching between query and target mass lists with tolerance-based matching and precursor filtering\n\n\n\njoin_gnps_wrapper(x, y, xPrecursorMz, yPrecursorMz, tolerance, ppm)\n\n\n\n\n\n\n\nx\n\n\nNumeric vector of query m/z values\n\n\n\n\ny\n\n\nNumeric vector of target m/z values\n\n\n\n\nxPrecursorMz\n\n\nNumeric precursor m/z for query spectrum\n\n\n\n\nyPrecursorMz\n\n\nNumeric precursor m/z for target spectrum\n\n\n\n\ntolerance\n\n\nNumeric value specifying the absolute tolerance in Daltons\n\n\n\n\nppm\n\n\nNumeric value specifying the relative tolerance in ppm\n\n\n\n\n\n\nA list with two integer vectors:\n\n\n\nindices_x\n\n\nIndices of matched peaks in x\n\n\n\n\nindices_y\n\n\nIndices of matched peaks in y\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nquery_mz &lt;- c(100.05, 200.10, 300.15)\ntarget_mz &lt;- c(100.06, 200.11, 400.20)\nmatches &lt;- join_gnps_wrapper(\n  x = query_mz,\n  y = target_mz,\n  xPrecursorMz = 500.0,\n  yPrecursorMz = 500.0,\n  tolerance = 0.01,\n  ppm = 10\n)"
  },
  {
    "objectID": "man/join_gnps_wrapper.html#wrapper-for-the-c-function-join_gnps",
    "href": "man/join_gnps_wrapper.html#wrapper-for-the-c-function-join_gnps",
    "title": "tima",
    "section": "",
    "text": "Performs GNPS-style peak matching between query and target mass lists with tolerance-based matching and precursor filtering\n\n\n\njoin_gnps_wrapper(x, y, xPrecursorMz, yPrecursorMz, tolerance, ppm)\n\n\n\n\n\n\n\nx\n\n\nNumeric vector of query m/z values\n\n\n\n\ny\n\n\nNumeric vector of target m/z values\n\n\n\n\nxPrecursorMz\n\n\nNumeric precursor m/z for query spectrum\n\n\n\n\nyPrecursorMz\n\n\nNumeric precursor m/z for target spectrum\n\n\n\n\ntolerance\n\n\nNumeric value specifying the absolute tolerance in Daltons\n\n\n\n\nppm\n\n\nNumeric value specifying the relative tolerance in ppm\n\n\n\n\n\n\nA list with two integer vectors:\n\n\n\nindices_x\n\n\nIndices of matched peaks in x\n\n\n\n\nindices_y\n\n\nIndices of matched peaks in y\n\n\n\n\n\n\n\nlibrary(\"tima\")\n\nquery_mz &lt;- c(100.05, 200.10, 300.15)\ntarget_mz &lt;- c(100.06, 200.11, 400.20)\nmatches &lt;- join_gnps_wrapper(\n  x = query_mz,\n  y = target_mz,\n  xPrecursorMz = 500.0,\n  yPrecursorMz = 500.0,\n  tolerance = 0.01,\n  ppm = 10\n)"
  },
  {
    "objectID": "man/tima_full.html",
    "href": "man/tima_full.html",
    "title": "tima",
    "section": "",
    "text": "This function runs the complete TIMA annotation workflow from start to finish, including all preparation, library loading, annotation, weighting, and output steps. Executes the full targets pipeline and saves logs with timestamps.\n\n\n\ntima_full()\n\n\n\n\nNULL (invisibly). Runs complete workflow as side effect and saves timestamped log files.\n\n\n\n\nlibrary(\"tima\")\n\ntima_full()",
    "crumbs": [
      "Get started",
      "Functions",
      "tima_full"
    ]
  },
  {
    "objectID": "man/tima_full.html#tima-full",
    "href": "man/tima_full.html#tima-full",
    "title": "tima",
    "section": "",
    "text": "This function runs the complete TIMA annotation workflow from start to finish, including all preparation, library loading, annotation, weighting, and output steps. Executes the full targets pipeline and saves logs with timestamps.\n\n\n\ntima_full()\n\n\n\n\nNULL (invisibly). Runs complete workflow as side effect and saves timestamped log files.\n\n\n\n\nlibrary(\"tima\")\n\ntima_full()",
    "crumbs": [
      "Get started",
      "Functions",
      "tima_full"
    ]
  },
  {
    "objectID": "man/get_file.html",
    "href": "man/get_file.html",
    "title": "tima",
    "section": "",
    "text": "This function downloads files from a URL with retry logic and exponential backoff.\n\n\n\nget_file(url, export, limit = 3600L)\n\n\n\n\n\n\n\nurl\n\n\nCharacter string URL of the file to be downloaded\n\n\n\n\nexport\n\n\nCharacter string file path where the file should be saved\n\n\n\n\nlimit\n\n\nInteger timeout limit in seconds (default: 3600)\n\n\n\n\n\n\nThe path to the downloaded file\n\n\n\n\nlibrary(\"tima\")\n\ngit &lt;- \"https://github.com/\"\norg &lt;- \"taxonomicallyinformedannotation\"\nrepo &lt;- \"tima-example-files\"\nbranch &lt;- \"main\"\nfile &lt;- \"example_metadata.tsv\"\nget_file(\n  url = paste(git, org, repo, \"raw\", branch, file, sep = \"/\"),\n  export = \"data/source/example_metadata.tsv\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_file"
    ]
  },
  {
    "objectID": "man/get_file.html#get-file",
    "href": "man/get_file.html#get-file",
    "title": "tima",
    "section": "",
    "text": "This function downloads files from a URL with retry logic and exponential backoff.\n\n\n\nget_file(url, export, limit = 3600L)\n\n\n\n\n\n\n\nurl\n\n\nCharacter string URL of the file to be downloaded\n\n\n\n\nexport\n\n\nCharacter string file path where the file should be saved\n\n\n\n\nlimit\n\n\nInteger timeout limit in seconds (default: 3600)\n\n\n\n\n\n\nThe path to the downloaded file\n\n\n\n\nlibrary(\"tima\")\n\ngit &lt;- \"https://github.com/\"\norg &lt;- \"taxonomicallyinformedannotation\"\nrepo &lt;- \"tima-example-files\"\nbranch &lt;- \"main\"\nfile &lt;- \"example_metadata.tsv\"\nget_file(\n  url = paste(git, org, repo, \"raw\", branch, file, sep = \"/\"),\n  export = \"data/source/example_metadata.tsv\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_file"
    ]
  },
  {
    "objectID": "man/prepare_libraries_rt.html",
    "href": "man/prepare_libraries_rt.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares retention time libraries by combining experimental and in silico predicted retention times from multiple sources (MGF files, CSV files). It standardizes retention time units, validates structures, and creates both RT libraries and pseudo structure-organism pairs for RT-based annotation.\n\n\n\nprepare_libraries_rt(\n  mgf_exp = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$exp\\$mgf,\n  mgf_is = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$is\\$mgf,\n  temp_exp = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$exp\\$csv,\n  temp_is = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$is\\$csv,\n  output_rt = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$prepared,\n  output_sop = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$sop\\$prepared\\$rt,\n  col_ik = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$inchikey,\n  col_rt = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$retention_time,\n  col_sm = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$smiles,\n  name_inchikey = get_params(step = \"prepare_libraries_rt\")\\$names\\$inchikey,\n  name_rt = get_params(step = \"prepare_libraries_rt\")\\$names\\$rt\\$library,\n  name_smiles = get_params(step = \"prepare_libraries_rt\")\\$names\\$smiles,\n  unit_rt = get_params(step = \"prepare_libraries_rt\")\\$units\\$rt\n)\n\n\n\n\n\n\n\nmgf_exp\n\n\nCharacter vector of paths to MGF files with experimental RT\n\n\n\n\nmgf_is\n\n\nCharacter vector of paths to MGF files with in silico predicted RT\n\n\n\n\ntemp_exp\n\n\nCharacter vector of paths to CSV files with experimental RT\n\n\n\n\ntemp_is\n\n\nCharacter vector of paths to CSV files with in silico predicted RT\n\n\n\n\noutput_rt\n\n\nCharacter string path for prepared RT library output\n\n\n\n\noutput_sop\n\n\nCharacter string path for pseudo SOP output\n\n\n\n\ncol_ik\n\n\nCharacter string name of InChIKey column in MGF\n\n\n\n\ncol_rt\n\n\nCharacter string name of retention time column in MGF\n\n\n\n\ncol_sm\n\n\nCharacter string name of SMILES column in MGF\n\n\n\n\nname_inchikey\n\n\nCharacter string name of InChIKey column in CSV\n\n\n\n\nname_rt\n\n\nCharacter string name of retention time column in CSV\n\n\n\n\nname_smiles\n\n\nCharacter string name of SMILES column in CSV\n\n\n\n\nunit_rt\n\n\nCharacter string RT unit: \"seconds\" or \"minutes\"\n\n\n\n\n\n\nCharacter string path to the prepared retention time library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_rt()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "prepare_libraries_rt"
    ]
  },
  {
    "objectID": "man/prepare_libraries_rt.html#prepare-libraries-of-retention-times",
    "href": "man/prepare_libraries_rt.html#prepare-libraries-of-retention-times",
    "title": "tima",
    "section": "",
    "text": "This function prepares retention time libraries by combining experimental and in silico predicted retention times from multiple sources (MGF files, CSV files). It standardizes retention time units, validates structures, and creates both RT libraries and pseudo structure-organism pairs for RT-based annotation.\n\n\n\nprepare_libraries_rt(\n  mgf_exp = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$exp\\$mgf,\n  mgf_is = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$is\\$mgf,\n  temp_exp = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$exp\\$csv,\n  temp_is = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$is\\$csv,\n  output_rt = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$prepared,\n  output_sop = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$sop\\$prepared\\$rt,\n  col_ik = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$inchikey,\n  col_rt = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$retention_time,\n  col_sm = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$smiles,\n  name_inchikey = get_params(step = \"prepare_libraries_rt\")\\$names\\$inchikey,\n  name_rt = get_params(step = \"prepare_libraries_rt\")\\$names\\$rt\\$library,\n  name_smiles = get_params(step = \"prepare_libraries_rt\")\\$names\\$smiles,\n  unit_rt = get_params(step = \"prepare_libraries_rt\")\\$units\\$rt\n)\n\n\n\n\n\n\n\nmgf_exp\n\n\nCharacter vector of paths to MGF files with experimental RT\n\n\n\n\nmgf_is\n\n\nCharacter vector of paths to MGF files with in silico predicted RT\n\n\n\n\ntemp_exp\n\n\nCharacter vector of paths to CSV files with experimental RT\n\n\n\n\ntemp_is\n\n\nCharacter vector of paths to CSV files with in silico predicted RT\n\n\n\n\noutput_rt\n\n\nCharacter string path for prepared RT library output\n\n\n\n\noutput_sop\n\n\nCharacter string path for pseudo SOP output\n\n\n\n\ncol_ik\n\n\nCharacter string name of InChIKey column in MGF\n\n\n\n\ncol_rt\n\n\nCharacter string name of retention time column in MGF\n\n\n\n\ncol_sm\n\n\nCharacter string name of SMILES column in MGF\n\n\n\n\nname_inchikey\n\n\nCharacter string name of InChIKey column in CSV\n\n\n\n\nname_rt\n\n\nCharacter string name of retention time column in CSV\n\n\n\n\nname_smiles\n\n\nCharacter string name of SMILES column in CSV\n\n\n\n\nunit_rt\n\n\nCharacter string RT unit: \"seconds\" or \"minutes\"\n\n\n\n\n\n\nCharacter string path to the prepared retention time library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_rt()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "prepare_libraries_rt"
    ]
  },
  {
    "objectID": "man/minimize_results.html",
    "href": "man/minimize_results.html",
    "title": "tima",
    "section": "",
    "text": "This function minimizes annotation results by filtering to the most confident chemical classifications and compound identifications. It extracts the best-scoring chemical taxonomy predictions and the top compound candidates based on weighted chemical scores.\n\n\n\nminimize_results(\n  df,\n  features_table,\n  min_score_classes = 0.6,\n  min_score_compounds = 0.4,\n  best_percentile = 0.9\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing full annotation results with scores and chemical taxonomy predictions\n\n\n\n\nfeatures_table\n\n\nData frame of features to join with minimized results\n\n\n\n\nmin_score_classes\n\n\nNumeric minimum score threshold (0-1) for keeping chemical classifications (default: 0.6)\n\n\n\n\nmin_score_compounds\n\n\nNumeric minimum score threshold (0-1) for keeping compound identifications (default: 0.4)\n\n\n\n\nbest_percentile\n\n\nNumeric percentile threshold (0-1) for selecting top candidates within each feature (default: 0.9, keeps candidates with scores &gt;= 90% of the maximum score for that feature)\n\n\n\n\n\n\nA minimized data frame with features joined to their best chemical classifications and top compound identifications\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/minimize_results.html#minimize-results",
    "href": "man/minimize_results.html#minimize-results",
    "title": "tima",
    "section": "",
    "text": "This function minimizes annotation results by filtering to the most confident chemical classifications and compound identifications. It extracts the best-scoring chemical taxonomy predictions and the top compound candidates based on weighted chemical scores.\n\n\n\nminimize_results(\n  df,\n  features_table,\n  min_score_classes = 0.6,\n  min_score_compounds = 0.4,\n  best_percentile = 0.9\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing full annotation results with scores and chemical taxonomy predictions\n\n\n\n\nfeatures_table\n\n\nData frame of features to join with minimized results\n\n\n\n\nmin_score_classes\n\n\nNumeric minimum score threshold (0-1) for keeping chemical classifications (default: 0.6)\n\n\n\n\nmin_score_compounds\n\n\nNumeric minimum score threshold (0-1) for keeping compound identifications (default: 0.4)\n\n\n\n\nbest_percentile\n\n\nNumeric percentile threshold (0-1) for selecting top candidates within each feature (default: 0.9, keeps candidates with scores &gt;= 90% of the maximum score for that feature)\n\n\n\n\n\n\nA minimized data frame with features joined to their best chemical classifications and top compound identifications\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/annotate_spectra.html",
    "href": "man/annotate_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function annotates spectra\n\n\n\nannotate_spectra(\n  input = get_params(step = \"annotate_spectra\")\\$files\\$spectral\\$raw,\n  libraries = get_params(step = \"annotate_spectra\")\\$files\\$libraries\\$spectral,\n  polarity = get_params(step = \"annotate_spectra\")\\$ms\\$polarity,\n  output = get_params(step = \"annotate_spectra\")\\$files\\$annotations\\$raw\\$spectral\\$spectral,\n  method = get_params(step = \"annotate_spectra\")\\$similarities\\$methods\\$annotations,\n  threshold = get_params(step = \"annotate_spectra\")\\$similarities\\$thresholds\\$annotations,\n  ppm = get_params(step = \"annotate_spectra\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms2,\n  dalton = get_params(step = \"annotate_spectra\")\\$ms\\$tolerances\\$mass\\$dalton\\$ms2,\n  qutoff = get_params(step = \"annotate_spectra\")\\$ms\\$thresholds\\$ms2\\$intensity,\n  approx = get_params(step = \"annotate_spectra\")\\$annotations\\$ms2approx\n)\n\n\n\n\n\n\n\ninput\n\n\nQuery file or list of files containing spectra. Currently ‘.mgf’ file(s)\n\n\n\n\nlibraries\n\n\nLibraries containing spectra to match against. Can be ‘.mgf’ or ‘.sqlite’ (Spectra formatted). Accepts a vector or list of file paths.\n\n\n\n\npolarity\n\n\nMS polarity. Must be ‘pos’ or ‘neg’.\n\n\n\n\noutput\n\n\nOutput file.\n\n\n\n\nmethod\n\n\nSimilarity method\n\n\n\n\nthreshold\n\n\nMinimal similarity to report\n\n\n\n\nppm\n\n\nRelative ppm tolerance to be used\n\n\n\n\ndalton\n\n\nAbsolute Dalton tolerance to be used\n\n\n\n\nqutoff\n\n\nIntensity under which ms2 fragments will be removed.\n\n\n\n\napprox\n\n\nPerform matching without precursor match\n\n\n\n\n\n\nIt takes two files as input. A query file that will be matched against a library file.\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_params(step = \"annotate_spectra\")$files$spectral$raw\n)\nget_file(\n  url = get_default_paths()$urls$examples$spectral_lib_mini$with_rt,\n  export = get_default_paths()$data$source$libraries$spectra$exp$with_rt\n)\nannotate_spectra(\n  libraries = get_default_paths()$data$source$libraries$spectra$exp$with_rt\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Annotate",
      "annotate_spectra"
    ]
  },
  {
    "objectID": "man/annotate_spectra.html#annotate-spectra",
    "href": "man/annotate_spectra.html#annotate-spectra",
    "title": "tima",
    "section": "",
    "text": "This function annotates spectra\n\n\n\nannotate_spectra(\n  input = get_params(step = \"annotate_spectra\")\\$files\\$spectral\\$raw,\n  libraries = get_params(step = \"annotate_spectra\")\\$files\\$libraries\\$spectral,\n  polarity = get_params(step = \"annotate_spectra\")\\$ms\\$polarity,\n  output = get_params(step = \"annotate_spectra\")\\$files\\$annotations\\$raw\\$spectral\\$spectral,\n  method = get_params(step = \"annotate_spectra\")\\$similarities\\$methods\\$annotations,\n  threshold = get_params(step = \"annotate_spectra\")\\$similarities\\$thresholds\\$annotations,\n  ppm = get_params(step = \"annotate_spectra\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms2,\n  dalton = get_params(step = \"annotate_spectra\")\\$ms\\$tolerances\\$mass\\$dalton\\$ms2,\n  qutoff = get_params(step = \"annotate_spectra\")\\$ms\\$thresholds\\$ms2\\$intensity,\n  approx = get_params(step = \"annotate_spectra\")\\$annotations\\$ms2approx\n)\n\n\n\n\n\n\n\ninput\n\n\nQuery file or list of files containing spectra. Currently ‘.mgf’ file(s)\n\n\n\n\nlibraries\n\n\nLibraries containing spectra to match against. Can be ‘.mgf’ or ‘.sqlite’ (Spectra formatted). Accepts a vector or list of file paths.\n\n\n\n\npolarity\n\n\nMS polarity. Must be ‘pos’ or ‘neg’.\n\n\n\n\noutput\n\n\nOutput file.\n\n\n\n\nmethod\n\n\nSimilarity method\n\n\n\n\nthreshold\n\n\nMinimal similarity to report\n\n\n\n\nppm\n\n\nRelative ppm tolerance to be used\n\n\n\n\ndalton\n\n\nAbsolute Dalton tolerance to be used\n\n\n\n\nqutoff\n\n\nIntensity under which ms2 fragments will be removed.\n\n\n\n\napprox\n\n\nPerform matching without precursor match\n\n\n\n\n\n\nIt takes two files as input. A query file that will be matched against a library file.\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_params(step = \"annotate_spectra\")$files$spectral$raw\n)\nget_file(\n  url = get_default_paths()$urls$examples$spectral_lib_mini$with_rt,\n  export = get_default_paths()$data$source$libraries$spectra$exp$with_rt\n)\nannotate_spectra(\n  libraries = get_default_paths()$data$source$libraries$spectra$exp$with_rt\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Annotate",
      "annotate_spectra"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_hmdb.html",
    "href": "man/prepare_libraries_sop_hmdb.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares HMDB (Human Metabolome Database) structure-organism pairs by parsing SDF files, extracting metadata, and formatting for TIMA annotation workflows. Handles human metabolite data with structures and biofluid/tissue associations.\n\n\n\nprepare_libraries_sop_hmdb(\n  input = get_params(step = \"prepare_libraries_sop_hmdb\")\\$files\\$libraries\\$sop\\$raw\\$hmdb,\n  output = get_params(step =\n    \"prepare_libraries_sop_hmdb\")\\$files\\$libraries\\$sop\\$prepared\\$hmdb\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to HMDB SDF zip file\n\n\n\n\noutput\n\n\nCharacter string path for prepared HMDB library output\n\n\n\n\n\n\nCharacter string path to prepared HMDB structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_hmdb()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_hmdb"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_hmdb.html#prepare-libraries-of-structure-organism-pairs-hmdb",
    "href": "man/prepare_libraries_sop_hmdb.html#prepare-libraries-of-structure-organism-pairs-hmdb",
    "title": "tima",
    "section": "",
    "text": "This function prepares HMDB (Human Metabolome Database) structure-organism pairs by parsing SDF files, extracting metadata, and formatting for TIMA annotation workflows. Handles human metabolite data with structures and biofluid/tissue associations.\n\n\n\nprepare_libraries_sop_hmdb(\n  input = get_params(step = \"prepare_libraries_sop_hmdb\")\\$files\\$libraries\\$sop\\$raw\\$hmdb,\n  output = get_params(step =\n    \"prepare_libraries_sop_hmdb\")\\$files\\$libraries\\$sop\\$prepared\\$hmdb\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to HMDB SDF zip file\n\n\n\n\noutput\n\n\nCharacter string path for prepared HMDB library output\n\n\n\n\n\n\nCharacter string path to prepared HMDB structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_hmdb()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_hmdb"
    ]
  },
  {
    "objectID": "man/get_params.html",
    "href": "man/get_params.html",
    "title": "tima",
    "section": "",
    "text": "This function retrieves and merges parameters for a workflow step, combining default parameters, user-specified YAML configurations, and command-line arguments. It handles both regular and advanced parameter sets.\n\n\n\nget_params(step)\n\n\n\n\n\n\n\nstep\n\n\nCharacter string name of the workflow step (e.g., \"prepare_params\", \"annotate_masses\"). Must match an available step in the package.\n\n\n\n\n\n\nNamed list containing the merged parameters for the specified step\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nparams &lt;- get_params(\"prepare_params\")"
  },
  {
    "objectID": "man/get_params.html#get-parameters",
    "href": "man/get_params.html#get-parameters",
    "title": "tima",
    "section": "",
    "text": "This function retrieves and merges parameters for a workflow step, combining default parameters, user-specified YAML configurations, and command-line arguments. It handles both regular and advanced parameter sets.\n\n\n\nget_params(step)\n\n\n\n\n\n\n\nstep\n\n\nCharacter string name of the workflow step (e.g., \"prepare_params\", \"annotate_masses\"). Must match an available step in the package.\n\n\n\n\n\n\nNamed list containing the merged parameters for the specified step\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nparams &lt;- get_params(\"prepare_params\")"
  },
  {
    "objectID": "man/prepare_annotations_spectra.html",
    "href": "man/prepare_annotations_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares MS2 spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows. Handles various spectral matching result formats.\n\n\n\nprepare_annotations_spectra(\n  input = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$annotations\\$raw\\$spectral\\$spectral,\n  output = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$annotations\\$prepared\\$structural\\$spectral,\n  str_stereo = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to spectral matching results file\n\n\n\n\noutput\n\n\nCharacter string path for prepared spectral annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared spectral annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo)\ninput &lt;- get_params(step = \"prepare_annotations_spectra\")$files$annotations$raw$spectral$spectral |&gt;\n  gsub(pattern = \".tsv.gz\", replacement = \"_pos.tsv\", fixed = TRUE)\nget_file(url = paste0(dir, input), export = input)\ndir &lt;- paste0(dir, data_interim)\nprepare_annotations_spectra(\n  input = input,\n  str_stereo = paste0(dir, \"libraries/sop/merged/structures/stereo.tsv\"),\n  str_met = paste0(dir, \"libraries/sop/merged/structures/metadata.tsv\"),\n  str_nam = paste0(dir, \"libraries/sop/merged/structures/names.tsv\"),\n  str_tax_cla = paste0(dir, \"libraries/sop/merged/structures/taxonomies/classyfire.tsv\"),\n  str_tax_npc = paste0(dir, \"libraries/sop/merged/structures/taxonomies/npc.tsv\")\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_spectra"
    ]
  },
  {
    "objectID": "man/prepare_annotations_spectra.html#prepare-annotations-ms2",
    "href": "man/prepare_annotations_spectra.html#prepare-annotations-ms2",
    "title": "tima",
    "section": "",
    "text": "This function prepares MS2 spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows. Handles various spectral matching result formats.\n\n\n\nprepare_annotations_spectra(\n  input = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$annotations\\$raw\\$spectral\\$spectral,\n  output = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$annotations\\$prepared\\$structural\\$spectral,\n  str_stereo = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to spectral matching results file\n\n\n\n\noutput\n\n\nCharacter string path for prepared spectral annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared spectral annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo)\ninput &lt;- get_params(step = \"prepare_annotations_spectra\")$files$annotations$raw$spectral$spectral |&gt;\n  gsub(pattern = \".tsv.gz\", replacement = \"_pos.tsv\", fixed = TRUE)\nget_file(url = paste0(dir, input), export = input)\ndir &lt;- paste0(dir, data_interim)\nprepare_annotations_spectra(\n  input = input,\n  str_stereo = paste0(dir, \"libraries/sop/merged/structures/stereo.tsv\"),\n  str_met = paste0(dir, \"libraries/sop/merged/structures/metadata.tsv\"),\n  str_nam = paste0(dir, \"libraries/sop/merged/structures/names.tsv\"),\n  str_tax_cla = paste0(dir, \"libraries/sop/merged/structures/taxonomies/classyfire.tsv\"),\n  str_tax_npc = paste0(dir, \"libraries/sop/merged/structures/taxonomies/npc.tsv\")\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_spectra"
    ]
  },
  {
    "objectID": "man/parse_yaml_params.html",
    "href": "man/parse_yaml_params.html",
    "title": "tima",
    "section": "",
    "text": "This function parses YAML parameter files, loading default parameters and optionally overriding them with user-specified values.\n\n\n\nparse_yaml_params(\n  def = get(\"default_path\", envir = parent.frame()),\n  usr = get(\"user_path\", envir = parent.frame())\n)\n\n\n\n\n\n\n\ndef\n\n\nCharacter string path to the default YAML parameters file\n\n\n\n\nusr\n\n\nCharacter string path to the user-specified YAML parameters file (optional). If it exists, it will override default values.\n\n\n\n\n\n\nA list containing the parameters specified in the YAML files\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/parse_yaml_params.html#parse-yaml-parameters",
    "href": "man/parse_yaml_params.html#parse-yaml-parameters",
    "title": "tima",
    "section": "",
    "text": "This function parses YAML parameter files, loading default parameters and optionally overriding them with user-specified values.\n\n\n\nparse_yaml_params(\n  def = get(\"default_path\", envir = parent.frame()),\n  usr = get(\"user_path\", envir = parent.frame())\n)\n\n\n\n\n\n\n\ndef\n\n\nCharacter string path to the default YAML parameters file\n\n\n\n\nusr\n\n\nCharacter string path to the user-specified YAML parameters file (optional). If it exists, it will override default values.\n\n\n\n\n\n\nA list containing the parameters specified in the YAML files\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/summarize_results.html",
    "href": "man/summarize_results.html",
    "title": "tima",
    "section": "",
    "text": "This function summarizes annotation results by adding feature metadata, filtering/collapsing columns, and optionally removing tied scores or summarizing to one row per feature. Creates a final, simplified results table for downstream analysis or reporting.\n\n\n\nsummarize_results(\n  df,\n  features_table,\n  components_table,\n  structure_organism_pairs_table,\n  annot_table_wei_chemo,\n  remove_ties,\n  summarize\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing weighted annotation results\n\n\n\n\nfeatures_table\n\n\nData frame with feature metadata (RT, m/z, etc.)\n\n\n\n\ncomponents_table\n\n\nData frame with molecular network component assignments\n\n\n\n\nstructure_organism_pairs_table\n\n\nData frame with structure-organism pairs\n\n\n\n\nannot_table_wei_chemo\n\n\nData frame with chemically weighted annotations\n\n\n\n\nremove_ties\n\n\nLogical whether to remove tied scores (keep only highest)\n\n\n\n\nsummarize\n\n\nLogical whether to collapse to 1 row per feature\n\n\n\n\n\n\nData frame containing summarized annotation results\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/summarize_results.html#summarize-results",
    "href": "man/summarize_results.html#summarize-results",
    "title": "tima",
    "section": "",
    "text": "This function summarizes annotation results by adding feature metadata, filtering/collapsing columns, and optionally removing tied scores or summarizing to one row per feature. Creates a final, simplified results table for downstream analysis or reporting.\n\n\n\nsummarize_results(\n  df,\n  features_table,\n  components_table,\n  structure_organism_pairs_table,\n  annot_table_wei_chemo,\n  remove_ties,\n  summarize\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing weighted annotation results\n\n\n\n\nfeatures_table\n\n\nData frame with feature metadata (RT, m/z, etc.)\n\n\n\n\ncomponents_table\n\n\nData frame with molecular network component assignments\n\n\n\n\nstructure_organism_pairs_table\n\n\nData frame with structure-organism pairs\n\n\n\n\nannot_table_wei_chemo\n\n\nData frame with chemically weighted annotations\n\n\n\n\nremove_ties\n\n\nLogical whether to remove tied scores (keep only highest)\n\n\n\n\nsummarize\n\n\nLogical whether to collapse to 1 row per feature\n\n\n\n\n\n\nData frame containing summarized annotation results\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/prepare_libraries_sop_ecmdb.html",
    "href": "man/prepare_libraries_sop_ecmdb.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares ECMDB (E. coli Metabolome Database) structure-organism pairs by parsing JSON data, extracting metabolite information, and formatting for TIMA workflows. Handles E. coli metabolite data with structures.\n\n\n\nprepare_libraries_sop_ecmdb(\n  input = get_params(step = \"prepare_libraries_sop_ecmdb\")\\$files\\$libraries\\$sop\\$raw\\$ecmdb,\n  output = get_params(step =\n    \"prepare_libraries_sop_ecmdb\")\\$files\\$libraries\\$sop\\$prepared\\$ecmdb\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to ECMDB JSON zip file\n\n\n\n\noutput\n\n\nCharacter string path for prepared ECMDB library output\n\n\n\n\n\n\nCharacter string path to prepared ECMDB structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_ecmdb()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_ecmdb"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_ecmdb.html#prepare-libraries-of-structure-organism-pairs-ecmdb",
    "href": "man/prepare_libraries_sop_ecmdb.html#prepare-libraries-of-structure-organism-pairs-ecmdb",
    "title": "tima",
    "section": "",
    "text": "This function prepares ECMDB (E. coli Metabolome Database) structure-organism pairs by parsing JSON data, extracting metabolite information, and formatting for TIMA workflows. Handles E. coli metabolite data with structures.\n\n\n\nprepare_libraries_sop_ecmdb(\n  input = get_params(step = \"prepare_libraries_sop_ecmdb\")\\$files\\$libraries\\$sop\\$raw\\$ecmdb,\n  output = get_params(step =\n    \"prepare_libraries_sop_ecmdb\")\\$files\\$libraries\\$sop\\$prepared\\$ecmdb\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to ECMDB JSON zip file\n\n\n\n\noutput\n\n\nCharacter string path for prepared ECMDB library output\n\n\n\n\n\n\nCharacter string path to prepared ECMDB structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_ecmdb()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_ecmdb"
    ]
  },
  {
    "objectID": "man/calculate_entropy_and_similarity.html",
    "href": "man/calculate_entropy_and_similarity.html",
    "title": "tima",
    "section": "",
    "text": "This function calculates spectral entropy and similarity scores by comparing query spectra against library spectra. Uses entropy-based similarity measures to match MS2 fragmentation patterns.\n\n\n\ncalculate_entropy_and_similarity(\n  lib_ids,\n  lib_precursors,\n  lib_spectra,\n  query_ids,\n  query_precursors,\n  query_spectra,\n  method,\n  dalton,\n  ppm,\n  threshold,\n  approx\n)\n\n\n\n\n\n\n\nlib_ids\n\n\nCharacter vector of library spectrum IDs\n\n\n\n\nlib_precursors\n\n\nNumeric vector of library precursor m/z values\n\n\n\n\nlib_spectra\n\n\nList of library spectra (each a matrix of mz/intensity)\n\n\n\n\nquery_ids\n\n\nCharacter vector of query spectrum IDs\n\n\n\n\nquery_precursors\n\n\nNumeric vector of query precursor m/z values\n\n\n\n\nquery_spectra\n\n\nList of query spectra (each a matrix of mz/intensity)\n\n\n\n\nmethod\n\n\nCharacter string similarity method to use\n\n\n\n\ndalton\n\n\nNumeric absolute mass tolerance in Daltons\n\n\n\n\nppm\n\n\nNumeric relative mass tolerance in ppm\n\n\n\n\nthreshold\n\n\nNumeric minimum similarity threshold (0-1)\n\n\n\n\napprox\n\n\nLogical whether to perform approximate matching without precursor mass filtering\n\n\n\n\n\n\nData frame with spectrum IDs, entropy scores, and similarity scores\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/calculate_entropy_and_similarity.html#calculate-entropy-score",
    "href": "man/calculate_entropy_and_similarity.html#calculate-entropy-score",
    "title": "tima",
    "section": "",
    "text": "This function calculates spectral entropy and similarity scores by comparing query spectra against library spectra. Uses entropy-based similarity measures to match MS2 fragmentation patterns.\n\n\n\ncalculate_entropy_and_similarity(\n  lib_ids,\n  lib_precursors,\n  lib_spectra,\n  query_ids,\n  query_precursors,\n  query_spectra,\n  method,\n  dalton,\n  ppm,\n  threshold,\n  approx\n)\n\n\n\n\n\n\n\nlib_ids\n\n\nCharacter vector of library spectrum IDs\n\n\n\n\nlib_precursors\n\n\nNumeric vector of library precursor m/z values\n\n\n\n\nlib_spectra\n\n\nList of library spectra (each a matrix of mz/intensity)\n\n\n\n\nquery_ids\n\n\nCharacter vector of query spectrum IDs\n\n\n\n\nquery_precursors\n\n\nNumeric vector of query precursor m/z values\n\n\n\n\nquery_spectra\n\n\nList of query spectra (each a matrix of mz/intensity)\n\n\n\n\nmethod\n\n\nCharacter string similarity method to use\n\n\n\n\ndalton\n\n\nNumeric absolute mass tolerance in Daltons\n\n\n\n\nppm\n\n\nNumeric relative mass tolerance in ppm\n\n\n\n\nthreshold\n\n\nNumeric minimum similarity threshold (0-1)\n\n\n\n\napprox\n\n\nLogical whether to perform approximate matching without precursor mass filtering\n\n\n\n\n\n\nData frame with spectrum IDs, entropy scores, and similarity scores\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/harmonize_spectra.html",
    "href": "man/harmonize_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function harmonizes spectra header fields by mapping library-specific column names to standardized field names. It handles different naming conventions across spectral libraries and ensures consistent metadata structure.\n\n\n\nharmonize_spectra(\n  spectra,\n  metad = get(\"metad\", envir = parent.frame()),\n  mode,\n  col_ad = get(\"col_ad\", envir = parent.frame()),\n  col_ce = get(\"col_ce\", envir = parent.frame()),\n  col_ci = get(\"col_ci\", envir = parent.frame()),\n  col_em = get(\"col_em\", envir = parent.frame()),\n  col_in = get(\"col_in\", envir = parent.frame()),\n  col_io = get(\"col_io\", envir = parent.frame()),\n  col_ik = get(\"col_ik\", envir = parent.frame()),\n  col_il = get(\"col_il\", envir = parent.frame()),\n  col_mf = get(\"col_mf\", envir = parent.frame()),\n  col_na = get(\"col_na\", envir = parent.frame()),\n  col_po = get(\"col_po\", envir = parent.frame()),\n  col_sm = get(\"col_sm\", envir = parent.frame()),\n  col_sn = get(\"col_sn\", envir = parent.frame()),\n  col_si = get(\"col_si\", envir = parent.frame()),\n  col_sp = get(\"col_sp\", envir = parent.frame()),\n  col_sy = get(\"col_sy\", envir = parent.frame()),\n  col_xl = get(\"col_xl\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nspectra\n\n\nData frame containing spectra data to be harmonized\n\n\n\n\nmetad\n\n\nMetadata to identify the library source\n\n\n\n\nmode\n\n\nMS ionization mode. Must contain ‘pos’ or ‘neg’\n\n\n\n\ncol_ad\n\n\nName of the adduct field in the input spectra\n\n\n\n\ncol_ce\n\n\nName of the collision energy field\n\n\n\n\ncol_ci\n\n\nName of the compound ID field\n\n\n\n\ncol_em\n\n\nName of the exact mass field\n\n\n\n\ncol_in\n\n\nName of the InChI field\n\n\n\n\ncol_io\n\n\nName of the InChI without stereochemistry field\n\n\n\n\ncol_ik\n\n\nName of the InChIKey field\n\n\n\n\ncol_il\n\n\nName of the InChIKey connectivity layer field\n\n\n\n\ncol_mf\n\n\nName of the molecular formula field\n\n\n\n\ncol_na\n\n\nName of the compound name field\n\n\n\n\ncol_po\n\n\nName of the polarity field\n\n\n\n\ncol_sm\n\n\nName of the SMILES field\n\n\n\n\ncol_sn\n\n\nName of the SMILES without stereochemistry field\n\n\n\n\ncol_si\n\n\nName of the spectrum ID field\n\n\n\n\ncol_sp\n\n\nName of the SPLASH field\n\n\n\n\ncol_sy\n\n\nName of the synonyms field\n\n\n\n\ncol_xl\n\n\nName of the xLogP field\n\n\n\n\n\n\nA data frame with harmonized column names and standardized field names\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/harmonize_spectra.html#harmonize-spectra",
    "href": "man/harmonize_spectra.html#harmonize-spectra",
    "title": "tima",
    "section": "",
    "text": "This function harmonizes spectra header fields by mapping library-specific column names to standardized field names. It handles different naming conventions across spectral libraries and ensures consistent metadata structure.\n\n\n\nharmonize_spectra(\n  spectra,\n  metad = get(\"metad\", envir = parent.frame()),\n  mode,\n  col_ad = get(\"col_ad\", envir = parent.frame()),\n  col_ce = get(\"col_ce\", envir = parent.frame()),\n  col_ci = get(\"col_ci\", envir = parent.frame()),\n  col_em = get(\"col_em\", envir = parent.frame()),\n  col_in = get(\"col_in\", envir = parent.frame()),\n  col_io = get(\"col_io\", envir = parent.frame()),\n  col_ik = get(\"col_ik\", envir = parent.frame()),\n  col_il = get(\"col_il\", envir = parent.frame()),\n  col_mf = get(\"col_mf\", envir = parent.frame()),\n  col_na = get(\"col_na\", envir = parent.frame()),\n  col_po = get(\"col_po\", envir = parent.frame()),\n  col_sm = get(\"col_sm\", envir = parent.frame()),\n  col_sn = get(\"col_sn\", envir = parent.frame()),\n  col_si = get(\"col_si\", envir = parent.frame()),\n  col_sp = get(\"col_sp\", envir = parent.frame()),\n  col_sy = get(\"col_sy\", envir = parent.frame()),\n  col_xl = get(\"col_xl\", envir = parent.frame())\n)\n\n\n\n\n\n\n\nspectra\n\n\nData frame containing spectra data to be harmonized\n\n\n\n\nmetad\n\n\nMetadata to identify the library source\n\n\n\n\nmode\n\n\nMS ionization mode. Must contain ‘pos’ or ‘neg’\n\n\n\n\ncol_ad\n\n\nName of the adduct field in the input spectra\n\n\n\n\ncol_ce\n\n\nName of the collision energy field\n\n\n\n\ncol_ci\n\n\nName of the compound ID field\n\n\n\n\ncol_em\n\n\nName of the exact mass field\n\n\n\n\ncol_in\n\n\nName of the InChI field\n\n\n\n\ncol_io\n\n\nName of the InChI without stereochemistry field\n\n\n\n\ncol_ik\n\n\nName of the InChIKey field\n\n\n\n\ncol_il\n\n\nName of the InChIKey connectivity layer field\n\n\n\n\ncol_mf\n\n\nName of the molecular formula field\n\n\n\n\ncol_na\n\n\nName of the compound name field\n\n\n\n\ncol_po\n\n\nName of the polarity field\n\n\n\n\ncol_sm\n\n\nName of the SMILES field\n\n\n\n\ncol_sn\n\n\nName of the SMILES without stereochemistry field\n\n\n\n\ncol_si\n\n\nName of the spectrum ID field\n\n\n\n\ncol_sp\n\n\nName of the SPLASH field\n\n\n\n\ncol_sy\n\n\nName of the synonyms field\n\n\n\n\ncol_xl\n\n\nName of the xLogP field\n\n\n\n\n\n\nA data frame with harmonized column names and standardized field names\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/select_sirius_columns_canopus.html",
    "href": "man/select_sirius_columns_canopus.html",
    "title": "tima",
    "section": "",
    "text": "This function selects and standardizes CANOPUS chemical classification columns from SIRIUS results, mapping SIRIUS-specific column names to TIMA standard names for downstream processing.\n\n\n\nselect_sirius_columns_canopus(df, sirius_version)\n\n\n\n\n\n\n\ndf\n\n\nData frame with SIRIUS CANOPUS results\n\n\n\n\nsirius_version\n\n\nCharacter string SIRIUS version (\"5\" or \"6\")\n\n\n\n\n\n\nData frame with standardized CANOPUS column names\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/select_sirius_columns_canopus.html#select-sirius-columns-canopus",
    "href": "man/select_sirius_columns_canopus.html#select-sirius-columns-canopus",
    "title": "tima",
    "section": "",
    "text": "This function selects and standardizes CANOPUS chemical classification columns from SIRIUS results, mapping SIRIUS-specific column names to TIMA standard names for downstream processing.\n\n\n\nselect_sirius_columns_canopus(df, sirius_version)\n\n\n\n\n\n\n\ndf\n\n\nData frame with SIRIUS CANOPUS results\n\n\n\n\nsirius_version\n\n\nCharacter string SIRIUS version (\"5\" or \"6\")\n\n\n\n\n\n\nData frame with standardized CANOPUS column names\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/parse_adduct.html",
    "href": "man/parse_adduct.html",
    "title": "tima",
    "section": "",
    "text": "This function parses mass spectrometry adduct notation strings into their components: multimer count, isotope shift, modifications, charge state, and charge sign. It handles complex adducts with multiple additions/losses.\n\n\n\nparse_adduct(\n  adduct_string,\n  regex = \"\\\\[(\\\\d*)M(?![a-z])(\\\\d*)([+-][\\\\w\\\\d].*)?.*\\\\](\\\\d*)([+-])?\"\n)\n\n\n\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct in standard notation (e.g., \"[M+H]+\", \"[2M+Na]+\", \"[M-H2O+H]+\")\n\n\n\n\nregex\n\n\nCharacter string regular expression pattern for parsing (default handles standard adduct notation)\n\n\n\n\n\n\nNamed numeric vector containing:\n\n\n\nn_mer\n\n\nInteger number of monomers (e.g., 2 for dimer)\n\n\n\n\nn_iso\n\n\nInteger isotope shift (e.g., 1 for M1)\n\n\n\n\nlos_add_clu\n\n\nNumeric total mass change from modifications\n\n\n\n\nn_charges\n\n\nInteger absolute number of charges\n\n\n\n\ncharge\n\n\nInteger charge sign (+1 or -1)\n\n\n\nReturns all zeros if parsing fails.\n\n\n\n\nlibrary(\"tima\")\n\nparse_adduct(\"[M+H]+\")\nparse_adduct(\"[2M+Na]+\")\nparse_adduct(\"[2M1-C6H12O6 (hexose)+NaCl+H]2+\")",
    "crumbs": [
      "Get started",
      "Functions",
      "parse_adduct"
    ]
  },
  {
    "objectID": "man/parse_adduct.html#parse-adduct",
    "href": "man/parse_adduct.html#parse-adduct",
    "title": "tima",
    "section": "",
    "text": "This function parses mass spectrometry adduct notation strings into their components: multimer count, isotope shift, modifications, charge state, and charge sign. It handles complex adducts with multiple additions/losses.\n\n\n\nparse_adduct(\n  adduct_string,\n  regex = \"\\\\[(\\\\d*)M(?![a-z])(\\\\d*)([+-][\\\\w\\\\d].*)?.*\\\\](\\\\d*)([+-])?\"\n)\n\n\n\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct in standard notation (e.g., \"[M+H]+\", \"[2M+Na]+\", \"[M-H2O+H]+\")\n\n\n\n\nregex\n\n\nCharacter string regular expression pattern for parsing (default handles standard adduct notation)\n\n\n\n\n\n\nNamed numeric vector containing:\n\n\n\nn_mer\n\n\nInteger number of monomers (e.g., 2 for dimer)\n\n\n\n\nn_iso\n\n\nInteger isotope shift (e.g., 1 for M1)\n\n\n\n\nlos_add_clu\n\n\nNumeric total mass change from modifications\n\n\n\n\nn_charges\n\n\nInteger absolute number of charges\n\n\n\n\ncharge\n\n\nInteger charge sign (+1 or -1)\n\n\n\nReturns all zeros if parsing fails.\n\n\n\n\nlibrary(\"tima\")\n\nparse_adduct(\"[M+H]+\")\nparse_adduct(\"[2M+Na]+\")\nparse_adduct(\"[2M1-C6H12O6 (hexose)+NaCl+H]2+\")",
    "crumbs": [
      "Get started",
      "Functions",
      "parse_adduct"
    ]
  },
  {
    "objectID": "man/export_params.html",
    "href": "man/export_params.html",
    "title": "tima",
    "section": "",
    "text": "This function writes parameter configurations to a timestamped YAML file for reproducibility and tracking. Each export includes the step identifier and exact timestamp.\n\n\n\nexport_params(\n  parameters = get(\"parameters\", envir = parent.frame()),\n  directory = get_default_paths()\\$data\\$interim\\$params\\$path,\n  step\n)\n\n\n\n\n\n\n\nparameters\n\n\nNamed list of parameters to be exported\n\n\n\n\ndirectory\n\n\nCharacter string path to the directory where the YAML file will be saved (default: data/interim/params/)\n\n\n\n\nstep\n\n\nCharacter string step identifier to be included in the YAML file name (required)\n\n\n\n\n\n\nNULL (invisibly). Creates YAML file as side effect.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/export_params.html#export-parameters",
    "href": "man/export_params.html#export-parameters",
    "title": "tima",
    "section": "",
    "text": "This function writes parameter configurations to a timestamped YAML file for reproducibility and tracking. Each export includes the step identifier and exact timestamp.\n\n\n\nexport_params(\n  parameters = get(\"parameters\", envir = parent.frame()),\n  directory = get_default_paths()\\$data\\$interim\\$params\\$path,\n  step\n)\n\n\n\n\n\n\n\nparameters\n\n\nNamed list of parameters to be exported\n\n\n\n\ndirectory\n\n\nCharacter string path to the directory where the YAML file will be saved (default: data/interim/params/)\n\n\n\n\nstep\n\n\nCharacter string step identifier to be included in the YAML file name (required)\n\n\n\n\n\n\nNULL (invisibly). Creates YAML file as side effect.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/create_edges.html",
    "href": "man/create_edges.html",
    "title": "tima",
    "section": "",
    "text": "This function calculates pairwise spectral similarity between all spectra to create a network edge list. Uses parallel processing via purrr for efficiency.\n\n\n\ncreate_edges(\n  frags,\n  nspecs,\n  precs,\n  method,\n  ms2_tolerance,\n  ppm_tolerance,\n  threshold,\n  matched_peaks\n)\n\n\n\n\n\n\n\nfrags\n\n\nList of fragment spectra matrices\n\n\n\n\nnspecs\n\n\nInteger number of spectra\n\n\n\n\nprecs\n\n\nNumeric vector of precursor m/z values\n\n\n\n\nmethod\n\n\nCharacter string similarity method (\"entropy\", \"gnps\", or \"cosine\")\n\n\n\n\nms2_tolerance\n\n\nNumeric MS2 tolerance in Daltons\n\n\n\n\nppm_tolerance\n\n\nNumeric PPM tolerance\n\n\n\n\nthreshold\n\n\nNumeric minimum similarity score threshold\n\n\n\n\nmatched_peaks\n\n\nInteger minimum number of matched peaks required\n\n\n\n\n\n\nA data frame with columns: feature_id, target_id, score, matched_peaks. Returns empty data frame with NA values if no edges pass thresholds.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/create_edges.html#create-edges",
    "href": "man/create_edges.html#create-edges",
    "title": "tima",
    "section": "",
    "text": "This function calculates pairwise spectral similarity between all spectra to create a network edge list. Uses parallel processing via purrr for efficiency.\n\n\n\ncreate_edges(\n  frags,\n  nspecs,\n  precs,\n  method,\n  ms2_tolerance,\n  ppm_tolerance,\n  threshold,\n  matched_peaks\n)\n\n\n\n\n\n\n\nfrags\n\n\nList of fragment spectra matrices\n\n\n\n\nnspecs\n\n\nInteger number of spectra\n\n\n\n\nprecs\n\n\nNumeric vector of precursor m/z values\n\n\n\n\nmethod\n\n\nCharacter string similarity method (\"entropy\", \"gnps\", or \"cosine\")\n\n\n\n\nms2_tolerance\n\n\nNumeric MS2 tolerance in Daltons\n\n\n\n\nppm_tolerance\n\n\nNumeric PPM tolerance\n\n\n\n\nthreshold\n\n\nNumeric minimum similarity score threshold\n\n\n\n\nmatched_peaks\n\n\nInteger minimum number of matched peaks required\n\n\n\n\n\n\nA data frame with columns: feature_id, target_id, score, matched_peaks. Returns empty data frame with NA values if no edges pass thresholds.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/prepare_features_components.html",
    "href": "man/prepare_features_components.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares molecular network component (cluster) assignments by loading, standardizing, and formatting component IDs for each feature. Components represent groups of related features in the molecular network.\n\n\n\nprepare_features_components(\n  input = get_params(step =\n    \"prepare_features_components\")\\$files\\$networks\\$spectral\\$components\\$raw,\n  output = get_params(step =\n    \"prepare_features_components\")\\$files\\$networks\\$spectral\\$components\\$prepared\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of paths to input component files. Can be a single file or multiple files that will be combined.\n\n\n\n\noutput\n\n\nCharacter string path where prepared components should be saved\n\n\n\n\n\n\nCharacter string path to the prepared features’ components file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\ninput &lt;- get_params(step = \"prepare_features_components\")$files$networks$spectral$components$raw\nget_file(url = paste0(dir, input), export = input)\nprepare_features_components(input = input)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_components"
    ]
  },
  {
    "objectID": "man/prepare_features_components.html#prepare-features-components",
    "href": "man/prepare_features_components.html#prepare-features-components",
    "title": "tima",
    "section": "",
    "text": "This function prepares molecular network component (cluster) assignments by loading, standardizing, and formatting component IDs for each feature. Components represent groups of related features in the molecular network.\n\n\n\nprepare_features_components(\n  input = get_params(step =\n    \"prepare_features_components\")\\$files\\$networks\\$spectral\\$components\\$raw,\n  output = get_params(step =\n    \"prepare_features_components\")\\$files\\$networks\\$spectral\\$components\\$prepared\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of paths to input component files. Can be a single file or multiple files that will be combined.\n\n\n\n\noutput\n\n\nCharacter string path where prepared components should be saved\n\n\n\n\n\n\nCharacter string path to the prepared features’ components file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\ninput &lt;- get_params(step = \"prepare_features_components\")$files$networks$spectral$components$raw\nget_file(url = paste0(dir, input), export = input)\nprepare_features_components(input = input)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_components"
    ]
  },
  {
    "objectID": "man/get_gnps_tables.html",
    "href": "man/get_gnps_tables.html",
    "title": "tima",
    "section": "",
    "text": "This function downloads and retrieves GNPS (Global Natural Products Social Molecular Networking) result tables from a completed job. It fetches features, metadata, spectra, and annotation files from GNPS servers. When a job ID is not provided or GNPS resources are missing, small fake files are written so downstream steps do not fail during testing.\n\n\n\nget_gnps_tables(\n  gnps_job_id,\n  gnps_job_example = get_default_paths()\\$gnps\\$example,\n  filename = \"\",\n  workflow = \"fbmn\",\n  path_features,\n  path_metadata,\n  path_spectra,\n  path_source = get_default_paths()\\$data\\$source\\$path,\n  path_interim_a = get_default_paths()\\$data\\$interim\\$annotations\\$path,\n  path_interim_f = get_default_paths()\\$data\\$interim\\$features\\$path\n)\n\n\n\n\n\n\n\ngnps_job_id\n\n\nCharacter string GNPS job ID (32 characters). Can be NULL or empty string to skip download.\n\n\n\n\ngnps_job_example\n\n\nCharacter string example GNPS job ID for testing\n\n\n\n\nfilename\n\n\nCharacter string name of the file to download (used for fake outputs)\n\n\n\n\nworkflow\n\n\nCharacter string indicating workflow type: \"fbmn\" (feature-based) or \"classical\" molecular networking\n\n\n\n\npath_features\n\n\nCharacter string path for features output (file path)\n\n\n\n\npath_metadata\n\n\nCharacter string path for metadata output (file path or list)\n\n\n\n\npath_spectra\n\n\nCharacter string path for spectra output (file path)\n\n\n\n\npath_source\n\n\nCharacter string path to store source files\n\n\n\n\npath_interim_a\n\n\nCharacter string path to store interim annotations\n\n\n\n\npath_interim_f\n\n\nCharacter string path to store interim features\n\n\n\n\n\n\nA named character vector with paths to the written/available files.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/get_gnps_tables.html#get-gnps-tables",
    "href": "man/get_gnps_tables.html#get-gnps-tables",
    "title": "tima",
    "section": "",
    "text": "This function downloads and retrieves GNPS (Global Natural Products Social Molecular Networking) result tables from a completed job. It fetches features, metadata, spectra, and annotation files from GNPS servers. When a job ID is not provided or GNPS resources are missing, small fake files are written so downstream steps do not fail during testing.\n\n\n\nget_gnps_tables(\n  gnps_job_id,\n  gnps_job_example = get_default_paths()\\$gnps\\$example,\n  filename = \"\",\n  workflow = \"fbmn\",\n  path_features,\n  path_metadata,\n  path_spectra,\n  path_source = get_default_paths()\\$data\\$source\\$path,\n  path_interim_a = get_default_paths()\\$data\\$interim\\$annotations\\$path,\n  path_interim_f = get_default_paths()\\$data\\$interim\\$features\\$path\n)\n\n\n\n\n\n\n\ngnps_job_id\n\n\nCharacter string GNPS job ID (32 characters). Can be NULL or empty string to skip download.\n\n\n\n\ngnps_job_example\n\n\nCharacter string example GNPS job ID for testing\n\n\n\n\nfilename\n\n\nCharacter string name of the file to download (used for fake outputs)\n\n\n\n\nworkflow\n\n\nCharacter string indicating workflow type: \"fbmn\" (feature-based) or \"classical\" molecular networking\n\n\n\n\npath_features\n\n\nCharacter string path for features output (file path)\n\n\n\n\npath_metadata\n\n\nCharacter string path for metadata output (file path or list)\n\n\n\n\npath_spectra\n\n\nCharacter string path for spectra output (file path)\n\n\n\n\npath_source\n\n\nCharacter string path to store source files\n\n\n\n\npath_interim_a\n\n\nCharacter string path to store interim annotations\n\n\n\n\npath_interim_f\n\n\nCharacter string path to store interim features\n\n\n\n\n\n\nA named character vector with paths to the written/available files.\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/prepare_features_edges.html",
    "href": "man/prepare_features_edges.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares molecular network edges by combining MS1-based and spectral similarity edges, adding entropy information, and standardizing column names. Edges represent relationships between features in the molecular network.\n\n\n\nprepare_features_edges(\n  input = get_params(step = \"prepare_features_edges\")\\$files\\$networks\\$spectral\\$edges\\$raw,\n  output = get_params(step =\n    \"prepare_features_edges\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  name_source = get_params(step = \"prepare_features_edges\")\\$names\\$source,\n  name_target = get_params(step = \"prepare_features_edges\")\\$names\\$target\n)\n\n\n\n\n\n\n\ninput\n\n\nNamed list containing paths to edge files. Must have \"ms1\" and \"spectral\" elements pointing to respective edge files.\n\n\n\n\noutput\n\n\nCharacter string path where prepared edges should be saved\n\n\n\n\nname_source\n\n\nCharacter string name of the source feature column in input files\n\n\n\n\nname_target\n\n\nCharacter string name of the target feature column in input files\n\n\n\n\n\n\nCharacter string path to the prepared edges file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\ninput_1 &lt;- get_params(step = \"prepare_features_edges\")$files$networks$spectral$edges$raw$ms1\ninput_2 &lt;- get_params(step = \"prepare_features_edges\")$files$networks$spectral$edges$raw$spectral\nget_file(url = paste0(dir, input_1), export = input_1)\nget_file(url = paste0(dir, input_2), export = input_2)\nprepare_features_edges(\n  input = list(\"ms1\" = input_1, \"spectral\" = input_2)\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_edges"
    ]
  },
  {
    "objectID": "man/prepare_features_edges.html#prepare-features-edges",
    "href": "man/prepare_features_edges.html#prepare-features-edges",
    "title": "tima",
    "section": "",
    "text": "This function prepares molecular network edges by combining MS1-based and spectral similarity edges, adding entropy information, and standardizing column names. Edges represent relationships between features in the molecular network.\n\n\n\nprepare_features_edges(\n  input = get_params(step = \"prepare_features_edges\")\\$files\\$networks\\$spectral\\$edges\\$raw,\n  output = get_params(step =\n    \"prepare_features_edges\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  name_source = get_params(step = \"prepare_features_edges\")\\$names\\$source,\n  name_target = get_params(step = \"prepare_features_edges\")\\$names\\$target\n)\n\n\n\n\n\n\n\ninput\n\n\nNamed list containing paths to edge files. Must have \"ms1\" and \"spectral\" elements pointing to respective edge files.\n\n\n\n\noutput\n\n\nCharacter string path where prepared edges should be saved\n\n\n\n\nname_source\n\n\nCharacter string name of the source feature column in input files\n\n\n\n\nname_target\n\n\nCharacter string name of the target feature column in input files\n\n\n\n\n\n\nCharacter string path to the prepared edges file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\ninput_1 &lt;- get_params(step = \"prepare_features_edges\")$files$networks$spectral$edges$raw$ms1\ninput_2 &lt;- get_params(step = \"prepare_features_edges\")$files$networks$spectral$edges$raw$spectral\nget_file(url = paste0(dir, input_1), export = input_1)\nget_file(url = paste0(dir, input_2), export = input_2)\nprepare_features_edges(\n  input = list(\"ms1\" = input_1, \"spectral\" = input_2)\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_edges"
    ]
  },
  {
    "objectID": "man/select_sirius_columns_structures.html",
    "href": "man/select_sirius_columns_structures.html",
    "title": "tima",
    "section": "",
    "text": "This function selects sirius columns (structures)\n\n\n\nselect_sirius_columns_structures(df, sirius_version)\n\n\n\n\n\n\n\ndf\n\n\nDataframe\n\n\n\n\nsirius_version\n\n\nSirius version\n\n\n\n\n\n\nThe dataframe with selected structure columns\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  },
  {
    "objectID": "man/select_sirius_columns_structures.html#select-sirius-columns-structures",
    "href": "man/select_sirius_columns_structures.html#select-sirius-columns-structures",
    "title": "tima",
    "section": "",
    "text": "This function selects sirius columns (structures)\n\n\n\nselect_sirius_columns_structures(df, sirius_version)\n\n\n\n\n\n\n\ndf\n\n\nDataframe\n\n\n\n\nsirius_version\n\n\nSirius version\n\n\n\n\n\n\nThe dataframe with selected structure columns\n\n\n\n\nlibrary(\"tima\")\n\nNULL\n\nNULL"
  }
]