[
  {
    "objectID": "vignettes/articles/IV-benchmarking.html",
    "href": "vignettes/articles/IV-benchmarking.html",
    "title": "4 Benchmarking Performance",
    "section": "",
    "text": "This vignette simply shows the actual performance of TIMA.\nThe benchmarking dataset was built using https://zenodo.org/record/5186176.\nIt contained positive and negative MS2 spectra of multiple ion species ([M+H]+, [M+Na]+, [M+H4N]+, …) coming from different mass spectrometers.\nIn positive mode, It was filtered to 27,789 spectra, representing 17,822 structures without stereo. Of those, only 15,005 spectra (54.0%) corresponded to structures present in the library we used to annotate.\nIn negative mode, It was filtered to 12,060 spectra, representing 9,112 structures without stereo. Of those, only 6,282 spectra (52.1%) corresponded to structures present in the library we used to annotate."
  },
  {
    "objectID": "vignettes/articles/IV-benchmarking.html#best-100-candidates",
    "href": "vignettes/articles/IV-benchmarking.html#best-100-candidates",
    "title": "4 Benchmarking Performance",
    "section": "Best 100 candidates",
    "text": "Best 100 candidates\n\nPositive\n\n\n\n\nNegative"
  },
  {
    "objectID": "vignettes/articles/IV-benchmarking.html#best-25-candidates-zoomed",
    "href": "vignettes/articles/IV-benchmarking.html#best-25-candidates-zoomed",
    "title": "4 Benchmarking Performance",
    "section": "Best 25 candidates (zoomed)",
    "text": "Best 25 candidates (zoomed)\n\nPositive\n\n\n\n\nNegative"
  },
  {
    "objectID": "vignettes/articles/IV-benchmarking.html#candidates-distribution",
    "href": "vignettes/articles/IV-benchmarking.html#candidates-distribution",
    "title": "4 Benchmarking Performance",
    "section": "Candidates distribution",
    "text": "Candidates distribution\n\nPositive\n\n\n\n\nNegative"
  },
  {
    "objectID": "vignettes/articles/0-validating.html",
    "href": "vignettes/articles/0-validating.html",
    "title": "0 Validating your data",
    "section": "",
    "text": "Before starting the TIMA pipeline, it’s recommended to validate your input data. This pre-flight check saves you time by catching issues immediately instead of discovering them after expensive operations like library downloads."
  },
  {
    "objectID": "vignettes/articles/0-validating.html#why-validate-your-data-first",
    "href": "vignettes/articles/0-validating.html#why-validate-your-data-first",
    "title": "0 Validating your data",
    "section": "",
    "text": "Before starting the TIMA pipeline, it’s recommended to validate your input data. This pre-flight check saves you time by catching issues immediately instead of discovering them after expensive operations like library downloads."
  },
  {
    "objectID": "vignettes/articles/0-validating.html#quick-start",
    "href": "vignettes/articles/0-validating.html#quick-start",
    "title": "0 Validating your data",
    "section": "Quick Start",
    "text": "Quick Start\n\nlibrary(tima)\n\n# Validate all inputs before starting\nvalidate_inputs(\n  features = \"data/features.csv\",\n  spectra = \"data/spectra.mgf\",\n  metadata = \"data/metadata.tsv\",\n  sirius = \"output/sirius\"\n)\n\n\nExample Output\n============================================================\nData Sanitizing: Pre-flight Checks\n============================================================\nChecking features file...\n✓ Features file: 1250 rows, 15 columns\n  Columns: feature_id, mz, rt, intensity, ...\n\nChecking MGF file...\n✓ MGF file: 843 spectra found\n\nChecking metadata file...\n✓ Metadata file: 120/120 files found\n\nChecking SIRIUS output...\n✓ SIRIUS output: 843 annotations, all required files present\n  - formula_identifications_all.tsv: ✓\n  - canopus_summary_all.tsv: ✓\n  - compound_identifications_all.tsv: ✓\n============================================================\n✓ All pre-flight checks passed!\nData validation complete. Ready to proceed.\n============================================================"
  },
  {
    "objectID": "vignettes/articles/0-validating.html#what-gets-validated",
    "href": "vignettes/articles/0-validating.html#what-gets-validated",
    "title": "0 Validating your data",
    "section": "What Gets Validated?",
    "text": "What Gets Validated?\n\nMGF Files (Mass Spectra)\nChecks for: - File exists and is readable - Contains valid spectra (BEGIN IONS markers) - Each spectrum has PEPMASS field - Peak data is present - BEGIN IONS/END IONS are balanced\nReports: - Number of spectra found - Specific issues if any\n\n# Validate just the MGF file\nvalidate_inputs(spectra = \"data/spectra.mgf\")\n\n\n\nFeatures Files (CSV/TSV)\nChecks for: - File exists and is readable - Not empty (has rows and columns) - Required columns present (e.g., feature_id) - Valid CSV/TSV format\nReports: - Number of rows (features) - Number of columns - Column names - Missing required columns\n\n# Validate features file\nvalidate_inputs(features = \"data/features.csv\")\n\n\n\nMetadata Consistency\nChecks for: - Metadata file exists - Filename column present - Files referenced in metadata actually exist - No broken file references\nReports: - Number of files in metadata - Number of files found on disk - List of missing files\n\n# Validate metadata consistency\nvalidate_inputs(\n  metadata = \"data/metadata.tsv\",\n  metadata_filename_col = \"filename\",\n  metadata_data_dir = \"data/raw\"\n)\n\n\n\nSIRIUS Output Completeness\nChecks for: - SIRIUS directory exists - Required summary files present: - formula_identifications_all.tsv - canopus_formula_summary_all.tsv - compound/structure_identifications_all.tsv - Feature directories exist\nReports: - Presence of each required file - Number of feature directories - Specific missing files\n\n# Validate SIRIUS output\nvalidate_inputs(sirius = \"output/sirius\")"
  },
  {
    "objectID": "vignettes/articles/0-validating.html#integration-with-pipeline",
    "href": "vignettes/articles/0-validating.html#integration-with-pipeline",
    "title": "0 Validating your data",
    "section": "Integration with Pipeline",
    "text": "Integration with Pipeline\nThe validation functions are automatically integrated into the main TIMA functions:\n\nAutomatic Validation\n\n# These functions automatically validate inputs\nannotate_masses(...)  # Validates features file first\nannotate_spectra(...) # Validates MGF files first\n\n\n\nManual Validation\nFor full control, validate manually before running the pipeline:\n\n# 1. Validate everything first\nvalidate_inputs(\n  features = \"data/features.csv\",\n  spectra = \"data/spectra.mgf\",\n  sirius = \"output/sirius\"\n)\n\n# 2. If all checks pass, run the pipeline\nrun_tima()"
  },
  {
    "objectID": "vignettes/articles/0-validating.html#common-issues-caught",
    "href": "vignettes/articles/0-validating.html#common-issues-caught",
    "title": "0 Validating your data",
    "section": "Common Issues Caught",
    "text": "Common Issues Caught\n\nMGF File Issues\nIssue: No spectra found\n✗ MGF file has issues:\n  - No spectra found (no BEGIN IONS markers)\nFix: Check if the file is actually in MGF format and contains spectra.\n\nIssue: Mismatched markers\n✗ MGF file has issues:\n  - Mismatched BEGIN IONS (10) and END IONS (9)\nFix: One spectrum is missing an END IONS marker. Check the file structure.\n\nIssue: Missing required fields\n✗ MGF file has issues:\n  - First spectrum missing PEPMASS field\nFix: Each spectrum must have a PEPMASS field specifying the precursor m/z.\n\n\nFeatures File Issues\nIssue: Empty file\n✗ Features file has issues:\n  - features table is empty (0 rows)\nFix: Ensure the file contains actual feature data, not just a header.\n\nIssue: Missing columns\n✗ Features file has issues:\n  - Missing required columns: feature_id, mz\nFix: Add the required columns to your features table.\n\n\nMetadata Issues\nIssue: Files not found\n✗ Metadata validation has issues:\n  - Missing 5/120 files referenced in metadata\n  Missing files:\n    - sample_001.mzML\n    - sample_023.mzML\n    - sample_045.mzML\n    - sample_067.mzML\n    - sample_089.mzML\nFix: Ensure all files listed in metadata are present in the data directory.\n\nIssue: Wrong column name\n✗ Metadata validation has issues:\n  - Column 'filename' not found in metadata\nFix: Rename your column to filename or specify the correct column with metadata_filename_col parameter.\n\n\nSIRIUS Issues\nIssue: Missing output files\n✗ SIRIUS output has issues:\n  - Missing formula_identifications_all.tsv\n  - Missing canopus_summary_all.tsv\nFix: Re-run SIRIUS with the correct output options enabled, or check that the output directory path is correct.\n\nIssue: Empty output\n✗ SIRIUS output has issues:\n  - No feature directories found in SIRIUS output\nFix: The SIRIUS job may have failed or not completed. Check SIRIUS logs."
  },
  {
    "objectID": "vignettes/articles/0-validating.html#advanced-usage",
    "href": "vignettes/articles/0-validating.html#advanced-usage",
    "title": "0 Validating your data",
    "section": "Advanced Usage",
    "text": "Advanced Usage\n\nProgrammatic Validation\nFor scripts and pipelines, you can capture validation results:\n\n# The function returns TRUE on success or stops with error\ntryCatch(\n  {\n    validate_inputs(features = \"data/features.csv\")\n    message(\"Validation passed - proceeding...\")\n    # Continue with pipeline\n  },\n  error = function(e) {\n    message(\"Validation failed: \", e$message)\n    # Handle error (e.g., send notification, log, exit)\n  }\n)\n\n\n\nBatch Validation\nValidate multiple datasets in a loop:\n\ndatasets &lt;- c(\"dataset1\", \"dataset2\", \"dataset3\")\n\nfor (dataset in datasets) {\n  message(\"Validating \", dataset, \"...\")\n\n  tryCatch(\n    {\n      validate_inputs(\n        features = file.path(dataset, \"features.csv\"),\n        spectra = file.path(dataset, \"spectra.mgf\")\n      )\n      message(\"  ✓ \", dataset, \" is valid\")\n    },\n    error = function(e) {\n      message(\"  ✗ \", dataset, \" has issues: \", e$message)\n    }\n  )\n}"
  },
  {
    "objectID": "vignettes/articles/0-validating.html#best-practices",
    "href": "vignettes/articles/0-validating.html#best-practices",
    "title": "0 Validating your data",
    "section": "Best Practices",
    "text": "Best Practices\n\nAlways Validate First\nMake validation the first step in your workflow:\n\n# Good practice\nvalidate_inputs(features = \"data/features.csv\")\nrun_tima()\n\n# Bad practice (will waste time if data is invalid)\nrun_tima()  # Might fail after 10+ minutes\n\n\n\nValidate After Data Conversion\nIf you convert data formats (e.g., mzML to MGF), validate the output:\n\n# Convert data\nconvert_mzml_to_mgf(\"raw_data/\", \"processed/spectra.mgf\")\n\n# Validate immediately\nvalidate_inputs(spectra = \"processed/spectra.mgf\")\n\n\n\nInclude in Automated Pipelines\nAdd validation to your automated workflows:\n\n# Snakemake, Nextflow, or targets pipeline\nvalidate_inputs(...)  # Fails fast if data is bad\nrun_tima()            # Only runs if validation passes\n\n\n\nDocument Your Validation\nKeep a record of validation results:\n\n# Capture output to log file\nsink(\"validation_log.txt\")\nvalidate_inputs(\n  features = \"data/features.csv\",\n  spectra = \"data/spectra.mgf\",\n  sirius = \"output/sirius\"\n)\nsink()"
  },
  {
    "objectID": "vignettes/articles/0-validating.html#summary",
    "href": "vignettes/articles/0-validating.html#summary",
    "title": "0 Validating your data",
    "section": "Summary",
    "text": "Summary\nData validation with validate_inputs():\n\nSaves time - Catches errors in seconds, not minutes\nClear messages - Shows exactly what’s wrong and how to fix it\nComprehensive - Checks MGF, CSV, metadata, and SIRIUS data\nAutomatic - Integrated into main TIMA functions\nPreventive - Stops before expensive operations\nInformative - Reports row counts, spectra counts, file lists\n\nAlways validate your data first! It’s the best way to ensure a smooth TIMA experience."
  },
  {
    "objectID": "vignettes/articles/0-validating.html#next-steps",
    "href": "vignettes/articles/0-validating.html#next-steps",
    "title": "0 Validating your data",
    "section": "Next Steps",
    "text": "Next Steps\nNow that your data is validated:\n\nI. Gathering - Collect your data\nII. Preparing - Prepare your libraries\nIII. Processing - Run the annotation pipeline\nIV. Benchmarking - Evaluate your results"
  },
  {
    "objectID": "vignettes/articles/III-processing.html",
    "href": "vignettes/articles/III-processing.html",
    "title": "3 Performing Taxonomically Informed Metabolite Annotation",
    "section": "",
    "text": "This vignette describes how Taxonomically Informed Metabolite Annotation is performed. If you followed all previous steps successfully, this should be a piece of cake, you deserve it!\n\ntima::run_tima()\n#&gt; + par_def_pre_lib_sop_lot dispatched\n#&gt; ✔ par_def_pre_lib_sop_lot completed [26ms, 494 B]\n#&gt; + par_def_pre_ann_sir dispatched\n#&gt; ✔ par_def_pre_ann_sir completed [1ms, 1.93 kB]\n#&gt; + par_def_pre_lib_sop_ecm dispatched\n#&gt; ✔ par_def_pre_lib_sop_ecm completed [1ms, 492 B]\n#&gt; + par_def_pre_ann_gnp dispatched\n#&gt; ✔ par_def_pre_ann_gnp completed [1ms, 1.42 kB]\n#&gt; + par_def_pre_lib_sop_mer dispatched\n#&gt; ✔ par_def_pre_lib_sop_mer completed [1ms, 3.39 kB]\n#&gt; + par_def_ann_spe dispatched\n#&gt; ✔ par_def_ann_spe completed [1ms, 2.14 kB]\n#&gt; + par_def_cre_edg_spe dispatched\n#&gt; ✔ par_def_cre_edg_spe completed [0ms, 1.42 kB]\n#&gt; + par_def_pre_fea_com dispatched\n#&gt; ✔ par_def_pre_fea_com completed [1ms, 358 B]\n#&gt; + par_def_pre_fea_tab dispatched\n#&gt; ✔ par_def_pre_fea_tab completed [0ms, 860 B]\n#&gt; + par_def_pre_lib_rt dispatched\n#&gt; ✔ par_def_pre_lib_rt completed [1ms, 2.20 kB]\n#&gt; + par_def_pre_tax dispatched\n#&gt; ✔ par_def_pre_tax completed [1ms, 1.51 kB]\n#&gt; + par_def_pre_lib_sop_hmd dispatched\n#&gt; ✔ par_def_pre_lib_sop_hmd completed [0ms, 492 B]\n#&gt; + par_def_pre_lib_sop_big dispatched\n#&gt; ✔ par_def_pre_lib_sop_big completed [1ms, 314 B]\n#&gt; + par_def_cre_com dispatched\n#&gt; ✔ par_def_cre_com completed [0ms, 375 B]\n#&gt; + par_def_wei_ann dispatched\n#&gt; ✔ par_def_wei_ann completed [1ms, 5.34 kB]\n#&gt; + par_def_pre_fea_edg dispatched\n#&gt; ✔ par_def_pre_fea_edg completed [0ms, 706 B]\n#&gt; + yaml_paths dispatched\n#&gt; ✔ yaml_paths completed [1ms, 11.70 kB]\n#&gt; + par_def_fil_ann dispatched\n#&gt; ✔ par_def_fil_ann completed [1ms, 1.34 kB]\n#&gt; + par_def_pre_lib_sop_clo dispatched\n#&gt; ✔ par_def_pre_lib_sop_clo completed [0ms, 523 B]\n#&gt; + par_def_pre_lib_spe dispatched\n#&gt; ✔ par_def_pre_lib_spe completed [1ms, 1.57 kB]\n#&gt; + par_def_pre_ann_mzm dispatched\n#&gt; ✔ par_def_pre_ann_mzm completed [0ms, 1.43 kB]\n#&gt; + par_def_ann_mas dispatched\n#&gt; ✔ par_def_ann_mas completed [1ms, 6.09 kB]\n#&gt; + par_def_pre_ann_spe dispatched\n#&gt; ✔ par_def_pre_ann_spe completed [1ms, 1.46 kB]\n#&gt; + paths dispatched\n#&gt; ✔ paths completed [1ms, 2.55 kB]\n#&gt; + lib_sop_ecm dispatched\n#&gt; [2026-02-28 15:59:57.594] [INFO ] &gt; Starting: download_file [url=https://ecmdb.ca/download/ecmdb.json.zip, destination=data/source/libraries/sop/ecmdb.json.zip]\n#&gt; [2026-02-28 15:59:58.364] [INFO ] [OK] Completed: download_file [size_bytes=1334921] (745ms)\n#&gt; ✔ lib_sop_ecm completed [802ms, 1.33 MB]\n#&gt; + lib_spe_exp_mer_pre_pos dispatched\n#&gt; [2026-02-28 15:59:58.526] [INFO ] &gt; Starting: download_file [url=https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/merlin_13911806_pos.rds, destination=data/interim/libraries/spectra/exp/merlin_13911806_pos.rds]\n#&gt; [2026-02-28 15:59:58.899] [INFO ] [OK] Completed: download_file [size_bytes=84936306] (373ms)\n#&gt; ✔ lib_spe_exp_mer_pre_pos completed [375ms, 84.94 MB]\n#&gt; + lib_spe_exp_gnp_pre_pos dispatched\n#&gt; [2026-02-28 15:59:59.086] [INFO ] &gt; Starting: download_file [url=https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/gnps_11566051_pos.rds, destination=data/interim/libraries/spectra/exp/gnps_11566051_pos.rds]\n#&gt; [2026-02-28 16:00:00.318] [INFO ] [OK] Completed: download_file [size_bytes=481271483] (1.2s)\n#&gt; ✔ lib_spe_exp_gnp_pre_pos completed [1.2s, 481.27 MB]\n#&gt; + lib_spe_exp_mb_pre_pos dispatched\n#&gt; [2026-02-28 16:00:00.646] [INFO ] &gt; Starting: download_file [url=https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/massbank_2025051_pos.rds, destination=data/interim/libraries/spectra/exp/massbank_2025051_pos.rds]\n#&gt; [2026-02-28 16:00:00.789] [INFO ] [OK] Completed: download_file [size_bytes=19411864] (143ms)\n#&gt; ✔ lib_spe_exp_mb_pre_pos completed [144ms, 19.41 MB]\n#&gt; + lib_spe_is_wik_pre_pos dispatched\n#&gt; [2026-02-28 16:00:00.950] [INFO ] &gt; Starting: download_file [url=https://github.com/taxonomicallyinformedannotation/tima-isdb-pos/raw/main/wikidata_5607185_pos.rds, destination=data/interim/libraries/spectra/is/wikidata_5607185_pos.rds]\n#&gt; [2026-02-28 16:00:03.019] [INFO ] [OK] Completed: download_file [size_bytes=863950396] (2.1s)\n#&gt; ✔ lib_spe_is_wik_pre_pos completed [2.1s, 863.95 MB]\n#&gt; + lib_spe_exp_mer_pre_sop dispatched\n#&gt; [2026-02-28 16:00:03.480] [INFO ] &gt; Starting: download_file [url=https://github.com/Adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/sop/merlin_13911806_prepared.tsv.gz, destination=data/interim/libraries/sop/merlin_13911806_prepared.tsv.gz]\n#&gt; [2026-02-28 16:00:03.689] [INFO ] [OK] Completed: download_file [size_bytes=1190284] (208ms)\n#&gt; ✔ lib_spe_exp_mer_pre_sop completed [209ms, 1.19 MB]\n#&gt; + lib_sop_lot dispatched\n#&gt; [2026-02-28 16:00:03.845] [INFO ] Retrieving latest version from Zenodo: 10.5281/zenodo.5794106\n#&gt; [2026-02-28 16:00:05.812] [INFO ] Downloading 230106_frozen_metadata.csv.gz from https://doi.org/10.5281/zenodo.5794106\n#&gt; [2026-02-28 16:00:05.814] [INFO ] &gt; Starting: download_file [url=https://zenodo.org/records/7534071/files/230106_frozen_metadata.csv.gz, destination=data/source/libraries/sop/lotus.csv.gz]\n#&gt; [2026-02-28 16:00:37.094] [INFO ] [OK] Completed: download_file [size_bytes=92979778] (31.3s)\n#&gt; [2026-02-28 16:00:37.095] [INFO ] Download completed: data/source/libraries/sop/lotus.csv.gz\n#&gt; ✔ lib_sop_lot completed [33.3s, 92.98 MB]\n#&gt; + lib_spe_exp_gnp_pre_neg dispatched\n#&gt; [2026-02-28 16:00:37.283] [INFO ] &gt; Starting: download_file [url=https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/gnps_11566051_neg.rds, destination=data/interim/libraries/spectra/exp/gnps_11566051_neg.rds]\n#&gt; [2026-02-28 16:00:37.762] [INFO ] [OK] Completed: download_file [size_bytes=154124724] (479ms)\n#&gt; ✔ lib_spe_exp_gnp_pre_neg completed [481ms, 154.12 MB]\n#&gt; + par_pre_par dispatched\n#&gt; ✔ par_pre_par completed [1ms, 1.54 kB]\n#&gt; + lib_spe_exp_mb_pre_neg dispatched\n#&gt; [2026-02-28 16:00:38.121] [INFO ] &gt; Starting: download_file [url=https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/massbank_2025051_neg.rds, destination=data/interim/libraries/spectra/exp/massbank_2025051_neg.rds]\n#&gt; [2026-02-28 16:00:38.233] [INFO ] [OK] Completed: download_file [size_bytes=7057574] (111ms)\n#&gt; ✔ lib_spe_exp_mb_pre_neg completed [113ms, 7.06 MB]\n#&gt; + lib_spe_exp_mer_pre_neg dispatched\n#&gt; [2026-02-28 16:00:38.391] [INFO ] &gt; Starting: download_file [url=https://github.com/adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/spectra/exp/merlin_13911806_neg.rds, destination=data/interim/libraries/spectra/exp/merlin_13911806_neg.rds]\n#&gt; [2026-02-28 16:00:38.548] [INFO ] [OK] Completed: download_file [size_bytes=31540426] (158ms)\n#&gt; ✔ lib_spe_exp_mer_pre_neg completed [159ms, 31.54 MB]\n#&gt; + lib_spe_is_wik_pre_neg dispatched\n#&gt; [2026-02-28 16:00:38.716] [INFO ] &gt; Starting: download_file [url=https://github.com/taxonomicallyinformedannotation/tima-isdb-neg/raw/main/wikidata_5607185_neg.rds, destination=data/interim/libraries/spectra/is/wikidata_5607185_neg.rds]\n#&gt; [2026-02-28 16:00:40.326] [INFO ] [OK] Completed: download_file [size_bytes=687328159] (1.6s)\n#&gt; ✔ lib_spe_is_wik_pre_neg completed [1.6s, 687.33 MB]\n#&gt; + lib_sop_hmd dispatched\n#&gt; [2026-02-28 16:00:40.723] [INFO ] &gt; Starting: download_file [url=https://hmdb.ca/system/downloads/current/structures.zip, destination=data/source/libraries/sop/hmdb/structures.zip]\n#&gt; [2026-02-28 16:00:40.852] [WARN ] file download failed (attempt 1/3), retrying in 1s: HTTP 403 Forbidden.\n#&gt; [2026-02-28 16:00:41.892] [WARN ] file download failed (attempt 2/3), retrying in 2s: HTTP 403 Forbidden.\n#&gt; [2026-02-28 16:00:43.936] [WARN ] HMDB download failed. Creating minimal placeholder SDF file.\n#&gt; ✔ lib_sop_hmd completed [3.3s, 337 B]\n#&gt; + lib_spe_is_wik_pre_sop dispatched\n#&gt; [2026-02-28 16:00:44.139] [INFO ] &gt; Starting: download_file [url=https://github.com/taxonomicallyinformedannotation/tima-example-files/raw/main/wikidata_spectral_5607185_prepared.tsv.gz, destination=data/interim/libraries/sop/wikidata_5607185_prepared.tsv.gz]\n#&gt; [2026-02-28 16:00:44.336] [INFO ] [OK] Completed: download_file [size_bytes=37904410] (197ms)\n#&gt; ✔ lib_spe_is_wik_pre_sop completed [199ms, 37.90 MB]\n#&gt; + par_pre_par2 dispatched\n#&gt; ✔ par_pre_par2 completed [0ms, 21.97 kB]\n#&gt; + lib_spe_exp_mb_pre_sop dispatched\n#&gt; [2026-02-28 16:00:44.659] [INFO ] &gt; Starting: download_file [url=https://github.com/Adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/sop/massbank_2025051_prepared.tsv.gz, destination=data/interim/libraries/sop/massbank_2025051_prepared.tsv.gz]\n#&gt; [2026-02-28 16:00:44.748] [INFO ] [OK] Completed: download_file [size_bytes=480970] (89ms)\n#&gt; ✔ lib_spe_exp_mb_pre_sop completed [91ms, 480.97 kB]\n#&gt; + lib_spe_exp_gnp_pre_sop dispatched\n#&gt; [2026-02-28 16:00:44.904] [INFO ] &gt; Starting: download_file [url=https://github.com/Adafede/SpectRalLibRaRies/raw/main/data/interim/libraries/sop/gnps_11566051_prepared.tsv.gz, destination=data/interim/libraries/sop/gnps_11566051_prepared.tsv.gz]\n#&gt; [2026-02-28 16:00:45.122] [INFO ] [OK] Completed: download_file [size_bytes=1416699] (218ms)\n#&gt; ✔ lib_spe_exp_gnp_pre_sop completed [220ms, 1.42 MB]\n#&gt; + par_fin_par dispatched\n#&gt; ✔ par_fin_par completed [1ms, 332 B]\n#&gt; + par_fin_par2 dispatched\n#&gt; ✔ par_fin_par2 completed [1ms, 3.04 kB]\n#&gt; + par_usr_pre_lib_sop_lot dispatched\n#&gt; ✔ par_usr_pre_lib_sop_lot completed [1.5s, 174 B]\n#&gt; + par_usr_pre_ann_sir dispatched\n#&gt; ✔ par_usr_pre_ann_sir completed [1.5s, 900 B]\n#&gt; + par_usr_pre_lib_sop_ecm dispatched\n#&gt; ✔ par_usr_pre_lib_sop_ecm completed [1.4s, 176 B]\n#&gt; + par_usr_ann_spe dispatched\n#&gt; ✔ par_usr_ann_spe completed [1.4s, 1.01 kB]\n#&gt; + par_usr_ann_mas dispatched\n#&gt; ✔ par_usr_ann_mas completed [1.5s, 2.75 kB]\n#&gt; + par_usr_pre_lib_sop_mer dispatched\n#&gt; ✔ par_usr_pre_lib_sop_mer completed [1.4s, 1.60 kB]\n#&gt; + par_usr_wei_ann dispatched\n#&gt; ✔ par_usr_wei_ann completed [1.4s, 1.80 kB]\n#&gt; + par_usr_cre_edg_spe dispatched\n#&gt; ✔ par_usr_cre_edg_spe completed [1.5s, 434 B]\n#&gt; + par_usr_pre_fea_edg dispatched\n#&gt; ✔ par_usr_pre_fea_edg completed [1.6s, 328 B]\n#&gt; + par_usr_pre_fea_com dispatched\n#&gt; ✔ par_usr_pre_fea_com completed [1.4s, 200 B]\n#&gt; + par_usr_pre_lib_rt dispatched\n#&gt; ✔ par_usr_pre_lib_rt completed [1.4s, 487 B]\n#&gt; + par_usr_pre_tax dispatched\n#&gt; ✔ par_usr_pre_tax completed [1.4s, 438 B]\n#&gt; + par_usr_pre_fea_tab dispatched\n#&gt; ✔ par_usr_pre_fea_tab completed [1.4s, 274 B]\n#&gt; + par_usr_pre_lib_sop_hmd dispatched\n#&gt; ✔ par_usr_pre_lib_sop_hmd completed [1.5s, 178 B]\n#&gt; + par_usr_pre_lib_sop_big dispatched\n#&gt; ✔ par_usr_pre_lib_sop_big completed [1.4s, 107 B]\n#&gt; + par_usr_pre_ann_spe dispatched\n#&gt; ✔ par_usr_pre_ann_spe completed [1.4s, 731 B]\n#&gt; + par_usr_cre_com dispatched\n#&gt; ✔ par_usr_cre_com completed [1.5s, 200 B]\n#&gt; + par_usr_pre_ann_mzm dispatched\n#&gt; ✔ par_usr_pre_ann_mzm completed [1.5s, 710 B]\n#&gt; + par_usr_pre_lib_sop_clo dispatched\n#&gt; ✔ par_usr_pre_lib_sop_clo completed [1.5s, 267 B]\n#&gt; + par_usr_pre_lib_spe dispatched\n#&gt; ✔ par_usr_pre_lib_spe completed [1.5s, 298 B]\n#&gt; + par_usr_pre_ann_gnp dispatched\n#&gt; ✔ par_usr_pre_ann_gnp completed [1.5s, 708 B]\n#&gt; + par_usr_fil_ann dispatched\n#&gt; ✔ par_usr_fil_ann completed [1.5s, 739 B]\n#&gt; + par_pre_lib_sop_lot dispatched\n#&gt; ✔ par_pre_lib_sop_lot completed [1ms, 186 B]\n#&gt; + par_pre_ann_sir dispatched\n#&gt; ✔ par_pre_ann_sir completed [2ms, 405 B]\n#&gt; + par_pre_lib_sop_ecm dispatched\n#&gt; ✔ par_pre_lib_sop_ecm completed [1ms, 191 B]\n#&gt; + par_ann_spe dispatched\n#&gt; ✔ par_ann_spe completed [2ms, 495 B]\n#&gt; + par_ann_mas dispatched\n#&gt; ✔ par_ann_mas completed [2ms, 1.14 kB]\n#&gt; + par_pre_lib_sop_mer dispatched\n#&gt; ✔ par_pre_lib_sop_mer completed [2ms, 559 B]\n#&gt; + par_wei_ann dispatched\n#&gt; ✔ par_wei_ann completed [2ms, 967 B]\n#&gt; + par_cre_edg_spe dispatched\n#&gt; ✔ par_cre_edg_spe completed [2ms, 387 B]\n#&gt; + par_pre_fea_edg dispatched\n#&gt; ✔ par_pre_fea_edg completed [2ms, 244 B]\n#&gt; + par_pre_fea_com dispatched\n#&gt; ✔ par_pre_fea_com completed [1ms, 184 B]\n#&gt; + par_pre_lib_rt dispatched\n#&gt; ✔ par_pre_lib_rt completed [2ms, 375 B]\n#&gt; + par_pre_tax dispatched\n#&gt; ✔ par_pre_tax completed [1ms, 330 B]\n#&gt; + par_pre_fea_tab dispatched\n#&gt; ✔ par_pre_fea_tab completed [1ms, 278 B]\n#&gt; + par_pre_lib_sop_hmd dispatched\n#&gt; ✔ par_pre_lib_sop_hmd completed [1ms, 191 B]\n#&gt; + par_pre_lib_sop_big dispatched\n#&gt; ✔ par_pre_lib_sop_big completed [1ms, 155 B]\n#&gt; + par_pre_ann_spe dispatched\n#&gt; ✔ par_pre_ann_spe completed [1ms, 334 B]\n#&gt; + par_cre_com dispatched\n#&gt; ✔ par_cre_com completed [1ms, 191 B]\n#&gt; + par_pre_ann_mzm dispatched\n#&gt; ✔ par_pre_ann_mzm completed [1ms, 341 B]\n#&gt; + par_pre_lib_sop_clo dispatched\n#&gt; ✔ par_pre_lib_sop_clo completed [1ms, 232 B]\n#&gt; + par_pre_lib_spe dispatched\n#&gt; ✔ par_pre_lib_spe completed [1ms, 404 B]\n#&gt; + par_pre_ann_gnp dispatched\n#&gt; ✔ par_pre_ann_gnp completed [1ms, 336 B]\n#&gt; + par_fil_ann dispatched\n#&gt; ✔ par_fil_ann completed [2ms, 360 B]\n#&gt; + lib_sop_lot_pre dispatched\n#&gt; [2026-02-28 16:01:25.244] [INFO ] &gt; Starting: prepare_libraries_sop_lotus [input=data/source/libraries/sop/lotus.csv.gz]\n#&gt; [2026-02-28 16:01:35.666] [INFO ] [OK] Completed: prepare_libraries_sop_lotus [n_pairs=791809] (10.4s)\n#&gt; [2026-02-28 16:01:35.668] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/lotus_prepared.tsv.gz, n_rows=791809]\n#&gt; [2026-02-28 16:01:39.134] [INFO ] [OK] Completed: export_output [size_bytes=46518841] (3.5s)\n#&gt; ✔ lib_sop_lot_pre completed [13.9s, 46.52 MB]\n#&gt; + lib_sop_ecm_pre dispatched\n#&gt; [2026-02-28 16:01:39.513] [INFO ] Preparing ECMDB structure-organism pairs\n#&gt; [2026-02-28 16:01:40.168] [INFO ] Exporting parameters to: data/interim/params/260228_160140_prepare_libraries_sop_ecmdb.yaml\n#&gt; [2026-02-28 16:01:40.170] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/ecmdb_prepared.tsv.gz, n_rows=3760]\n#&gt; [2026-02-28 16:01:40.187] [INFO ] [OK] Completed: export_output [size_bytes=177466] (16ms)\n#&gt; ✔ lib_sop_ecm_pre completed [676ms, 177.47 kB]\n#&gt; + par_ann_spe_fil_spe_raw dispatched\n#&gt; ✔ par_ann_spe_fil_spe_raw completed [0ms, 7.77 MB]\n#&gt; + lib_sop_mer_str_pro dispatched\n#&gt; [2026-02-28 16:01:40.594] [INFO ] &gt; Starting: download_file [url=https://github.com/taxonomicallyinformedannotation/tima-example-files/raw/main/processed.csv.gz, destination=data/interim/libraries/sop/merged/structures/processed.csv.gz]\n#&gt; [2026-02-28 16:01:40.900] [INFO ] [OK] Completed: download_file [size_bytes=80668186] (306ms)\n#&gt; ✔ lib_sop_mer_str_pro completed [308ms, 80.67 MB]\n#&gt; + lib_rt dispatched\n#&gt; [2026-02-28 16:01:41.112] [INFO ] Preparing retention time libraries\n#&gt; [2026-02-28 16:01:41.125] [WARN ] No retention time library found, returning empty retention time and sop tables.\n#&gt; [2026-02-28 16:01:41.168] [INFO ] Exporting parameters to: data/interim/params/260228_160141_prepare_libraries_rt.yaml\n#&gt; [2026-02-28 16:01:41.170] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/rt/prepared.tsv.gz, n_rows=1]\n#&gt; [2026-02-28 16:01:41.171] [INFO ] [OK] Completed: export_output [size_bytes=86] (1ms)\n#&gt; [2026-02-28 16:01:41.174] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/rt_prepared.tsv.gz, n_rows=1]\n#&gt; [2026-02-28 16:01:41.175] [INFO ] [OK] Completed: export_output [size_bytes=105] (1ms)\n#&gt; ✔ lib_rt completed [66ms, 191 B]\n#&gt; + par_pre_fea_tab_fil_fea_raw dispatched\n#&gt; ✔ par_pre_fea_tab_fil_fea_raw completed [1ms, 451.55 kB]\n#&gt; + lib_sop_hmd_pre dispatched\n#&gt; [2026-02-28 16:01:41.536] [INFO ] &gt; Starting: prepare_libraries_sop_hmdb [input=data/source/libraries/sop/hmdb/structures.zip]\n#&gt; [2026-02-28 16:01:41.563] [WARN ] Empty dataframe in select_sop_columns\n#&gt; [2026-02-28 16:01:41.567] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/hmdb_prepared.tsv.gz, n_rows=0]\n#&gt; [2026-02-28 16:01:41.568] [INFO ] [OK] Completed: export_output [size_bytes=257] (1ms)\n#&gt; [2026-02-28 16:01:41.569] [INFO ] [OK] Completed: prepare_libraries_sop_hmdb [n_pairs=0] (33ms)\n#&gt; ✔ lib_sop_hmd_pre completed [35ms, 257 B]\n#&gt; + lib_sop_big_pre dispatched\n#&gt; [2026-02-28 16:01:41.753] [INFO ] Preparing BiGG structure-organism pairs\n#&gt; [2026-02-28 16:02:04.603] [INFO ] &gt; Starting: process_smiles [n_structures=1420]\n#&gt; [2026-02-28 16:02:04.604] [INFO ] Processing SMILES with RDKit\n#&gt; [2026-02-28 16:02:05.550] [INFO ] Processing 1419 new SMILES with RDKit\n#&gt; [2026-02-28 16:02:05.552] [INFO ] Starting SMILES processing pipeline\n#&gt; [2026-02-28 16:02:05.552] [INFO ] Input: /tmp/RtmpA7jyLX/file270c7f41a1ac.smi\n#&gt; [2026-02-28 16:02:05.552] [INFO ] Output: /tmp/RtmpA7jyLX/file270c3f2f5ae0.csv.gz\n#&gt; [2026-02-28 16:02:05.552] [INFO ] Input file validated: /tmp/RtmpA7jyLX/file270c7f41a1ac.smi\n#&gt; [2026-02-28 16:02:05.552] [INFO ] Output file validated: /tmp/RtmpA7jyLX/file270c3f2f5ae0.csv.gz\n#&gt; [2026-02-28 16:02:05.553] [INFO ] Processing parameters: workers=8, batch_size=1000, progress_interval=10000\n#&gt; [2026-02-28 16:02:05.553] [INFO ] SMILES supplier initialized\n#&gt; [2026-02-28 16:02:07.601] [INFO ] Processing complete. Total molecules processed: 1419\n#&gt; [2026-02-28 16:02:07.639] [INFO ] Successfully processed 1419 SMILES\n#&gt; [2026-02-28 16:02:07.649] [INFO ] [OK] Completed: process_smiles [n_processed=1419] (3s)\n#&gt; [2026-02-28 16:02:13.417] [INFO ] &gt; Starting: process_smiles [n_structures=2084]\n#&gt; [2026-02-28 16:02:13.418] [INFO ] Processing SMILES with RDKit\n#&gt; [2026-02-28 16:02:13.430] [INFO ] Processing 1241 new SMILES with RDKit\n#&gt; [2026-02-28 16:02:13.431] [INFO ] Starting SMILES processing pipeline\n#&gt; [2026-02-28 16:02:13.431] [INFO ] Input: /tmp/RtmpA7jyLX/file270c7937b599.smi\n#&gt; [2026-02-28 16:02:13.431] [INFO ] Output: /tmp/RtmpA7jyLX/file270c369c0489.csv.gz\n#&gt; [2026-02-28 16:02:13.431] [INFO ] Input file validated: /tmp/RtmpA7jyLX/file270c7937b599.smi\n#&gt; [2026-02-28 16:02:13.431] [INFO ] Output file validated: /tmp/RtmpA7jyLX/file270c369c0489.csv.gz\n#&gt; [2026-02-28 16:02:13.432] [INFO ] Processing parameters: workers=8, batch_size=1000, progress_interval=10000\n#&gt; [2026-02-28 16:02:13.432] [INFO ] SMILES supplier initialized\n#&gt; [2026-02-28 16:02:15.253] [INFO ] Processing complete. Total molecules processed: 1241\n#&gt; [2026-02-28 16:02:15.289] [INFO ] Successfully processed 1241 SMILES\n#&gt; [2026-02-28 16:02:15.297] [INFO ] [OK] Completed: process_smiles [n_processed=1241] (1.9s)\n#&gt; [2026-02-28 16:02:15.380] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/bigg_prepared.tsv.gz, n_rows=2354]\n#&gt; [2026-02-28 16:02:15.394] [INFO ] [OK] Completed: export_output [size_bytes=85402] (13ms)\n#&gt; ✔ lib_sop_big_pre completed [33.6s, 85.40 kB]\n#&gt; + lib_sop_clo_pre dispatched\n#&gt; [2026-02-28 16:02:15.672] [INFO ] Preparing closed structure-organism pairs library\n#&gt; [2026-02-28 16:02:15.673] [WARN ] Closed resource not accessible at: ~/Git/lotus-processor/data/processed/240412_closed_metadata.csv.gz. Returning empty template instead.\n#&gt; [2026-02-28 16:02:15.689] [INFO ] Exporting parameters to: data/interim/params/260228_160215_prepare_libraries_sop_closed.yaml\n#&gt; [2026-02-28 16:02:15.690] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/closed_prepared.tsv.gz, n_rows=1]\n#&gt; [2026-02-28 16:02:15.692] [INFO ] [OK] Completed: export_output [size_bytes=273] (1ms)\n#&gt; ✔ lib_sop_clo_pre completed [21ms, 273 B]\n#&gt; + lib_spe_exp_int_pre dispatched\n#&gt; [2026-02-28 16:02:15.964] [INFO ] &gt; Starting: prepare_libraries_spectra [library_name=internal, n_input_files=1]\n#&gt; [2026-02-28 16:02:15.969] [WARN ] Input file(s) not found; creating empty library template\n#&gt; [2026-02-28 16:02:17.359] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/internal_prepared.tsv.gz, n_rows=1]\n#&gt; [2026-02-28 16:02:17.360] [INFO ] [OK] Completed: export_output [size_bytes=106] (1ms)\n#&gt; [2026-02-28 16:02:17.428] [INFO ] Exporting parameters to: data/interim/params/260228_160217_prepare_libraries_spectra.yaml\n#&gt; [2026-02-28 16:02:17.429] [INFO ] [OK] Completed: prepare_libraries_spectra [n_structures=1, n_spectra_total=2, files_exported=3] (1.5s)\n#&gt; ✔ lib_spe_exp_int_pre completed [1.5s, 1.30 kB]\n#&gt; + input_spectra dispatched\n#&gt; ✔ input_spectra completed [0ms, 7.77 MB]\n#&gt; + lib_rt_rts dispatched\n#&gt; ✔ lib_rt_rts completed [0ms, 86 B]\n#&gt; + lib_rt_sop dispatched\n#&gt; ✔ lib_rt_sop completed [0ms, 105 B]\n#&gt; + input_features dispatched\n#&gt; ✔ input_features completed [1ms, 451.55 kB]\n#&gt; + lib_spe_exp_int_pre_pos dispatched\n#&gt; ✔ lib_spe_exp_int_pre_pos completed [0ms, 599 B]\n#&gt; + lib_spe_exp_int_pre_neg dispatched\n#&gt; ✔ lib_spe_exp_int_pre_neg completed [0ms, 599 B]\n#&gt; + lib_spe_exp_int_pre_sop dispatched\n#&gt; ✔ lib_spe_exp_int_pre_sop completed [0ms, 106 B]\n#&gt; + fea_edg_spe dispatched\n#&gt; [2026-02-28 16:02:20.441] [INFO ] &gt; Starting: create_edges_spectra [method=gnps, threshold=0.7, n_input_files=1]\n#&gt; [2026-02-28 16:02:20.442] [INFO ] Creating spectral similarity network edges\n#&gt; [2026-02-28 16:02:20.444] [INFO ] Importing spectra from: data/source/example_spectra.mgf\n#&gt; [2026-02-28 16:02:20.470] [INFO ] Reading MGF file (7.41 MB) with optimized parser: data/source/example_spectra.mgf\n#&gt; [2026-02-28 16:02:22.437] [INFO ] Processed 10000 spectra...\n#&gt; [2026-02-28 16:02:23.952] [INFO ] Total spectra read: 16282\n#&gt; [2026-02-28 16:02:31.394] [INFO ] Loaded 16282 spectra from file\n#&gt; [2026-02-28 16:02:31.404] [INFO ] Combining replicate spectra by FEATURE_ID\n#&gt; [2026-02-28 16:02:33.851] [INFO ] Combined replicates: 12195 -&gt; 4087 spectra\n#&gt; [2026-02-28 16:02:33.853] [INFO ] Sanitizing 4087 spectra (cutoff: dynamic)\n#&gt; [2026-02-28 16:02:35.188] [INFO ] Sanitization complete: 3281/4087 spectra retained (80.3%, 806 removed)\n#&gt; [2026-02-28 16:02:35.190] [INFO ] Import complete: 3281 spectra ready for analysis\n#&gt; [2026-02-28 16:02:35.190] [INFO ] ======================================\n#&gt; [2026-02-28 16:02:35.191] [INFO ] Take yourself a break, you deserve it.\n#&gt; [2026-02-28 16:02:35.192] [INFO ] ======================================\n#&gt; [2026-02-28 16:02:35.193] [INFO ] &gt; Starting: create_edges [n_spectra=3281, method=gnps, threshold=0.7, min_peaks=6]\n#&gt; [2026-02-28 16:03:55.571] [INFO ] Processed 500 / 3280 queries\n#&gt; [2026-02-28 16:05:03.341] [INFO ] Processed 1000 / 3280 queries\n#&gt; [2026-02-28 16:05:56.347] [INFO ] Processed 1500 / 3280 queries\n#&gt; [2026-02-28 16:06:35.775] [INFO ] Processed 2000 / 3280 queries\n#&gt; [2026-02-28 16:07:01.466] [INFO ] Processed 2500 / 3280 queries\n#&gt; [2026-02-28 16:07:14.739] [INFO ] Processed 3000 / 3280 queries\n#&gt; [2026-02-28 16:07:14.814] [WARN ] Similarity calculation failed: LAPJV sparse: infeasible (bug)\n#&gt; [2026-02-28 16:07:14.836] [WARN ] Similarity calculation failed: LAPJV sparse: infeasible (bug)\n#&gt; [2026-02-28 16:07:16.860] [INFO ] Here is the distribution of edge similarity scores (0.1 bins) BEFORE filtering:\n#&gt; [2026-02-28 16:07:16.862] [INFO ] \n#&gt;        bin       N\n#&gt;    [0,0.1] 1948664\n#&gt;  (0.1,0.2]       5\n#&gt;  (0.2,0.3]     212\n#&gt;  (0.3,0.4]    1434\n#&gt;  (0.4,0.5]    4823\n#&gt;  (0.5,0.6]   13533\n#&gt;  (0.6,0.7]   36066\n#&gt;  (0.7,0.8]   94635\n#&gt;  (0.8,0.9]  232684\n#&gt;    (0.9,1] 3048784\n#&gt; [2026-02-28 16:07:16.869] [INFO ] [OK] Completed: create_edges [n_edges=147800, n_comparisons=5380840, pass_rate=2.7%] (4m 42s)\n#&gt; [2026-02-28 16:07:16.955] [INFO ] Exporting parameters to: data/interim/params/260228_160716_create_edges_spectra.yaml\n#&gt; [2026-02-28 16:07:16.957] [INFO ] &gt; Starting: export_output [file=data/interim/features/example_edgesSpectra.tsv, n_rows=149022]\n#&gt; [2026-02-28 16:07:16.977] [INFO ] [OK] Completed: export_output [size_bytes=7583516] (20ms)\n#&gt; [2026-02-28 16:07:16.979] [INFO ] [OK] Completed: create_edges_spectra [n_edges=149022] (4m 57s)\n#&gt; ✔ fea_edg_spe completed [4m 56.5s, 7.58 MB]\n#&gt; + fea_pre dispatched\n#&gt; [2026-02-28 16:07:17.366] [INFO ] &gt; Starting: prepare_features_tables [input=data/source/example_features.csv, candidates=1]\n#&gt; [2026-02-28 16:07:17.478] [INFO ] Prepared 5328 feature-sample pairs\n#&gt; [2026-02-28 16:07:17.480] [INFO ] [OK] Completed: prepare_features_tables [n_features=5328] (114ms)\n#&gt; [2026-02-28 16:07:17.504] [INFO ] Exporting parameters to: data/interim/params/260228_160717_prepare_features_tables.yaml\n#&gt; [2026-02-28 16:07:17.505] [INFO ] &gt; Starting: export_output [file=data/interim/features/example_features.tsv.gz, n_rows=5328]\n#&gt; [2026-02-28 16:07:17.519] [INFO ] [OK] Completed: export_output [size_bytes=95629] (13ms)\n#&gt; ✔ fea_pre completed [156ms, 95.63 kB]\n#&gt; + ann_spe_pos dispatched\n#&gt; [2026-02-28 16:07:17.931] [INFO ] ============================================================\n#&gt; [2026-02-28 16:07:17.932] [INFO ] Data Sanitizing: Pre-flight Checks\n#&gt; [2026-02-28 16:07:17.933] [INFO ] ============================================================\n#&gt; [2026-02-28 16:07:17.934] [INFO ] Checking MGF file...\n#&gt; [2026-02-28 16:07:19.054] [INFO ] [OK] MGF file: 12195 MS2 spectra found\n#&gt; [2026-02-28 16:07:19.055] [INFO ] ============================================================\n#&gt; [2026-02-28 16:07:19.056] [INFO ] [OK] All pre-flight checks passed!\n#&gt; [2026-02-28 16:07:19.057] [INFO ] Data validation complete. Ready to proceed.\n#&gt; [2026-02-28 16:07:19.058] [INFO ] ============================================================\n#&gt; [2026-02-28 16:07:19.059] [INFO ] Starting spectral annotation in pos mode\n#&gt; [2026-02-28 16:07:19.060] [INFO ] Importing spectra from: data/source/example_spectra.mgf\n#&gt; [2026-02-28 16:07:19.061] [INFO ] Reading MGF file (7.41 MB) with optimized parser: data/source/example_spectra.mgf\n#&gt; [2026-02-28 16:07:21.199] [INFO ] Processed 10000 spectra...\n#&gt; [2026-02-28 16:07:22.609] [INFO ] Total spectra read: 16282\n#&gt; [2026-02-28 16:07:29.130] [INFO ] Loaded 16282 spectra from file\n#&gt; [2026-02-28 16:07:29.143] [INFO ] Combining replicate spectra by FEATURE_ID\n#&gt; [2026-02-28 16:07:29.956] [INFO ] Combined replicates: 12195 -&gt; 4087 spectra\n#&gt; [2026-02-28 16:07:29.957] [INFO ] Sanitizing 4087 spectra (cutoff: dynamic)\n#&gt; [2026-02-28 16:07:31.257] [INFO ] Sanitization complete: 3281/4087 spectra retained (80.3%, 806 removed)\n#&gt; [2026-02-28 16:07:31.259] [INFO ] Import complete: 3281 spectra ready for analysis\n#&gt; [2026-02-28 16:07:31.260] [INFO ] Importing spectra from: data/interim/libraries/spectra/is/wikidata_5607185_pos.rds\n#&gt; [2026-02-28 16:07:55.389] [INFO ] Loaded 998198 spectra from file\n#&gt; [2026-02-28 16:07:57.348] [INFO ] Import complete: 998198 spectra ready for analysis\n#&gt; [2026-02-28 16:07:57.350] [INFO ] Importing spectra from: data/interim/libraries/spectra/exp/internal_pos.rds\n#&gt; [2026-02-28 16:07:57.351] [INFO ] Loaded 1 spectra from file\n#&gt; [2026-02-28 16:07:57.354] [INFO ] Import complete: 0 spectra ready for analysis\n#&gt; [2026-02-28 16:07:57.355] [INFO ] Importing spectra from: data/interim/libraries/spectra/exp/gnps_11566051_pos.rds\n#&gt; [2026-02-28 16:08:05.222] [INFO ] Loaded 354789 spectra from file\n#&gt; [2026-02-28 16:08:05.430] [INFO ] Import complete: 354788 spectra ready for analysis\n#&gt; [2026-02-28 16:08:05.431] [INFO ] Importing spectra from: data/interim/libraries/spectra/exp/massbank_2025051_pos.rds\n#&gt; [2026-02-28 16:08:06.173] [INFO ] Loaded 66388 spectra from file\n#&gt; [2026-02-28 16:08:06.221] [INFO ] Import complete: 66388 spectra ready for analysis\n#&gt; [2026-02-28 16:08:06.222] [INFO ] Importing spectra from: data/interim/libraries/spectra/exp/merlin_13911806_pos.rds\n#&gt; [2026-02-28 16:08:08.816] [INFO ] Loaded 208280 spectra from file\n#&gt; [2026-02-28 16:08:09.394] [INFO ] Import complete: 208273 spectra ready for analysis\n#&gt; [2026-02-28 16:08:23.755] [INFO ] \n#&gt;          library spectra unique_connectivities\n#&gt;  ISDB - Wikidata  998198                998198\n#&gt;             gnps  354788                 22675\n#&gt;           merlin  208273                 26197\n#&gt;         massbank   66388                  5901\n#&gt; [2026-02-28 16:08:25.065] [INFO ] &gt; Starting: calculate_entropy_similarity [n_library=478616, n_query=3281, method=gnps]\n#&gt; [2026-02-28 16:08:25.066] [INFO ] Calculating entropy and similarity for 3281 spectra\n#&gt; [2026-02-28 16:08:47.288] [WARN ] Similarity calculation failed: LAPJV sparse: infeasible (bug)\n#&gt; [2026-02-28 16:09:04.464] [WARN ] Similarity calculation failed: LAPJV sparse: infeasible (bug)\n#&gt; [2026-02-28 16:09:11.259] [WARN ] Similarity calculation failed: LAPJV sparse: infeasible (bug)\n#&gt; [2026-02-28 16:09:20.230] [WARN ] Similarity calculation failed: LAPJV sparse: infeasible (bug)\n#&gt; [2026-02-28 16:10:11.429] [WARN ] Similarity calculation failed: LAPJV sparse: infeasible (bug)\n#&gt; [2026-02-28 16:10:23.191] [WARN ] Similarity calculation failed: LAPJV sparse: infeasible (bug)\n#&gt; [2026-02-28 16:10:33.523] [INFO ] Processed 3281 / 3281 queries\n#&gt; [2026-02-28 16:10:33.628] [INFO ] [OK] Completed: calculate_entropy_similarity [n_comparisons=910214] (2m 9s)\n#&gt; [2026-02-28 16:10:33.640] [INFO ] &gt; Starting: harmonize_adducts [n_rows=478616]\n#&gt; [2026-02-28 16:10:35.326] [INFO ] [OK] Completed: harmonize_adducts [n_unique_before=48, n_unique_after=48] (1.7s)\n#&gt; [2026-02-28 16:10:36.479] [INFO ] Here is the distribution of annotation similarity scores (0.1 bins):\n#&gt; [2026-02-28 16:10:36.481] [INFO ] \n#&gt;        bin      N\n#&gt;    [0,0.1] 181645\n#&gt;  (0.1,0.2]     91\n#&gt;  (0.2,0.3]    718\n#&gt;  (0.3,0.4]   2595\n#&gt;  (0.4,0.5]   6256\n#&gt;  (0.5,0.6]  12783\n#&gt;  (0.6,0.7]  24790\n#&gt;  (0.7,0.8]  43852\n#&gt;  (0.8,0.9]  66528\n#&gt;    (0.9,1] 225351\n#&gt; [2026-02-28 16:10:36.529] [INFO ] 296847 Candidates annotated on 3126 features (threshold &gt;= 0).\n#&gt; [2026-02-28 16:10:36.532] [INFO ] Exporting parameters to: data/interim/params/260228_161036_annotate_spectra.yaml\n#&gt; [2026-02-28 16:10:36.534] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_spectralMatches_pos.tsv.gz, n_rows=564609]\n#&gt; [2026-02-28 16:10:38.044] [INFO ] [OK] Completed: export_output [size_bytes=29648151] (1.5s)\n#&gt; ✔ ann_spe_pos completed [3m 20.1s, 29.65 MB]\n#&gt; + ann_spe_neg dispatched\n#&gt; [2026-02-28 16:10:39.830] [INFO ] ============================================================\n#&gt; [2026-02-28 16:10:39.831] [INFO ] Data Sanitizing: Pre-flight Checks\n#&gt; [2026-02-28 16:10:39.832] [INFO ] ============================================================\n#&gt; [2026-02-28 16:10:39.833] [INFO ] Checking MGF file...\n#&gt; [2026-02-28 16:10:40.956] [INFO ] [OK] MGF file: 12195 MS2 spectra found\n#&gt; [2026-02-28 16:10:40.957] [INFO ] ============================================================\n#&gt; [2026-02-28 16:10:40.958] [INFO ] [OK] All pre-flight checks passed!\n#&gt; [2026-02-28 16:10:40.959] [INFO ] Data validation complete. Ready to proceed.\n#&gt; [2026-02-28 16:10:40.960] [INFO ] ============================================================\n#&gt; [2026-02-28 16:10:40.960] [INFO ] Starting spectral annotation in neg mode\n#&gt; [2026-02-28 16:10:40.961] [INFO ] Importing spectra from: data/source/example_spectra.mgf\n#&gt; [2026-02-28 16:10:40.962] [INFO ] Reading MGF file (7.41 MB) with optimized parser: data/source/example_spectra.mgf\n#&gt; [2026-02-28 16:10:42.744] [INFO ] Processed 10000 spectra...\n#&gt; [2026-02-28 16:10:43.854] [INFO ] Total spectra read: 16282\n#&gt; [2026-02-28 16:10:49.887] [INFO ] Loaded 16282 spectra from file\n#&gt; [2026-02-28 16:10:49.898] [INFO ] Combining replicate spectra by FEATURE_ID\n#&gt; [2026-02-28 16:10:49.902] [INFO ] Combined replicates: 0 -&gt; 0 spectra\n#&gt; [2026-02-28 16:10:49.903] [WARN ] No spectra to sanitize\n#&gt; [2026-02-28 16:10:49.904] [INFO ] Import complete: 0 spectra ready for analysis\n#&gt; [2026-02-28 16:10:49.905] [WARN ] No query spectra loaded\n#&gt; [2026-02-28 16:10:49.908] [INFO ] Exporting parameters to: data/interim/params/260228_161049_annotate_spectra.yaml\n#&gt; [2026-02-28 16:10:49.909] [WARN ] Returning empty annotation template\n#&gt; [2026-02-28 16:10:49.911] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_spectralMatches_neg.tsv.gz, n_rows=1]\n#&gt; [2026-02-28 16:10:49.913] [INFO ] [OK] Completed: export_output [size_bytes=237] (2ms)\n#&gt; ✔ ann_spe_neg completed [10.4s, 237 B]\n#&gt; + lib_sop_mer dispatched\n#&gt; [2026-02-28 16:10:50.580] [INFO ] &gt; Starting: prepare_libraries_sop_merged [n_libraries=11, filter_enabled=FALSE, filter_level=none]\n#&gt; [2026-02-28 16:10:55.951] [INFO ] Splitting SOP library into standardized components\n#&gt; [2026-02-28 16:10:57.930] [INFO ] &gt; Starting: process_smiles [n_structures=1349285]\n#&gt; [2026-02-28 16:10:57.931] [INFO ] Processing SMILES with RDKit\n#&gt; [2026-02-28 16:11:04.959] [INFO ] Processing 394 new SMILES with RDKit\n#&gt; [2026-02-28 16:11:04.961] [INFO ] Starting SMILES processing pipeline\n#&gt; [2026-02-28 16:11:04.962] [INFO ] Input: /tmp/RtmpA7jyLX/file270c3d0f2764.smi\n#&gt; [2026-02-28 16:11:04.962] [INFO ] Output: /tmp/RtmpA7jyLX/file270c1ec23c5b.csv.gz\n#&gt; [2026-02-28 16:11:04.962] [INFO ] Input file validated: /tmp/RtmpA7jyLX/file270c3d0f2764.smi\n#&gt; [2026-02-28 16:11:04.962] [INFO ] Output file validated: /tmp/RtmpA7jyLX/file270c1ec23c5b.csv.gz\n#&gt; [2026-02-28 16:11:04.962] [INFO ] Processing parameters: workers=8, batch_size=1000, progress_interval=10000\n#&gt; [2026-02-28 16:11:04.962] [INFO ] SMILES supplier initialized\n#&gt; [16:11:05] Explicit valence for atom # 1 N, 3, is greater than permitted\n#&gt; [16:11:05] ERROR: Could not sanitize molecule on line 387\n#&gt; [16:11:05] ERROR: Explicit valence for atom # 1 N, 3, is greater than permitted\n#&gt; [16:11:05] Explicit valence for atom # 0 He, 1, is greater than permitted\n#&gt; [16:11:05] ERROR: Could not sanitize molecule on line 390\n#&gt; [16:11:05] ERROR: Explicit valence for atom # 0 He, 1, is greater than permitted\n#&gt; [16:11:05] Explicit valence for atom # 0 He, 1, is greater than permitted\n#&gt; [16:11:05] ERROR: Could not sanitize molecule on line 391\n#&gt; [16:11:05] ERROR: Explicit valence for atom # 0 He, 1, is greater than permitted\n#&gt; [16:11:05] Explicit valence for atom # 0 He, 1, is greater than permitted\n#&gt; [16:11:05] ERROR: Could not sanitize molecule on line 392\n#&gt; [16:11:05] ERROR: Explicit valence for atom # 0 He, 1, is greater than permitted\n#&gt; [16:11:05] Explicit valence for atom # 56 P, 7, is greater than permitted\n#&gt; [2026-02-28 16:11:05.654] [WARNING] Failed to process SMILES 'CC(C)=CCCC(C)=CCCC(C)=CCCC(C)=CCCC(C)=CCCC(C)=CCCC(C)=CCCC(C)=CCCC(C)=CCCC(C)=CCCC(C)=CCO[P-]([O])(=O)=O': Explicit valence for atom # 56 P, 7, is greater than permitted\n#&gt; [16:11:05] Explicit valence for atom # 4 P, 7, is greater than permitted\n#&gt; [2026-02-28 16:11:05.654] [WARNING] Failed to process SMILES '[H][C@](O)(CO[P-]([O])(=O)=O)C=O': Explicit valence for atom # 4 P, 7, is greater than permitted\n#&gt; [16:11:05] Explicit valence for atom # 7 Si, 6, is greater than permitted\n#&gt; [2026-02-28 16:11:05.656] [WARNING] Failed to process SMILES 'C1COCC[NH+]1C[Si-]23(OC(C(O2)(C4=CC=CC=C4)C5=CC=CC=C5)(C6=CC=CC=C6)C7=CC=CC=C7)OC(C(O3)(C8=CC=CC=C8)C9=CC=CC=C9)(C1=CC=CC=C1)C1=CC=CC=C1': Explicit valence for atom # 7 Si, 6, is greater than permitted\n#&gt; [16:11:05] Explicit valence for atom # 7 As, 7, is greater than permitted\n#&gt; [2026-02-28 16:11:05.656] [WARNING] Failed to process SMILES 'C1=CC=C2C(=C1)O[As-]34(O2)(OC5=CC=CC=C5O3)OC6=CC=CC=C6O4': Explicit valence for atom # 7 As, 7, is greater than permitted\n#&gt; [2026-02-28 16:11:05.678] [WARNING] Batch processing: 4/390 molecules failed\n#&gt; [2026-02-28 16:11:05.680] [INFO ] Processing complete. Total molecules processed: 386\n#&gt; [2026-02-28 16:11:05.729] [INFO ] Successfully processed 386 SMILES\n#&gt; [2026-02-28 16:11:09.329] [INFO ] [OK] Completed: process_smiles [n_processed=1288417] (11.4s)\n#&gt; [2026-02-28 16:11:29.193] [INFO ] Referenced structure-organism pairs (663,718)\n#&gt; [2026-02-28 16:11:31.108] [INFO ] Structures: 213,802 stereoisomers, 998,097 without stereochemistry, 1,036,493 constitutional isomers\n#&gt; [2026-02-28 16:11:47.994] [INFO ] Unique organisms (36,801)\n#&gt; [2026-02-28 16:11:48.080] [INFO ] Processing 919 organism name(s) for OTT taxonomy lookup\n#&gt; [2026-02-28 16:11:48.450] [INFO ] Querying OTT API in 10 batches\n#&gt; [2026-02-28 16:11:51.396] [INFO ] Retrieving detailed taxonomy for 13 unique OTT IDs\n#&gt; [2026-02-28 16:11:52.300] [INFO ] Got OTTaxonomy!\n#&gt; [2026-02-28 16:11:52.336] [INFO ] Exporting parameters to: data/interim/params/260228_161152_prepare_libraries_sop_merged.yaml\n#&gt; [2026-02-28 16:11:52.338] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/merged/keys.tsv.gz, n_rows=663718]\n#&gt; [2026-02-28 16:11:53.221] [INFO ] [OK] Completed: export_output [size_bytes=13544719] (883ms)\n#&gt; [2026-02-28 16:11:53.224] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/merged/organisms/taxonomies/ott.tsv.gz, n_rows=35896]\n#&gt; [2026-02-28 16:11:53.311] [INFO ] [OK] Completed: export_output [size_bytes=939193] (87ms)\n#&gt; [2026-02-28 16:11:53.313] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/merged/structures/stereo.tsv.gz, n_rows=1213350]\n#&gt; [2026-02-28 16:11:56.358] [INFO ] [OK] Completed: export_output [size_bytes=38342201] (3s)\n#&gt; [2026-02-28 16:11:56.361] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/merged/structures/metadata.tsv.gz, n_rows=1257333]\n#&gt; [2026-02-28 16:11:57.884] [INFO ] [OK] Completed: export_output [size_bytes=32534356] (1.5s)\n#&gt; [2026-02-28 16:11:57.887] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/merged/structures/names.tsv.gz, n_rows=216879]\n#&gt; [2026-02-28 16:11:58.315] [INFO ] [OK] Completed: export_output [size_bytes=7430557] (428ms)\n#&gt; [2026-02-28 16:11:58.317] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/merged/structures/taxonomies/classyfire.tsv.gz, n_rows=146393]\n#&gt; [2026-02-28 16:11:58.467] [INFO ] [OK] Completed: export_output [size_bytes=2492954] (149ms)\n#&gt; [2026-02-28 16:11:58.469] [INFO ] &gt; Starting: export_output [file=data/interim/libraries/sop/merged/structures/taxonomies/npc.tsv.gz, n_rows=141815]\n#&gt; [2026-02-28 16:11:58.763] [INFO ] [OK] Completed: export_output [size_bytes=2395030] (294ms)\n#&gt; [2026-02-28 16:11:58.765] [INFO ] [OK] Completed: prepare_libraries_sop_merged [n_pairs=663718, n_structures=1213350, n_organisms=35896, files_exported=7] (1m 8s)\n#&gt; ✔ lib_sop_mer completed [1m 8.2s, 97.68 MB]\n#&gt; + edg_spe dispatched\n#&gt; ✔ edg_spe completed [1ms, 7.58 MB]\n#&gt; + lib_mer_str_met dispatched\n#&gt; ✔ lib_mer_str_met completed [0ms, 32.53 MB]\n#&gt; + lib_mer_str_nam dispatched\n#&gt; ✔ lib_mer_str_nam completed [0ms, 7.43 MB]\n#&gt; + lib_mer_str_stereo dispatched\n#&gt; ✔ lib_mer_str_stereo completed [0ms, 38.34 MB]\n#&gt; + lib_mer_str_tax_cla dispatched\n#&gt; ✔ lib_mer_str_tax_cla completed [1ms, 2.49 MB]\n#&gt; + lib_mer_str_tax_npc dispatched\n#&gt; ✔ lib_mer_str_tax_npc completed [0ms, 2.40 MB]\n#&gt; + lib_mer_org_tax_ott dispatched\n#&gt; ✔ lib_mer_org_tax_ott completed [0ms, 939.19 kB]\n#&gt; + lib_mer_key dispatched\n#&gt; ✔ lib_mer_key completed [0ms, 13.54 MB]\n#&gt; + ann_spe_pre dispatched\n#&gt; [2026-02-28 16:12:02.569] [INFO ] Preparing spectral matching annotations from 2 file(s)\n#&gt; [2026-02-28 16:12:05.980] [INFO ] &gt; Starting: complement_metadata [n_input=564609]\n#&gt; [2026-02-28 16:12:32.078] [INFO ] [OK] Completed: complement_metadata [n_enriched=564609] (26.1s)\n#&gt; [2026-02-28 16:12:32.095] [INFO ] Exporting parameters to: data/interim/params/260228_161232_prepare_annotations_spectra.yaml\n#&gt; [2026-02-28 16:12:32.097] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_spectralMatchesPrepared.tsv.gz, n_rows=564609]\n#&gt; [2026-02-28 16:12:34.458] [INFO ] [OK] Completed: export_output [size_bytes=48968005] (2.4s)\n#&gt; ✔ ann_spe_pre completed [31.9s, 48.97 MB]\n#&gt; + ann_spe_exp_gnp_pre dispatched\n#&gt; [2026-02-28 16:12:35.418] [INFO ] &gt; Starting: prepare_annotations_gnps [n_files=1]\n#&gt; [2026-02-28 16:12:35.419] [WARN ] No GNPS annotations found, returning an empty file instead\n#&gt; [2026-02-28 16:12:35.421] [INFO ] [OK] Completed: prepare_annotations_gnps [n_annotations=1] (3ms)\n#&gt; [2026-02-28 16:12:35.439] [INFO ] Exporting parameters to: data/interim/params/260228_161235_prepare_annotations_gnps.yaml\n#&gt; [2026-02-28 16:12:35.441] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_gnpsPrepared.tsv.gz, n_rows=1]\n#&gt; [2026-02-28 16:12:35.442] [INFO ] [OK] Completed: export_output [size_bytes=237] (1ms)\n#&gt; ✔ ann_spe_exp_gnp_pre completed [34ms, 237 B]\n#&gt; + ann_spe_exp_mzm_pre dispatched\n#&gt; [2026-02-28 16:12:35.788] [INFO ] &gt; Starting: prepare_annotations_mzmine [n_files=1]\n#&gt; [2026-02-28 16:12:35.789] [WARN ] No mzmine annotations found, returning an empty file instead\n#&gt; [2026-02-28 16:12:35.791] [INFO ] [OK] Completed: prepare_annotations_mzmine [n_annotations=1] (3ms)\n#&gt; [2026-02-28 16:12:35.806] [INFO ] Exporting parameters to: data/interim/params/260228_161235_prepare_annotations_mzmine.yaml\n#&gt; [2026-02-28 16:12:35.807] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_mzminePrepared.tsv.gz, n_rows=1]\n#&gt; [2026-02-28 16:12:35.809] [INFO ] [OK] Completed: export_output [size_bytes=237] (2ms)\n#&gt; ✔ ann_spe_exp_mzm_pre completed [24ms, 237 B]\n#&gt; + ann_sir_pre dispatched\n#&gt; [2026-02-28 16:12:36.158] [INFO ] &gt; Starting: prepare_annotations_sirius [version=6]\n#&gt; [2026-02-28 16:12:36.311] [INFO ] &gt; Starting: complement_metadata [n_input=479]\n#&gt; [2026-02-28 16:12:50.172] [INFO ] [OK] Completed: complement_metadata [n_enriched=479] (13.9s)\n#&gt; [2026-02-28 16:12:50.182] [INFO ] [OK] Completed: prepare_annotations_sirius [n_canopus=14, n_formulas=14, n_structures=479] (14s)\n#&gt; [2026-02-28 16:12:50.203] [INFO ] Exporting parameters to: data/interim/params/260228_161250_prepare_annotations_sirius.yaml\n#&gt; [2026-02-28 16:12:50.204] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_canopusPrepared.tsv.gz, n_rows=14]\n#&gt; [2026-02-28 16:12:50.206] [INFO ] [OK] Completed: export_output [size_bytes=784] (2ms)\n#&gt; [2026-02-28 16:12:50.208] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_formulaPrepared.tsv.gz, n_rows=14]\n#&gt; [2026-02-28 16:12:50.209] [INFO ] [OK] Completed: export_output [size_bytes=471] (1ms)\n#&gt; [2026-02-28 16:12:50.211] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_siriusPrepared.tsv.gz, n_rows=479]\n#&gt; [2026-02-28 16:12:50.215] [INFO ] [OK] Completed: export_output [size_bytes=24161] (4ms)\n#&gt; ✔ ann_sir_pre completed [14.1s, 25.42 kB]\n#&gt; + tax_pre dispatched\n#&gt; [2026-02-28 16:12:51.330] [INFO ] &gt; Starting: prepare_taxa [taxon=NULL]\n#&gt; [2026-02-28 16:12:51.496] [INFO ] Processing 2 organism name(s) for OTT taxonomy lookup\n#&gt; [2026-02-28 16:12:51.675] [INFO ] Querying OTT API in 1 batches\n#&gt; [2026-02-28 16:12:51.814] [INFO ] Retrying failed queries using genus names only\n#&gt; [2026-02-28 16:12:51.820] [INFO ] Retrying with 1 genus names: blk \n#&gt; [2026-02-28 16:12:51.962] [INFO ] Retrieving detailed taxonomy for 1 unique OTT IDs\n#&gt; [2026-02-28 16:12:52.052] [INFO ] Got OTTaxonomy!\n#&gt; [2026-02-28 16:12:52.499] [INFO ] [OK] Completed: prepare_taxa [n_features=5328] (1.2s)\n#&gt; [2026-02-28 16:12:52.530] [INFO ] Exporting parameters to: data/interim/params/260228_161252_prepare_taxa.yaml\n#&gt; [2026-02-28 16:12:52.532] [INFO ] &gt; Starting: export_output [file=data/interim/taxa/example_taxed.tsv.gz, n_rows=5328]\n#&gt; [2026-02-28 16:12:52.539] [INFO ] [OK] Completed: export_output [size_bytes=19697] (7ms)\n#&gt; ✔ tax_pre completed [1.2s, 19.70 kB]\n#&gt; + ann_ms1_pre dispatched\n#&gt; [2026-02-28 16:12:52.933] [INFO ] &gt; Starting: annotate_masses [ms_mode=pos, tolerance_ppm=10, tolerance_rt=0.02]\n#&gt; [2026-02-28 16:12:52.934] [INFO ] Starting mass-based annotation\n#&gt; [2026-02-28 16:12:52.936] [INFO ] ============================================================\n#&gt; [2026-02-28 16:12:52.937] [INFO ] Data Sanitizing: Pre-flight Checks\n#&gt; [2026-02-28 16:12:52.938] [INFO ] ============================================================\n#&gt; [2026-02-28 16:12:52.939] [INFO ] Checking features file...\n#&gt; [2026-02-28 16:12:52.973] [INFO ] [OK] Features file: 5328 rows, 5 columns\n#&gt; [2026-02-28 16:12:52.974] [INFO ] ============================================================\n#&gt; [2026-02-28 16:12:52.975] [INFO ] [OK] All pre-flight checks passed!\n#&gt; [2026-02-28 16:12:52.976] [INFO ] Data validation complete. Ready to proceed.\n#&gt; [2026-02-28 16:12:52.978] [INFO ] ============================================================\n#&gt; [2026-02-28 16:12:53.011] [INFO ] Processing 5328 features for annotation\n#&gt; [2026-02-28 16:13:07.938] [INFO ] Already 2112 adducts previously detected\n#&gt; [2026-02-28 16:13:07.939] [INFO ] &gt; Starting: harmonize_adducts [n_rows=5328]\n#&gt; [2026-02-28 16:13:07.949] [INFO ] [OK] Completed: harmonize_adducts [n_unique_before=13, n_unique_after=13] (10ms)\n#&gt; [2026-02-28 16:13:08.009] [INFO ] Here are the top 10 observed m/z differences inside the RT windows:\n#&gt; [2026-02-28 16:13:08.011] [INFO ] \n#&gt;              bin   N\n#&gt;  (4.8501,5.0366] 352\n#&gt;  (21.822,22.009] 283\n#&gt;   (16.973,17.16] 208\n#&gt;  (17.906,18.092] 192\n#&gt;  (15.854,16.041] 172\n#&gt;    (39.914,40.1] 143\n#&gt;  (38.981,39.168] 137\n#&gt;  (34.878,35.065] 115\n#&gt;  (77.962,78.148] 114\n#&gt;  (1.8659,2.0524] 108\n#&gt; [2026-02-28 16:13:08.012] [INFO ] These differences may help identify potential preprocessing issues\n#&gt; [2026-02-28 16:13:10.973] [WARN ] Some adducts were unproperly detected, defaulting to (de)protonated\n#&gt; [2026-02-28 16:13:51.570] [INFO ] &gt; Starting: decorate_masses [n_annotations=173959]\n#&gt; [2026-02-28 16:13:51.605] [INFO ] MS1 annotations: 42309 unique structures across 3992 features\n#&gt; [2026-02-28 16:13:51.607] [INFO ] [OK] Completed: decorate_masses [n_structures=42309, n_features=3992] (37ms)\n#&gt; [2026-02-28 16:13:51.657] [INFO ] Exporting parameters to: data/interim/params/260228_161351_annotate_masses.yaml\n#&gt; [2026-02-28 16:13:51.659] [INFO ] &gt; Starting: export_output [file=data/interim/features/example_edgesMasses.tsv, n_rows=2653]\n#&gt; [2026-02-28 16:13:51.660] [INFO ] [OK] Completed: export_output [size_bytes=81706] (2ms)\n#&gt; [2026-02-28 16:13:51.662] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_ms1Prepared.tsv.gz, n_rows=173959]\n#&gt; [2026-02-28 16:13:52.354] [INFO ] [OK] Completed: export_output [size_bytes=10225321] (692ms)\n#&gt; [2026-02-28 16:13:52.355] [INFO ] [OK] Completed: annotate_masses [n_annotations=173959, n_edges=2653] (59.4s)\n#&gt; ✔ ann_ms1_pre completed [59.4s, 10.31 MB]\n#&gt; + ann_sir_pre_can dispatched\n#&gt; ✔ ann_sir_pre_can completed [0ms, 784 B]\n#&gt; + ann_sir_pre_for dispatched\n#&gt; ✔ ann_sir_pre_for completed [0ms, 471 B]\n#&gt; + ann_sir_pre_str dispatched\n#&gt; ✔ ann_sir_pre_str completed [0ms, 24.16 kB]\n#&gt; + ann_ms1_pre_edg dispatched\n#&gt; ✔ ann_ms1_pre_edg completed [0ms, 81.71 kB]\n#&gt; + ann_ms1_pre_ann dispatched\n#&gt; ✔ ann_ms1_pre_ann completed [1ms, 10.23 MB]\n#&gt; + fea_edg_pre dispatched\n#&gt; [2026-02-28 16:13:54.800] [INFO ] &gt; Starting: prepare_features_edges [n_edge_types=2]\n#&gt; [2026-02-28 16:13:54.906] [INFO ] [OK] Completed: prepare_features_edges [n_edges=153901] (106ms)\n#&gt; [2026-02-28 16:13:54.926] [INFO ] Exporting parameters to: data/interim/params/260228_161354_prepare_features_edges.yaml\n#&gt; [2026-02-28 16:13:54.928] [INFO ] &gt; Starting: export_output [file=data/interim/features/example_edges.tsv, n_rows=153901]\n#&gt; [2026-02-28 16:13:54.943] [INFO ] [OK] Completed: export_output [size_bytes=7925912] (16ms)\n#&gt; ✔ fea_edg_pre completed [145ms, 7.93 MB]\n#&gt; + ann_fil dispatched\n#&gt; [2026-02-28 16:13:55.327] [INFO ] &gt; Starting: filter_annotations [n_annotation_files=5, tolerance_rt=Inf]\n#&gt; [2026-02-28 16:13:55.329] [INFO ] Filtering annotations\n#&gt; [2026-02-28 16:13:55.369] [INFO ] Processing 5328 unique features for annotation filtering\n#&gt; [2026-02-28 16:13:59.442] [INFO ] Removing MS1 annotations superseded by spectral matches\n#&gt; [2026-02-28 16:14:02.249] [INFO ] Removed 69103 redundant MS1 annotations\n#&gt; [2026-02-28 16:14:02.251] [INFO ] Total annotations before RT filtering: 669946\n#&gt; [2026-02-28 16:14:03.452] [INFO ] Filtering annotations outside Inf min RT tolerance\n#&gt; [2026-02-28 16:14:05.749] [INFO ] Removed 0 annotations based on retention time tolerance\n#&gt; [2026-02-28 16:14:05.968] [INFO ] Exporting parameters to: data/interim/params/260228_161405_filter_annotations.yaml\n#&gt; [2026-02-28 16:14:05.969] [INFO ] &gt; Starting: export_output [file=data/interim/annotations/example_annotationsFiltered.tsv.gz, n_rows=670599]\n#&gt; [2026-02-28 16:14:08.571] [INFO ] [OK] Completed: export_output [size_bytes=49168011] (2.6s)\n#&gt; [2026-02-28 16:14:08.572] [INFO ] [OK] Completed: filter_annotations [n_filtered=670599] (13.2s)\n#&gt; ✔ ann_fil completed [13.2s, 49.17 MB]\n#&gt; + fea_com dispatched\n#&gt; [2026-02-28 16:14:09.197] [INFO ] &gt; Starting: create_components [n_input_files=1]\n#&gt; [2026-02-28 16:14:09.199] [INFO ] Creating components from 1 edge file(s)\n#&gt; [2026-02-28 16:14:09.274] [INFO ] Loaded 152576 edges connecting 4537 unique features\n#&gt; [2026-02-28 16:14:09.298] [INFO ] Found 986 components\n#&gt; [2026-02-28 16:14:09.308] [INFO ] Component sizes - Min: 1, Max: 3050, Mean: 4.6\n#&gt; [2026-02-28 16:14:09.323] [INFO ] Exporting parameters to: data/interim/params/260228_161409_create_components.yaml\n#&gt; [2026-02-28 16:14:09.325] [INFO ] &gt; Starting: export_output [file=data/interim/features/example_components.tsv, n_rows=4537]\n#&gt; [2026-02-28 16:14:09.326] [INFO ] [OK] Completed: export_output [size_bytes=36341] (1ms)\n#&gt; [2026-02-28 16:14:09.327] [INFO ] Components written to: data/interim/features/example_components.tsv\n#&gt; [2026-02-28 16:14:09.328] [INFO ] [OK] Completed: create_components [n_components=986, n_features=4537] (131ms)\n#&gt; ✔ fea_com completed [135ms, 36.34 kB]\n#&gt; + int_com dispatched\n#&gt; ✔ int_com completed [0ms, 36.34 kB]\n#&gt; + fea_com_pre dispatched\n#&gt; [2026-02-28 16:14:10.048] [INFO ] &gt; Starting: prepare_features_components [n_files=1]\n#&gt; [2026-02-28 16:14:10.054] [INFO ] [OK] Completed: prepare_features_components [n_assignments=4537] (5ms)\n#&gt; [2026-02-28 16:14:10.070] [INFO ] Exporting parameters to: data/interim/params/260228_161410_prepare_features_components.yaml\n#&gt; [2026-02-28 16:14:10.071] [INFO ] &gt; Starting: export_output [file=data/interim/features/example_componentsPrepared.tsv, n_rows=4537]\n#&gt; [2026-02-28 16:14:10.073] [INFO ] [OK] Completed: export_output [size_bytes=36336] (2ms)\n#&gt; ✔ fea_com_pre completed [26ms, 36.34 kB]\n#&gt; + ann_wei dispatched\n#&gt; [2026-02-28 16:14:10.412] [INFO ] Starting annotation weighting and scoring\n#&gt; [2026-02-28 16:14:10.413] [INFO ] &gt; Starting: weight_annotations [n_candidates_neighbors=16, n_candidates_final=1]\n#&gt; [2026-02-28 16:14:26.090] [INFO ] \n#&gt;  candidate_library      n\n#&gt;    ISDB - Wikidata 519661\n#&gt;           TIMA MS1  79033\n#&gt;               gnps  21563\n#&gt;             merlin  20091\n#&gt;           massbank   3195\n#&gt;             SIRIUS    479\n#&gt; [2026-02-28 16:14:32.789] [INFO ] &gt; Starting: weight_bio [n_annotations=629114, n_sop=664935]\n#&gt; [2026-02-28 16:14:32.791] [INFO ] Weighting 629114 annotations by biological source\n#&gt; [2026-02-28 16:14:36.056] [INFO ] [OK] Completed: weight_bio [n_weighted=629114] (3.3s)\n#&gt; [2026-02-28 16:14:36.058] [INFO ] &gt; Starting: decorate_bio [n_annotations=629114]\n#&gt; [2026-02-28 16:14:36.729] [INFO ] Taxonomically informed metabolite annotation reranked:\n#&gt;     Kingdom level: 39425 structures\n#&gt;     Phylum level:  38980 structures\n#&gt;     Class level:   33432 structures\n#&gt;     Order level:   9156 structures\n#&gt;     Family level:  7372 structures\n#&gt;     Tribe level:   1239 structures\n#&gt;     Genus level:   979 structures\n#&gt;     Species level: 471 structures\n#&gt;     Variety level: 91 structures\n#&gt;     Biota level:   91 structures\n#&gt; [2026-02-28 16:14:36.731] [INFO ] [OK] Completed: decorate_bio [n_processed=629114] (673ms)\n#&gt; [2026-02-28 16:14:36.732] [INFO ] &gt; Starting: clean_bio [n_annotations=629114, minimal_consistency=0]\n#&gt; [2026-02-28 16:15:21.312] [INFO ] [OK] Completed: clean_bio [n_cleaned=629112] (44.6s)\n#&gt; [2026-02-28 16:15:21.314] [INFO ] &gt; Starting: weight_chemo [n_input=629112]\n#&gt; [2026-02-28 16:15:21.315] [INFO ] Weighting 629112 annotations by chemical consistency\n#&gt; [2026-02-28 16:15:22.551] [INFO ] [OK] Completed: weight_chemo [n_weighted=629112] (1.2s)\n#&gt; [2026-02-28 16:15:22.553] [INFO ] &gt; Starting: decorate_chemo [n_annotations=629112]\n#&gt; [2026-02-28 16:15:23.399] [INFO ] Chemically informed metabolite annotation reranked:\n#&gt;   Classyfire:\n#&gt;     Kingdom level:    38006 structures\n#&gt;     Superclass level: 37758 structures\n#&gt;     Class level:      33316 structures\n#&gt;     Parent level:     25109 structures\n#&gt;   NPClassifier:\n#&gt;     Pathway level:    38152 structures\n#&gt;     Superclass level: 36245 structures\n#&gt;     Class level:      25214 structures\n#&gt; [2026-02-28 16:15:23.401] [INFO ] [OK] Completed: decorate_chemo [n_processed=629112] (848ms)\n#&gt; [2026-02-28 16:15:23.403] [INFO ] &gt; Starting: clean_chemo [n_annotations=629112, candidates_final=1, high_confidence=FALSE]\n#&gt; [2026-02-28 16:15:32.596] [INFO ] Sampling candidates for 3197 features with more than 7 candidates per score\n#&gt; [2026-02-28 16:15:32.681] [INFO ] &gt; Starting: filter_high_confidence [n_input=24506, context=filtered]\n#&gt; [2026-02-28 16:15:32.720] [INFO ] [filtered]  Removed 18309 low-confidence candidates (74.7% of 24506 total)\n#&gt; [2026-02-28 16:15:32.721] [INFO ] [filtered]  6197 high-confidence candidates remaining (25.3%)\n#&gt; [2026-02-28 16:15:32.722] [INFO ] [OK] Completed: filter_high_confidence [n_filtered=6197, n_removed=18309] (41ms)\n#&gt; [2026-02-28 16:15:32.725] [INFO ] Summarizing annotation results\n#&gt; [2026-02-28 16:15:38.257] [INFO ] Annotated features: 1004/5328 (18.8%)\n#&gt; [2026-02-28 16:15:38.365] [INFO ] Summarizing annotation results\n#&gt; [2026-02-28 16:15:55.247] [INFO ] Annotated features: 4673/5328 (87.7%)\n#&gt; [2026-02-28 16:15:55.723] [INFO ] [OK] Completed: clean_chemo [n_final_full=351116, n_final_filtered=5793, n_final_mini=5793, n_features=5328] (32.3s)\n#&gt; [2026-02-28 16:15:55.724] [INFO ] [OK] Completed: weight_annotations [n_annotations=NULL] (1m 45s)\n#&gt; [2026-02-28 16:15:55.746] [INFO ] Exporting parameters to: data/processed/20260228_161555_example/260228_161555_prepare_params.yaml\n#&gt; [2026-02-28 16:15:55.767] [INFO ] Exporting parameters to: data/processed/20260228_161555_example/260228_161555_prepare_params_advanced.yaml\n#&gt; [2026-02-28 16:15:55.769] [INFO ] &gt; Starting: export_output [file=data/processed/20260228_161555_example/example_results_mini.tsv, n_rows=5793]\n#&gt; [2026-02-28 16:15:55.773] [INFO ] [OK] Completed: export_output [size_bytes=895860] (3ms)\n#&gt; [2026-02-28 16:15:55.774] [INFO ] &gt; Starting: export_output [file=data/processed/20260228_161555_example/example_results_filtered.tsv, n_rows=5793]\n#&gt; [2026-02-28 16:15:55.780] [INFO ] [OK] Completed: export_output [size_bytes=2017646] (6ms)\n#&gt; [2026-02-28 16:15:55.782] [INFO ] &gt; Starting: export_output [file=data/processed/20260228_161555_example/example_results.tsv, n_rows=351116]\n#&gt; [2026-02-28 16:15:56.293] [INFO ] [OK] Completed: export_output [size_bytes=233729115] (511ms)\n#&gt; [2026-02-28 16:15:56.294] [INFO ] Results exported: example_results.tsv\n#&gt; ✔ ann_wei completed [1m 45.9s, 235.75 MB]\n#&gt; ✔ ended pipeline [16m 2.5s, 134 completed, 0 skipped]\n#&gt; There were 14 warnings (use warnings() to see them)\n\nThe final exported file is formatted in order to be easily imported in Cytoscape to further explore your data!\nWe hope you enjoyed using TIMA and are pleased to hear from you!\nFor any remark or suggestion, please fill an issue or feel free to contact us directly.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{rutz2026,\n  author = {Rutz, Adriano},\n  title = {3 {Performing} {Taxonomically} {Informed} {Metabolite}\n    {Annotation}},\n  date = {2026-02-28},\n  url = {https://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/III-processing.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRutz, Adriano. 2026. “3 Performing Taxonomically Informed\nMetabolite Annotation.” February 28, 2026. https://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/III-processing.html."
  },
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite tima in publications use:\n\nRutz A, Allard P (2025). tima: Taxonomically Informed Metabolite Annotation. doi:10.5281/zenodo.5797920, R package, version 2.11.0.\n\n\nRutz A, Dounoue-Kubo M, Ollivier S, Bisson J, Bagheri M, Saesong T, Ebrahimi S, Ingkaninan K, Wolfender J, Allard P (2019). “Taxonomically Informed Scoring Enhances Confidence in Natural Products Annotation.” Frontiers in Plant Science, 10. ISSN 1664-462X, doi:10.3389/FPLS.2019.01329, https://doi.org/10.3389/fpls.2019.01329."
  },
  {
    "objectID": "man/create_edges.html",
    "href": "man/create_edges.html",
    "title": "tima",
    "section": "",
    "text": "Calculates pairwise spectral similarity between all spectra to create a network edge list. Uses parallel processing via purrr for efficiency.\n\n\n\ncreate_edges(\n  frags,\n  nspecs,\n  precs,\n  method,\n  ms2_tolerance,\n  ppm_tolerance,\n  threshold,\n  matched_peaks\n)\n\n\n\n\n\n\n\nfrags\n\n\nList of aligned fragment spectra matrices\n\n\n\n\nnspecs\n\n\nInteger number of spectra\n\n\n\n\nprecs\n\n\nNumeric vector of precursor m/z values\n\n\n\n\nmethod\n\n\nSimilarity method (\"entropy\", \"gnps\", or \"cosine\")\n\n\n\n\nms2_tolerance\n\n\nMS2 tolerance in Daltons\n\n\n\n\nppm_tolerance\n\n\nPPM tolerance\n\n\n\n\nthreshold\n\n\nMinimum similarity score threshold\n\n\n\n\nmatched_peaks\n\n\nMinimum number of matched peaks required\n\n\n\n\n\n\nData frame with columns: feature_id, target_id, score, matched_peaks. Returns empty data frame with NA values if no edges pass thresholds.\n\n\n\n\nlibrary(\"tima\")\n\n# Create network edges from spectra\nedges &lt;- create_edges(\n  frags = fragment_list,\n  nspecs = length(fragment_list),\n  precs = precursor_mz,\n  method = \"cosine\",\n  ms2_tolerance = 0.02,\n  ppm_tolerance = 10,\n  threshold = 0.7,\n  matched_peaks = 6\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Create",
      "create_edges"
    ]
  },
  {
    "objectID": "man/create_edges.html#create-spectral-similarity-network-edges",
    "href": "man/create_edges.html#create-spectral-similarity-network-edges",
    "title": "tima",
    "section": "",
    "text": "Calculates pairwise spectral similarity between all spectra to create a network edge list. Uses parallel processing via purrr for efficiency.\n\n\n\ncreate_edges(\n  frags,\n  nspecs,\n  precs,\n  method,\n  ms2_tolerance,\n  ppm_tolerance,\n  threshold,\n  matched_peaks\n)\n\n\n\n\n\n\n\nfrags\n\n\nList of aligned fragment spectra matrices\n\n\n\n\nnspecs\n\n\nInteger number of spectra\n\n\n\n\nprecs\n\n\nNumeric vector of precursor m/z values\n\n\n\n\nmethod\n\n\nSimilarity method (\"entropy\", \"gnps\", or \"cosine\")\n\n\n\n\nms2_tolerance\n\n\nMS2 tolerance in Daltons\n\n\n\n\nppm_tolerance\n\n\nPPM tolerance\n\n\n\n\nthreshold\n\n\nMinimum similarity score threshold\n\n\n\n\nmatched_peaks\n\n\nMinimum number of matched peaks required\n\n\n\n\n\n\nData frame with columns: feature_id, target_id, score, matched_peaks. Returns empty data frame with NA values if no edges pass thresholds.\n\n\n\n\nlibrary(\"tima\")\n\n# Create network edges from spectra\nedges &lt;- create_edges(\n  frags = fragment_list,\n  nspecs = length(fragment_list),\n  precs = precursor_mz,\n  method = \"cosine\",\n  ms2_tolerance = 0.02,\n  ppm_tolerance = 10,\n  threshold = 0.7,\n  matched_peaks = 6\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Create",
      "create_edges"
    ]
  },
  {
    "objectID": "man/prepare_annotations_sirius.html",
    "href": "man/prepare_annotations_sirius.html",
    "title": "tima",
    "section": "",
    "text": "Prepares SIRIUS annotation results (structure predictions, CANOPUS chemical classifications, and formula predictions) by harmonizing formats across SIRIUS versions (v5/v6), standardizing column names, and integrating with structure metadata.\n\n\n\nprepare_annotations_sirius(\n  input_directory = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$raw\\$sirius,\n  output_ann = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$structural\\$sirius,\n  output_can = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$canopus,\n  output_for = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$formula,\n  sirius_version = get_params(step = \"prepare_annotations_sirius\")\\$tools\\$sirius\\$version,\n  str_stereo = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput_directory\n\n\nCharacter path to directory or zip file containing SIRIUS results.\n\n\n\n\noutput_ann\n\n\nCharacter path for prepared structure annotation output.\n\n\n\n\noutput_can\n\n\nCharacter path for prepared CANOPUS output.\n\n\n\n\noutput_for\n\n\nCharacter path for prepared formula output.\n\n\n\n\nsirius_version\n\n\nCharacter SIRIUS version (\"5\" or \"6\").\n\n\n\n\nstr_stereo\n\n\nCharacter path to structure stereochemistry file.\n\n\n\n\nstr_met\n\n\nCharacter path to structure metadata file.\n\n\n\n\nstr_nam\n\n\nCharacter path to structure names file.\n\n\n\n\nstr_tax_cla\n\n\nCharacter path to ClassyFire taxonomy file.\n\n\n\n\nstr_tax_npc\n\n\nCharacter path to NPClassifier taxonomy file.\n\n\n\n\n\n\nThis function:\n\n\nValidates inputs (version, paths, file existence).\n\n\nLoads SIRIUS output files (CANOPUS, formulas, structures, denovo).\n\n\nHarmonizes column names across SIRIUS v5 and v6.\n\n\nJoins with structure metadata (stereochemistry, names, taxonomy).\n\n\nSplits results into three output files: annotations, CANOPUS, formulas.\n\n\nExports parameters and results.\n\n\nIf the input directory does not exist, returns an empty template with expected columns to ensure downstream compatibility.\n\n\n\nCharacter path to the prepared SIRIUS annotations file (invisible).\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_sirius()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_sirius"
    ]
  },
  {
    "objectID": "man/prepare_annotations_sirius.html#prepare-annotations-sirius",
    "href": "man/prepare_annotations_sirius.html#prepare-annotations-sirius",
    "title": "tima",
    "section": "",
    "text": "Prepares SIRIUS annotation results (structure predictions, CANOPUS chemical classifications, and formula predictions) by harmonizing formats across SIRIUS versions (v5/v6), standardizing column names, and integrating with structure metadata.\n\n\n\nprepare_annotations_sirius(\n  input_directory = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$raw\\$sirius,\n  output_ann = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$structural\\$sirius,\n  output_can = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$canopus,\n  output_for = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$annotations\\$prepared\\$formula,\n  sirius_version = get_params(step = \"prepare_annotations_sirius\")\\$tools\\$sirius\\$version,\n  str_stereo = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_sirius\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput_directory\n\n\nCharacter path to directory or zip file containing SIRIUS results.\n\n\n\n\noutput_ann\n\n\nCharacter path for prepared structure annotation output.\n\n\n\n\noutput_can\n\n\nCharacter path for prepared CANOPUS output.\n\n\n\n\noutput_for\n\n\nCharacter path for prepared formula output.\n\n\n\n\nsirius_version\n\n\nCharacter SIRIUS version (\"5\" or \"6\").\n\n\n\n\nstr_stereo\n\n\nCharacter path to structure stereochemistry file.\n\n\n\n\nstr_met\n\n\nCharacter path to structure metadata file.\n\n\n\n\nstr_nam\n\n\nCharacter path to structure names file.\n\n\n\n\nstr_tax_cla\n\n\nCharacter path to ClassyFire taxonomy file.\n\n\n\n\nstr_tax_npc\n\n\nCharacter path to NPClassifier taxonomy file.\n\n\n\n\n\n\nThis function:\n\n\nValidates inputs (version, paths, file existence).\n\n\nLoads SIRIUS output files (CANOPUS, formulas, structures, denovo).\n\n\nHarmonizes column names across SIRIUS v5 and v6.\n\n\nJoins with structure metadata (stereochemistry, names, taxonomy).\n\n\nSplits results into three output files: annotations, CANOPUS, formulas.\n\n\nExports parameters and results.\n\n\nIf the input directory does not exist, returns an empty template with expected columns to ensure downstream compatibility.\n\n\n\nCharacter path to the prepared SIRIUS annotations file (invisible).\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_sirius()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_sirius"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_hmdb.html",
    "href": "man/prepare_libraries_sop_hmdb.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares HMDB (Human Metabolome Database) structure-organism pairs by parsing SDF files, extracting metadata, and formatting for TIMA annotation workflows.\n\n\n\nprepare_libraries_sop_hmdb(\n  input = get_params(step = \"prepare_libraries_sop_hmdb\")\\$files\\$libraries\\$sop\\$raw\\$hmdb,\n  output = get_params(step =\n    \"prepare_libraries_sop_hmdb\")\\$files\\$libraries\\$sop\\$prepared\\$hmdb\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to HMDB SDF zip file\n\n\n\n\noutput\n\n\nCharacter string path for prepared HMDB library output\n\n\n\n\n\n\nCharacter string path to prepared HMDB structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_hmdb()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_hmdb"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_hmdb.html#prepare-libraries-of-structure-organism-pairs-hmdb",
    "href": "man/prepare_libraries_sop_hmdb.html#prepare-libraries-of-structure-organism-pairs-hmdb",
    "title": "tima",
    "section": "",
    "text": "This function prepares HMDB (Human Metabolome Database) structure-organism pairs by parsing SDF files, extracting metadata, and formatting for TIMA annotation workflows.\n\n\n\nprepare_libraries_sop_hmdb(\n  input = get_params(step = \"prepare_libraries_sop_hmdb\")\\$files\\$libraries\\$sop\\$raw\\$hmdb,\n  output = get_params(step =\n    \"prepare_libraries_sop_hmdb\")\\$files\\$libraries\\$sop\\$prepared\\$hmdb\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to HMDB SDF zip file\n\n\n\n\noutput\n\n\nCharacter string path for prepared HMDB library output\n\n\n\n\n\n\nCharacter string path to prepared HMDB structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_hmdb()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_hmdb"
    ]
  },
  {
    "objectID": "man/read_mgf_opti.html",
    "href": "man/read_mgf_opti.html",
    "title": "tima",
    "section": "",
    "text": "This function reads a Mascot Generic Format (MGF) file using a memory-efficient approach. It mimics the MsBackendMgf implementation but uses significantly lower memory, making it suitable for processing large MGF files that might otherwise cause memory issues.\n\n\n\nread_mgf_opti(\n  f,\n  msLevel = 2L,\n  mapping = Spectra::spectraVariableMapping(object = MsBackendMgf::MsBackendMgf())\n)\n\n\n\n\n\n\n\nf\n\n\nCharacter string specifying the path to a single MGF file\n\n\n\n\nmsLevel\n\n\nInteger MS level to assign to spectra (default: 2L for MS2)\n\n\n\n\nmapping\n\n\nNamed character vector mapping MGF field names to standard spectra variable names. Default uses the mapping from MsBackendMgf.\n\n\n\n\n\n\nThe function processes spectra in batches to minimize memory usage and avoid loading the entire file into memory at once. It extracts all MGF fields and maps them to standard spectra variable names.\n\n\n\nA DataFrame containing the parsed spectra data with standardized variable names\n\n\n\n\nlibrary(\"tima\")\n\n# Read MGF file with memory-efficient approach\nlibrary(Spectra)\nlibrary(MsBackendMgf)\n\nspectra_data &lt;- read_mgf_opti(\n  f = \"path/to/spectra.mgf\",\n  msLevel = 2L\n)\n\n# Create Spectra object from result\nsps &lt;- Spectra(spectra_data)"
  },
  {
    "objectID": "man/read_mgf_opti.html#read-mgf-opti",
    "href": "man/read_mgf_opti.html#read-mgf-opti",
    "title": "tima",
    "section": "",
    "text": "This function reads a Mascot Generic Format (MGF) file using a memory-efficient approach. It mimics the MsBackendMgf implementation but uses significantly lower memory, making it suitable for processing large MGF files that might otherwise cause memory issues.\n\n\n\nread_mgf_opti(\n  f,\n  msLevel = 2L,\n  mapping = Spectra::spectraVariableMapping(object = MsBackendMgf::MsBackendMgf())\n)\n\n\n\n\n\n\n\nf\n\n\nCharacter string specifying the path to a single MGF file\n\n\n\n\nmsLevel\n\n\nInteger MS level to assign to spectra (default: 2L for MS2)\n\n\n\n\nmapping\n\n\nNamed character vector mapping MGF field names to standard spectra variable names. Default uses the mapping from MsBackendMgf.\n\n\n\n\n\n\nThe function processes spectra in batches to minimize memory usage and avoid loading the entire file into memory at once. It extracts all MGF fields and maps them to standard spectra variable names.\n\n\n\nA DataFrame containing the parsed spectra data with standardized variable names\n\n\n\n\nlibrary(\"tima\")\n\n# Read MGF file with memory-efficient approach\nlibrary(Spectra)\nlibrary(MsBackendMgf)\n\nspectra_data &lt;- read_mgf_opti(\n  f = \"path/to/spectra.mgf\",\n  msLevel = 2L\n)\n\n# Create Spectra object from result\nsps &lt;- Spectra(spectra_data)"
  },
  {
    "objectID": "man/clean_chemo.html",
    "href": "man/clean_chemo.html",
    "title": "tima",
    "section": "",
    "text": "Cleans and filters chemically weighted annotation results through a multi-tier pipeline. Applies MS1 score thresholds, percentile filtering, ranking, and optional high-confidence filtering. Returns three-tier output: full (comprehensive), filtered (top candidates), and mini (one row per feature).\n\n\n\nclean_chemo(\n  annot_table_wei_chemo,\n  components_table,\n  features_table,\n  structure_organism_pairs_table,\n  candidates_final,\n  best_percentile,\n  minimal_ms1_bio,\n  minimal_ms1_chemo,\n  minimal_ms1_condition,\n  compounds_names,\n  high_confidence,\n  remove_ties,\n  summarize,\n  score_chemical_cla_kingdom = 0.1,\n  score_chemical_cla_superclass = 0.2,\n  score_chemical_cla_class = 0.3,\n  score_chemical_cla_parent = 0.4,\n  score_chemical_npc_pathway = 0.3,\n  score_chemical_npc_superclass = 0.2,\n  score_chemical_npc_class = 0.1,\n  max_per_score = 7L\n)\n\n\n\n\n\n\n\nannot_table_wei_chemo\n\n\nData frame with chemically weighted annotations. Required columns: feature_id, candidate_structure_inchikey_connectivity_layer, score_weighted_chemo, score_biological, score_chemical, candidate_score_pseudo_initial\n\n\n\n\ncomponents_table\n\n\nData frame with molecular network component assignments. Required columns: feature_id, component_id\n\n\n\n\nfeatures_table\n\n\nData frame with feature metadata (RT, m/z, etc.). Required columns: feature_id\n\n\n\n\nstructure_organism_pairs_table\n\n\nData frame linking structures to organisms. Required columns: structure_inchikey_connectivity_layer\n\n\n\n\ncandidates_final\n\n\nInteger, number of top candidates to retain per feature (&gt;= 1)\n\n\n\n\nbest_percentile\n\n\nNumeric (0-1), percentile threshold for score filtering. Candidates with scores &gt;= percentile * max_score are kept. Default: 0.9 (90th percentile)\n\n\n\n\nminimal_ms1_bio\n\n\nNumeric (0-1), minimum biological score for MS1-only annotations\n\n\n\n\nminimal_ms1_chemo\n\n\nNumeric (0-1), minimum chemical score for MS1-only annotations\n\n\n\n\nminimal_ms1_condition\n\n\nCharacter, logical operator for MS1 filtering: \"OR\" or \"AND\". \"OR\" = keep if bio &gt;= threshold OR chem &gt;= threshold. \"AND\" = keep if bio &gt;= threshold AND chem &gt;= threshold\n\n\n\n\ncompounds_names\n\n\nLogical, include compound names in output (may increase size)\n\n\n\n\nhigh_confidence\n\n\nLogical, apply strict high-confidence filters\n\n\n\n\nremove_ties\n\n\nLogical, remove tied scores (keep only highest-ranked)\n\n\n\n\nsummarize\n\n\nLogical, collapse results to one row per feature\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nNumeric (0-1), score for ClassyFire kingdom level\n\n\n\n\nscore_chemical_cla_superclass\n\n\nNumeric (0-1), score for ClassyFire superclass level\n\n\n\n\nscore_chemical_cla_class\n\n\nNumeric (0-1), score for ClassyFire class level\n\n\n\n\nscore_chemical_cla_parent\n\n\nNumeric (0-1), score for ClassyFire direct parent level\n\n\n\n\nscore_chemical_npc_pathway\n\n\nNumeric (0-1), score for NPClassifier pathway level\n\n\n\n\nscore_chemical_npc_superclass\n\n\nNumeric (0-1), score for NPClassifier superclass level\n\n\n\n\nscore_chemical_npc_class\n\n\nNumeric (0-1), score for NPClassifier class level\n\n\n\n\nmax_per_score\n\n\nInteger, max candidates to keep per feature per score. If more exist, they are randomly sampled and a note is added. Default 7.\n\n\n\n\n\n\nNamed list with three data frames:\n\n\nfull\n\n\nAll annotations (optionally high-confidence filtered)\n\n\nfiltered\n\n\nTop candidates meeting percentile + rank thresholds\n\n\nmini\n\n\nOne row per feature with best compound/taxonomy\n\n\n\n\n\nweight_chemo, filter_high_confidence_only, summarize_results\n\n\n\n\nlibrary(\"tima\")\n\nresults &lt;- clean_chemo(\n  annot_table_wei_chemo = annotations,\n  features_table = features,\n  components_table = components,\n  structure_organism_pairs_table = sop_table,\n  candidates_final = 10,\n  best_percentile = 0.9,\n  minimal_ms1_bio = 0.5,\n  minimal_ms1_chemo = 0.5,\n  minimal_ms1_condition = \"OR\",\n  compounds_names = TRUE,\n  high_confidence = FALSE,\n  remove_ties = FALSE,\n  summarize = FALSE\n)"
  },
  {
    "objectID": "man/clean_chemo.html#clean-chemical-annotations",
    "href": "man/clean_chemo.html#clean-chemical-annotations",
    "title": "tima",
    "section": "",
    "text": "Cleans and filters chemically weighted annotation results through a multi-tier pipeline. Applies MS1 score thresholds, percentile filtering, ranking, and optional high-confidence filtering. Returns three-tier output: full (comprehensive), filtered (top candidates), and mini (one row per feature).\n\n\n\nclean_chemo(\n  annot_table_wei_chemo,\n  components_table,\n  features_table,\n  structure_organism_pairs_table,\n  candidates_final,\n  best_percentile,\n  minimal_ms1_bio,\n  minimal_ms1_chemo,\n  minimal_ms1_condition,\n  compounds_names,\n  high_confidence,\n  remove_ties,\n  summarize,\n  score_chemical_cla_kingdom = 0.1,\n  score_chemical_cla_superclass = 0.2,\n  score_chemical_cla_class = 0.3,\n  score_chemical_cla_parent = 0.4,\n  score_chemical_npc_pathway = 0.3,\n  score_chemical_npc_superclass = 0.2,\n  score_chemical_npc_class = 0.1,\n  max_per_score = 7L\n)\n\n\n\n\n\n\n\nannot_table_wei_chemo\n\n\nData frame with chemically weighted annotations. Required columns: feature_id, candidate_structure_inchikey_connectivity_layer, score_weighted_chemo, score_biological, score_chemical, candidate_score_pseudo_initial\n\n\n\n\ncomponents_table\n\n\nData frame with molecular network component assignments. Required columns: feature_id, component_id\n\n\n\n\nfeatures_table\n\n\nData frame with feature metadata (RT, m/z, etc.). Required columns: feature_id\n\n\n\n\nstructure_organism_pairs_table\n\n\nData frame linking structures to organisms. Required columns: structure_inchikey_connectivity_layer\n\n\n\n\ncandidates_final\n\n\nInteger, number of top candidates to retain per feature (&gt;= 1)\n\n\n\n\nbest_percentile\n\n\nNumeric (0-1), percentile threshold for score filtering. Candidates with scores &gt;= percentile * max_score are kept. Default: 0.9 (90th percentile)\n\n\n\n\nminimal_ms1_bio\n\n\nNumeric (0-1), minimum biological score for MS1-only annotations\n\n\n\n\nminimal_ms1_chemo\n\n\nNumeric (0-1), minimum chemical score for MS1-only annotations\n\n\n\n\nminimal_ms1_condition\n\n\nCharacter, logical operator for MS1 filtering: \"OR\" or \"AND\". \"OR\" = keep if bio &gt;= threshold OR chem &gt;= threshold. \"AND\" = keep if bio &gt;= threshold AND chem &gt;= threshold\n\n\n\n\ncompounds_names\n\n\nLogical, include compound names in output (may increase size)\n\n\n\n\nhigh_confidence\n\n\nLogical, apply strict high-confidence filters\n\n\n\n\nremove_ties\n\n\nLogical, remove tied scores (keep only highest-ranked)\n\n\n\n\nsummarize\n\n\nLogical, collapse results to one row per feature\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nNumeric (0-1), score for ClassyFire kingdom level\n\n\n\n\nscore_chemical_cla_superclass\n\n\nNumeric (0-1), score for ClassyFire superclass level\n\n\n\n\nscore_chemical_cla_class\n\n\nNumeric (0-1), score for ClassyFire class level\n\n\n\n\nscore_chemical_cla_parent\n\n\nNumeric (0-1), score for ClassyFire direct parent level\n\n\n\n\nscore_chemical_npc_pathway\n\n\nNumeric (0-1), score for NPClassifier pathway level\n\n\n\n\nscore_chemical_npc_superclass\n\n\nNumeric (0-1), score for NPClassifier superclass level\n\n\n\n\nscore_chemical_npc_class\n\n\nNumeric (0-1), score for NPClassifier class level\n\n\n\n\nmax_per_score\n\n\nInteger, max candidates to keep per feature per score. If more exist, they are randomly sampled and a note is added. Default 7.\n\n\n\n\n\n\nNamed list with three data frames:\n\n\nfull\n\n\nAll annotations (optionally high-confidence filtered)\n\n\nfiltered\n\n\nTop candidates meeting percentile + rank thresholds\n\n\nmini\n\n\nOne row per feature with best compound/taxonomy\n\n\n\n\n\nweight_chemo, filter_high_confidence_only, summarize_results\n\n\n\n\nlibrary(\"tima\")\n\nresults &lt;- clean_chemo(\n  annot_table_wei_chemo = annotations,\n  features_table = features,\n  components_table = components,\n  structure_organism_pairs_table = sop_table,\n  candidates_final = 10,\n  best_percentile = 0.9,\n  minimal_ms1_bio = 0.5,\n  minimal_ms1_chemo = 0.5,\n  minimal_ms1_condition = \"OR\",\n  compounds_names = TRUE,\n  high_confidence = FALSE,\n  remove_ties = FALSE,\n  summarize = FALSE\n)"
  },
  {
    "objectID": "man/parse_adduct.html",
    "href": "man/parse_adduct.html",
    "title": "tima",
    "section": "",
    "text": "This function parses mass spectrometry adduct notation strings into their components: multimer count, isotope shift, modifications, charge state, and charge sign. It handles complex adducts with multiple additions/losses.\n\n\n\nparse_adduct(adduct_string, regex = ADDUCT_REGEX_PATTERN)\n\n\n\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct in standard notation (e.g., \"[M+H]+\", \"[2M+Na]+\", \"[M-H2O+H]+\")\n\n\n\n\nregex\n\n\nCharacter string regular expression pattern for parsing (default: uses ADDUCT_REGEX_PATTERN from constants)\n\n\n\n\n\n\nNamed numeric vector containing:\n\n\n\nn_mer\n\n\nInteger number of monomers (e.g., 2 for dimer, 1 for monomer)\n\n\n\n\nn_iso\n\n\nInteger isotope shift (e.g., 1 for M+1 isotopologue, 0 for monoisotopic)\n\n\n\n\nlos_add_clu\n\n\nNumeric total mass change in Daltons from all modifications\n\n\n\n\nn_charges\n\n\nInteger absolute number of charges (always positive)\n\n\n\n\ncharge\n\n\nInteger charge polarity (+1 for positive mode, -1 for negative mode)\n\n\n\nReturns all zeros if parsing fails.\n\n\n\n\nlibrary(\"tima\")\n\n# Simple adducts\nparse_adduct(\"[M+H]+\") # Protonated molecule\nparse_adduct(\"[M-H]-\") # Deprotonated molecule\nparse_adduct(\"[M+Na]+\") # Sodium adduct\n\n# Complex adducts\nparse_adduct(\"[2M+Na]+\") # Dimer with sodium\nparse_adduct(\"[M+H-H2O]+\") # Protonated with water loss\nparse_adduct(\"[M1+H]+\") # M+1 isotopologue\nparse_adduct(\"[2M1-C6H12O6 (hexose)+NaCl+H]2+\") # Complex modification\n\n# Error cases\nparse_adduct(NULL) # Returns all zeros\nparse_adduct(\"invalid\") # Returns all zeros with warning",
    "crumbs": [
      "Get started",
      "Functions",
      "Parse",
      "parse_adduct"
    ]
  },
  {
    "objectID": "man/parse_adduct.html#parse-adduct",
    "href": "man/parse_adduct.html#parse-adduct",
    "title": "tima",
    "section": "",
    "text": "This function parses mass spectrometry adduct notation strings into their components: multimer count, isotope shift, modifications, charge state, and charge sign. It handles complex adducts with multiple additions/losses.\n\n\n\nparse_adduct(adduct_string, regex = ADDUCT_REGEX_PATTERN)\n\n\n\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct in standard notation (e.g., \"[M+H]+\", \"[2M+Na]+\", \"[M-H2O+H]+\")\n\n\n\n\nregex\n\n\nCharacter string regular expression pattern for parsing (default: uses ADDUCT_REGEX_PATTERN from constants)\n\n\n\n\n\n\nNamed numeric vector containing:\n\n\n\nn_mer\n\n\nInteger number of monomers (e.g., 2 for dimer, 1 for monomer)\n\n\n\n\nn_iso\n\n\nInteger isotope shift (e.g., 1 for M+1 isotopologue, 0 for monoisotopic)\n\n\n\n\nlos_add_clu\n\n\nNumeric total mass change in Daltons from all modifications\n\n\n\n\nn_charges\n\n\nInteger absolute number of charges (always positive)\n\n\n\n\ncharge\n\n\nInteger charge polarity (+1 for positive mode, -1 for negative mode)\n\n\n\nReturns all zeros if parsing fails.\n\n\n\n\nlibrary(\"tima\")\n\n# Simple adducts\nparse_adduct(\"[M+H]+\") # Protonated molecule\nparse_adduct(\"[M-H]-\") # Deprotonated molecule\nparse_adduct(\"[M+Na]+\") # Sodium adduct\n\n# Complex adducts\nparse_adduct(\"[2M+Na]+\") # Dimer with sodium\nparse_adduct(\"[M+H-H2O]+\") # Protonated with water loss\nparse_adduct(\"[M1+H]+\") # M+1 isotopologue\nparse_adduct(\"[2M1-C6H12O6 (hexose)+NaCl+H]2+\") # Complex modification\n\n# Error cases\nparse_adduct(NULL) # Returns all zeros\nparse_adduct(\"invalid\") # Returns all zeros with warning",
    "crumbs": [
      "Get started",
      "Functions",
      "Parse",
      "parse_adduct"
    ]
  },
  {
    "objectID": "man/create_components.html",
    "href": "man/create_components.html",
    "title": "tima",
    "section": "",
    "text": "This function creates network components (connected subgraphs) from edge lists using igraph. Each component represents a set of features that are connected through spectral similarity or other relationships.\n\n\n\ncreate_components(\n  input = get_params(step = \"create_components\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  output = get_params(step = \"create_components\")\\$files\\$networks\\$spectral\\$components\\$raw\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of file path(s) containing edge data. Files should have feature_source and feature_target columns.\n\n\n\n\noutput\n\n\nCharacter string path for the output components file\n\n\n\n\n\n\nCharacter string path to the created components file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo, data_interim)\nget_file(\n  url = paste0(dir, \"features/example_edges.tsv\"),\n  export = get_params(step = \"create_components\")$files$networks$spectral$edges$prepared\n)\ncreate_components()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Create",
      "create_components"
    ]
  },
  {
    "objectID": "man/create_components.html#create-components",
    "href": "man/create_components.html#create-components",
    "title": "tima",
    "section": "",
    "text": "This function creates network components (connected subgraphs) from edge lists using igraph. Each component represents a set of features that are connected through spectral similarity or other relationships.\n\n\n\ncreate_components(\n  input = get_params(step = \"create_components\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  output = get_params(step = \"create_components\")\\$files\\$networks\\$spectral\\$components\\$raw\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of file path(s) containing edge data. Files should have feature_source and feature_target columns.\n\n\n\n\noutput\n\n\nCharacter string path for the output components file\n\n\n\n\n\n\nCharacter string path to the created components file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo, data_interim)\nget_file(\n  url = paste0(dir, \"features/example_edges.tsv\"),\n  export = get_params(step = \"create_components\")$files$networks$spectral$edges$prepared\n)\ncreate_components()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Create",
      "create_components"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_bigg.html",
    "href": "man/prepare_libraries_sop_bigg.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares BiGG (Biochemical, Genetic and Genomic) structure-organism pairs by querying BiGG models and PubChem for metabolite information, extracting chemical structures, and formatting for TIMA annotation workflows.\n\n**Biota organism**: This function creates a special \"Biota\" organism\nfor metabolites present in all models (shared core metabolism). These\nstructures represent universal biochemical pathways found across all\nlife forms and are always assigned maximum biological score during\nannotation, regardless of sample taxonomy. The Biota organism has\norganism_taxonomy_01domain = \"Biota\" and ottid = 0.\n\n\n\n\n\nprepare_libraries_sop_bigg(\n  bigg_doi = \"10.1093/nar/gkv1049\",\n  bigg_models = list(`Escherichia coli` = c(model_id = \"iML1515\", doi =\n    \"10.1038/nbt.3956\"), `Saccharomyces cerevisiae` = c(model_id = \"iMM904\", doi =\n    \"10.1186/1752-0509-3-37\"), `Homo sapiens` = c(model_id = \"Recon3D\", doi =\n    \"10.1038/nbt.4072\")),\n  bigg_url = \"http://bigg.ucsd.edu/static/models/\",\n  output = get_params(step =\n    \"prepare_libraries_sop_bigg\")\\$files\\$libraries\\$sop\\$prepared\\$bigg\n)\n\n\n\n\n\n\n\nbigg_doi\n\n\nCharacter string DOI for BiGG database reference\n\n\n\n\nbigg_models\n\n\nNamed list of BiGG models with organism names as keys and named character vectors containing \"model_id\" and \"doi\" as values\n\n\n\n\nbigg_url\n\n\nCharacter string base URL for BiGG models API\n\n\n\n\noutput\n\n\nCharacter string path for prepared BiGG library output\n\n\n\n\n\n\nCharacter string path to prepared BiGG structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_bigg()\nunlink(\"data\", recursive = TRUE)"
  },
  {
    "objectID": "man/prepare_libraries_sop_bigg.html#prepare-libraries-of-structure-organism-pairs-bigg",
    "href": "man/prepare_libraries_sop_bigg.html#prepare-libraries-of-structure-organism-pairs-bigg",
    "title": "tima",
    "section": "",
    "text": "This function prepares BiGG (Biochemical, Genetic and Genomic) structure-organism pairs by querying BiGG models and PubChem for metabolite information, extracting chemical structures, and formatting for TIMA annotation workflows.\n\n**Biota organism**: This function creates a special \"Biota\" organism\nfor metabolites present in all models (shared core metabolism). These\nstructures represent universal biochemical pathways found across all\nlife forms and are always assigned maximum biological score during\nannotation, regardless of sample taxonomy. The Biota organism has\norganism_taxonomy_01domain = \"Biota\" and ottid = 0.\n\n\n\n\n\nprepare_libraries_sop_bigg(\n  bigg_doi = \"10.1093/nar/gkv1049\",\n  bigg_models = list(`Escherichia coli` = c(model_id = \"iML1515\", doi =\n    \"10.1038/nbt.3956\"), `Saccharomyces cerevisiae` = c(model_id = \"iMM904\", doi =\n    \"10.1186/1752-0509-3-37\"), `Homo sapiens` = c(model_id = \"Recon3D\", doi =\n    \"10.1038/nbt.4072\")),\n  bigg_url = \"http://bigg.ucsd.edu/static/models/\",\n  output = get_params(step =\n    \"prepare_libraries_sop_bigg\")\\$files\\$libraries\\$sop\\$prepared\\$bigg\n)\n\n\n\n\n\n\n\nbigg_doi\n\n\nCharacter string DOI for BiGG database reference\n\n\n\n\nbigg_models\n\n\nNamed list of BiGG models with organism names as keys and named character vectors containing \"model_id\" and \"doi\" as values\n\n\n\n\nbigg_url\n\n\nCharacter string base URL for BiGG models API\n\n\n\n\noutput\n\n\nCharacter string path for prepared BiGG library output\n\n\n\n\n\n\nCharacter string path to prepared BiGG structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_bigg()\nunlink(\"data\", recursive = TRUE)"
  },
  {
    "objectID": "man/log_failed.html",
    "href": "man/log_failed.html",
    "title": "tima",
    "section": "",
    "text": "Logs operation failure with error details and timing.\n\n\n\nlog_failed(ctx, error, ...)\n\n\n\n\n\n\n\nctx\n\n\ntima_log_context object from log_operation()\n\n\n\n\nerror\n\n\nError object or character message\n\n\n\n\n…\n\n\nAdditional context to log\n\n\n\n\n\n\nThe input context (invisibly)"
  },
  {
    "objectID": "man/log_failed.html#log-operation-failure",
    "href": "man/log_failed.html#log-operation-failure",
    "title": "tima",
    "section": "",
    "text": "Logs operation failure with error details and timing.\n\n\n\nlog_failed(ctx, error, ...)\n\n\n\n\n\n\n\nctx\n\n\ntima_log_context object from log_operation()\n\n\n\n\nerror\n\n\nError object or character message\n\n\n\n\n…\n\n\nAdditional context to log\n\n\n\n\n\n\nThe input context (invisibly)"
  },
  {
    "objectID": "man/prepare_features_components.html",
    "href": "man/prepare_features_components.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares molecular network component (cluster) assignments by loading, standardizing, and formatting component IDs for each feature. Components represent groups of related features in the molecular network.\n\n\n\nprepare_features_components(\n  input = get_params(step =\n    \"prepare_features_components\")\\$files\\$networks\\$spectral\\$components\\$raw,\n  output = get_params(step =\n    \"prepare_features_components\")\\$files\\$networks\\$spectral\\$components\\$prepared\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of paths to input component files. Can be a single file or multiple files that will be combined.\n\n\n\n\noutput\n\n\nCharacter string path where prepared components should be saved\n\n\n\n\n\n\nCharacter string path to the prepared features’ components file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\ninput &lt;- get_params(step = \"prepare_features_components\")$files$networks$spectral$components$raw\nget_file(url = paste0(dir, input), export = input)\nprepare_features_components(input = input)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_components"
    ]
  },
  {
    "objectID": "man/prepare_features_components.html#prepare-features-components",
    "href": "man/prepare_features_components.html#prepare-features-components",
    "title": "tima",
    "section": "",
    "text": "This function prepares molecular network component (cluster) assignments by loading, standardizing, and formatting component IDs for each feature. Components represent groups of related features in the molecular network.\n\n\n\nprepare_features_components(\n  input = get_params(step =\n    \"prepare_features_components\")\\$files\\$networks\\$spectral\\$components\\$raw,\n  output = get_params(step =\n    \"prepare_features_components\")\\$files\\$networks\\$spectral\\$components\\$prepared\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of paths to input component files. Can be a single file or multiple files that will be combined.\n\n\n\n\noutput\n\n\nCharacter string path where prepared components should be saved\n\n\n\n\n\n\nCharacter string path to the prepared features’ components file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\ninput &lt;- get_params(step = \"prepare_features_components\")$files$networks$spectral$components$raw\nget_file(url = paste0(dir, input), export = input)\nprepare_features_components(input = input)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_components"
    ]
  },
  {
    "objectID": "man/go_to_cache.html",
    "href": "man/go_to_cache.html",
    "title": "tima",
    "section": "",
    "text": "Creates and navigates to a cache directory in the user’s home directory. Useful for storing temporary files, intermediate results, and downloaded data in a consistent location across sessions.\n\n\n\ngo_to_cache(dir = \".tima\")\n\n\n\n\n\n\n\ndir\n\n\nCharacter string name of cache directory (default: \".tima\"). Created in user’s home directory. Must be non-empty.\n\n\n\n\n\n\nThe function:\n\n\nConstructs full path in user’s home directory\n\n\nCreates directory if it doesn’t exist\n\n\nChanges working directory to cache location\n\n\nLogs all operations\n\n\nCache directory persists across R sessions until explicitly deleted.\n\n\n\nPath to cache directory (invisibly). Changes working directory as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Default cache (~/.tima)\ngo_to_cache()\n\n# Custom cache\ngo_to_cache(dir = \".my_cache\")\n\n# Store path\ncache_path &lt;- go_to_cache()"
  },
  {
    "objectID": "man/go_to_cache.html#navigate-to-cache-directory",
    "href": "man/go_to_cache.html#navigate-to-cache-directory",
    "title": "tima",
    "section": "",
    "text": "Creates and navigates to a cache directory in the user’s home directory. Useful for storing temporary files, intermediate results, and downloaded data in a consistent location across sessions.\n\n\n\ngo_to_cache(dir = \".tima\")\n\n\n\n\n\n\n\ndir\n\n\nCharacter string name of cache directory (default: \".tima\"). Created in user’s home directory. Must be non-empty.\n\n\n\n\n\n\nThe function:\n\n\nConstructs full path in user’s home directory\n\n\nCreates directory if it doesn’t exist\n\n\nChanges working directory to cache location\n\n\nLogs all operations\n\n\nCache directory persists across R sessions until explicitly deleted.\n\n\n\nPath to cache directory (invisibly). Changes working directory as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Default cache (~/.tima)\ngo_to_cache()\n\n# Custom cache\ngo_to_cache(dir = \".my_cache\")\n\n# Store path\ncache_path &lt;- go_to_cache()"
  },
  {
    "objectID": "man/filter_annotations.html",
    "href": "man/filter_annotations.html",
    "title": "tima",
    "section": "",
    "text": "This function filters initial annotations by removing MS1-only annotations that also have spectral matches, and optionally filtering by retention time tolerance when RT libraries are available.\n\n\n\nfilter_annotations(\n  annotations = get_params(step =\n    \"filter_annotations\")\\$files\\$annotations\\$prepared\\$structural,\n  features = get_params(step = \"filter_annotations\")\\$files\\$features\\$prepared,\n  rts = get_params(step = \"filter_annotations\")\\$files\\$libraries\\$temporal\\$prepared,\n  output = get_params(step = \"filter_annotations\")\\$files\\$annotations\\$filtered,\n  tolerance_rt = get_params(step = \"filter_annotations\")\\$ms\\$tolerances\\$rt\\$library\n)\n\n\n\n\n\n\n\nannotations\n\n\nCharacter vector or list of paths to prepared annotation files\n\n\n\n\nfeatures\n\n\nCharacter string path to prepared features file\n\n\n\n\nrts\n\n\nCharacter string path to prepared retention time library (optional)\n\n\n\n\noutput\n\n\nCharacter string path for filtered annotations output\n\n\n\n\ntolerance_rt\n\n\nNumeric RT tolerance in minutes for library matching\n\n\n\n\n\n\nCharacter string path to the filtered annotations file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nannotations &lt;- get_params(step = \"filter_annotations\")$files$annotations$prepared$structural[[2]] |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nfeatures &lt;- get_params(step = \"filter_annotations\")$files$features$prepared |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nrts &lt;- get_params(step = \"filter_annotations\")$files$libraries$temporal$prepared |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nget_file(url = paste0(dir, annotations), export = annotations)\nget_file(url = paste0(dir, features), export = features)\nget_file(url = paste0(dir, rts), export = rts)\nfilter_annotations(\n  annotations = annotations,\n  features = features,\n  rts = rts\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Filter",
      "filter_annotations"
    ]
  },
  {
    "objectID": "man/filter_annotations.html#filter-annotations",
    "href": "man/filter_annotations.html#filter-annotations",
    "title": "tima",
    "section": "",
    "text": "This function filters initial annotations by removing MS1-only annotations that also have spectral matches, and optionally filtering by retention time tolerance when RT libraries are available.\n\n\n\nfilter_annotations(\n  annotations = get_params(step =\n    \"filter_annotations\")\\$files\\$annotations\\$prepared\\$structural,\n  features = get_params(step = \"filter_annotations\")\\$files\\$features\\$prepared,\n  rts = get_params(step = \"filter_annotations\")\\$files\\$libraries\\$temporal\\$prepared,\n  output = get_params(step = \"filter_annotations\")\\$files\\$annotations\\$filtered,\n  tolerance_rt = get_params(step = \"filter_annotations\")\\$ms\\$tolerances\\$rt\\$library\n)\n\n\n\n\n\n\n\nannotations\n\n\nCharacter vector or list of paths to prepared annotation files\n\n\n\n\nfeatures\n\n\nCharacter string path to prepared features file\n\n\n\n\nrts\n\n\nCharacter string path to prepared retention time library (optional)\n\n\n\n\noutput\n\n\nCharacter string path for filtered annotations output\n\n\n\n\ntolerance_rt\n\n\nNumeric RT tolerance in minutes for library matching\n\n\n\n\n\n\nCharacter string path to the filtered annotations file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nannotations &lt;- get_params(step = \"filter_annotations\")$files$annotations$prepared$structural[[2]] |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nfeatures &lt;- get_params(step = \"filter_annotations\")$files$features$prepared |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nrts &lt;- get_params(step = \"filter_annotations\")$files$libraries$temporal$prepared |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nget_file(url = paste0(dir, annotations), export = annotations)\nget_file(url = paste0(dir, features), export = features)\nget_file(url = paste0(dir, rts), export = rts)\nfilter_annotations(\n  annotations = annotations,\n  features = features,\n  rts = rts\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Filter",
      "filter_annotations"
    ]
  },
  {
    "objectID": "man/log_operation.html",
    "href": "man/log_operation.html",
    "title": "tima",
    "section": "",
    "text": "Creates a structured logging context that tracks operation lifecycle: start time, parameters, results, and elapsed time. This enables rich, hierarchical logging that’s easy to parse and analyze.\n\n\n\nlog_operation(operation_name, ...)\n\n\n\n\n\n\n\noperation_name\n\n\nCharacter, name of the operation (e.g., \"classify_structures\")\n\n\n\n\n…\n\n\nNamed parameters to log with the operation\n\n\n\n\n\n\nA list with class \"tima_log_context\" containing:\n\n\noperation: name of the operation\n\n\nstart_time: POSIXct timestamp\n\n\nparams: list of parameters\n\n\nmetadata: list for storing intermediate results\n\n\n\n\n\n\nlibrary(\"tima\")\n\nctx &lt;- log_operation(\"classify_structures\", n_smiles = 100)\n# ... do work ...\nlog_complete(ctx, n_classified = 95, n_failed = 5)"
  },
  {
    "objectID": "man/log_operation.html#create-a-logging-context-for-operations",
    "href": "man/log_operation.html#create-a-logging-context-for-operations",
    "title": "tima",
    "section": "",
    "text": "Creates a structured logging context that tracks operation lifecycle: start time, parameters, results, and elapsed time. This enables rich, hierarchical logging that’s easy to parse and analyze.\n\n\n\nlog_operation(operation_name, ...)\n\n\n\n\n\n\n\noperation_name\n\n\nCharacter, name of the operation (e.g., \"classify_structures\")\n\n\n\n\n…\n\n\nNamed parameters to log with the operation\n\n\n\n\n\n\nA list with class \"tima_log_context\" containing:\n\n\noperation: name of the operation\n\n\nstart_time: POSIXct timestamp\n\n\nparams: list of parameters\n\n\nmetadata: list for storing intermediate results\n\n\n\n\n\n\nlibrary(\"tima\")\n\nctx &lt;- log_operation(\"classify_structures\", n_smiles = 100)\n# ... do work ...\nlog_complete(ctx, n_classified = 95, n_failed = 5)"
  },
  {
    "objectID": "man/with_logging.html",
    "href": "man/with_logging.html",
    "title": "tima",
    "section": "",
    "text": "Wraps an expression with automatic start/complete/failed logging. This is the recommended way to add operation logging - it handles errors gracefully and ensures cleanup.\n\n\n\nwith_logging(operation_name, expr, ...)\n\n\n\n\n\n\n\noperation_name\n\n\nCharacter, name of the operation\n\n\n\n\nexpr\n\n\nExpression to evaluate\n\n\n\n\n…\n\n\nParameters to log with operation start\n\n\n\n\n\n\nResult of expr, or throws error if expr fails\n\n\n\n\nlibrary(\"tima\")\n\nresult &lt;- with_logging(\"classify\", n = 100, {\n  # Your code here\n  classify_structures(df)\n})"
  },
  {
    "objectID": "man/with_logging.html#wrap-an-operation-with-automatic-logging",
    "href": "man/with_logging.html#wrap-an-operation-with-automatic-logging",
    "title": "tima",
    "section": "",
    "text": "Wraps an expression with automatic start/complete/failed logging. This is the recommended way to add operation logging - it handles errors gracefully and ensures cleanup.\n\n\n\nwith_logging(operation_name, expr, ...)\n\n\n\n\n\n\n\noperation_name\n\n\nCharacter, name of the operation\n\n\n\n\nexpr\n\n\nExpression to evaluate\n\n\n\n\n…\n\n\nParameters to log with operation start\n\n\n\n\n\n\nResult of expr, or throws error if expr fails\n\n\n\n\nlibrary(\"tima\")\n\nresult &lt;- with_logging(\"classify\", n = 100, {\n  # Your code here\n  classify_structures(df)\n})"
  },
  {
    "objectID": "man/change_params_small.html",
    "href": "man/change_params_small.html",
    "title": "tima",
    "section": "",
    "text": "Updates TIMA workflow parameters for quick setup with a simplified interface. This function modifies the prepare_params YAML configuration file by copying provided input files to the appropriate directories and updating parameter values. Implements SOLID principles with clear separation of concerns.\n\n\n\nchange_params_small(\n  fil_pat = NULL,\n  fil_fea_raw = NULL,\n  fil_met_raw = NULL,\n  fil_sir_raw = NULL,\n  fil_spe_raw = NULL,\n  ms_pol = NULL,\n  org_tax = NULL,\n  hig_con = NULL,\n  summarize = NULL,\n  cache_dir = NULL\n)\n\n\n\n\n\n\n\nfil_pat\n\n\nCharacter. Job identifier/pattern for output files (optional)\n\n\n\n\nfil_fea_raw\n\n\nCharacter. Path to features file (e.g., from MZmine/SIRIUS)\n\n\n\n\nfil_met_raw\n\n\nCharacter. Path to metadata file (optional if single taxon)\n\n\n\n\nfil_sir_raw\n\n\nCharacter. Path to SIRIUS annotations directory/zip\n\n\n\n\nfil_spe_raw\n\n\nCharacter. Path to spectra file (MGF format with MS1/MS2)\n\n\n\n\nms_pol\n\n\nCharacter. MS polarity: \"pos\" or \"neg\"\n\n\n\n\norg_tax\n\n\nCharacter. Scientific name for single-taxon experiments\n\n\n\n\nhig_con\n\n\nLogical. Filter for high confidence candidates only\n\n\n\n\nsummarize\n\n\nLogical. Summarize all candidates per feature to single row\n\n\n\n\ncache_dir\n\n\nCharacter. Cache directory path (for testing; uses go_to_cache() if NULL)\n\n\n\n\n\n\nThis function:\n\n\nValidates all input files exist before copying\n\n\nCopies files to standardized cache locations\n\n\nUpdates the prepare_params YAML configuration\n\n\nHandles NA values properly for YAML null representation\n\n\n\n\n\nInvisible NULL. Modifies prepare_params YAML as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Setup complete workflow parameters\ncopy_backbone()\nchange_params_small(\n  fil_pat = \"gentiana_experiment\",\n  fil_fea_raw = \"data/raw/features.csv\",\n  fil_met_raw = \"data/raw/metadata.tsv\",\n  fil_sir_raw = \"data/raw/sirius_output.zip\",\n  fil_spe_raw = \"data/raw/spectra.mgf\",\n  ms_pol = \"pos\",\n  org_tax = \"Gentiana lutea\",\n  hig_con = TRUE,\n  summarize = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Change",
      "change_params_small"
    ]
  },
  {
    "objectID": "man/change_params_small.html#change-parameters-convenience-function",
    "href": "man/change_params_small.html#change-parameters-convenience-function",
    "title": "tima",
    "section": "",
    "text": "Updates TIMA workflow parameters for quick setup with a simplified interface. This function modifies the prepare_params YAML configuration file by copying provided input files to the appropriate directories and updating parameter values. Implements SOLID principles with clear separation of concerns.\n\n\n\nchange_params_small(\n  fil_pat = NULL,\n  fil_fea_raw = NULL,\n  fil_met_raw = NULL,\n  fil_sir_raw = NULL,\n  fil_spe_raw = NULL,\n  ms_pol = NULL,\n  org_tax = NULL,\n  hig_con = NULL,\n  summarize = NULL,\n  cache_dir = NULL\n)\n\n\n\n\n\n\n\nfil_pat\n\n\nCharacter. Job identifier/pattern for output files (optional)\n\n\n\n\nfil_fea_raw\n\n\nCharacter. Path to features file (e.g., from MZmine/SIRIUS)\n\n\n\n\nfil_met_raw\n\n\nCharacter. Path to metadata file (optional if single taxon)\n\n\n\n\nfil_sir_raw\n\n\nCharacter. Path to SIRIUS annotations directory/zip\n\n\n\n\nfil_spe_raw\n\n\nCharacter. Path to spectra file (MGF format with MS1/MS2)\n\n\n\n\nms_pol\n\n\nCharacter. MS polarity: \"pos\" or \"neg\"\n\n\n\n\norg_tax\n\n\nCharacter. Scientific name for single-taxon experiments\n\n\n\n\nhig_con\n\n\nLogical. Filter for high confidence candidates only\n\n\n\n\nsummarize\n\n\nLogical. Summarize all candidates per feature to single row\n\n\n\n\ncache_dir\n\n\nCharacter. Cache directory path (for testing; uses go_to_cache() if NULL)\n\n\n\n\n\n\nThis function:\n\n\nValidates all input files exist before copying\n\n\nCopies files to standardized cache locations\n\n\nUpdates the prepare_params YAML configuration\n\n\nHandles NA values properly for YAML null representation\n\n\n\n\n\nInvisible NULL. Modifies prepare_params YAML as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Setup complete workflow parameters\ncopy_backbone()\nchange_params_small(\n  fil_pat = \"gentiana_experiment\",\n  fil_fea_raw = \"data/raw/features.csv\",\n  fil_met_raw = \"data/raw/metadata.tsv\",\n  fil_sir_raw = \"data/raw/sirius_output.zip\",\n  fil_spe_raw = \"data/raw/spectra.mgf\",\n  ms_pol = \"pos\",\n  org_tax = \"Gentiana lutea\",\n  hig_con = TRUE,\n  summarize = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Change",
      "change_params_small"
    ]
  },
  {
    "objectID": "man/annotate_spectra.html",
    "href": "man/annotate_spectra.html",
    "title": "tima",
    "section": "",
    "text": "Annotates MS/MS query spectra against one or more spectral libraries, computing similarity scores and returning best candidate annotations above a similarity threshold.\n\n\n\nannotate_spectra(\n  input = get_params(step = \"annotate_spectra\")\\$files\\$spectral\\$raw,\n  libraries = get_params(step = \"annotate_spectra\")\\$files\\$libraries\\$spectral,\n  polarity = get_params(step = \"annotate_spectra\")\\$ms\\$polarity,\n  output = get_params(step = \"annotate_spectra\")\\$files\\$annotations\\$raw\\$spectral\\$spectral,\n  method = get_params(step = \"annotate_spectra\")\\$similarities\\$methods\\$annotations,\n  threshold = get_params(step = \"annotate_spectra\")\\$similarities\\$thresholds\\$annotations,\n  ppm = get_params(step = \"annotate_spectra\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms2,\n  dalton = get_params(step = \"annotate_spectra\")\\$ms\\$tolerances\\$mass\\$dalton\\$ms2,\n  qutoff = get_params(step = \"annotate_spectra\")\\$ms\\$thresholds\\$ms2\\$intensity,\n  approx = get_params(step = \"annotate_spectra\")\\$annotations\\$ms2approx\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector or list of query spectral file paths (.mgf).\n\n\n\n\nlibraries\n\n\nCharacter vector or list of library spectral file paths (.mgf / Spectra-supported). Must contain at least one path.\n\n\n\n\npolarity\n\n\nMS polarity; one of VALID_MS_MODES (\"pos\", \"neg\").\n\n\n\n\noutput\n\n\nOutput file path (the function writes a tabular file here).\n\n\n\n\nmethod\n\n\nSimilarity method; one of VALID_SIMILARITY_METHODS.\n\n\n\n\nthreshold\n\n\nMinimal similarity score to retain candidates (0-1).\n\n\n\n\nppm\n\n\nRelative mass tolerance (ppm) for MS/MS matching.\n\n\n\n\ndalton\n\n\nAbsolute mass tolerance (Daltons) for MS/MS matching.\n\n\n\n\nqutoff\n\n\nIntensity cutoff under which MS2 fragments are removed. (Parameter name kept for backwards compatibility; spelled \"cutoff\").\n\n\n\n\napprox\n\n\nLogical; if TRUE perform matching ignoring precursor masses (broader, slower); if FALSE restrict library to precursor-tolerant spectra first.\n\n\n\n\n\n\nThis is an orchestration wrapper that performs:\n\n\nInput validation & normalization (query + libraries, numeric params).\n\n\nQuery spectra import & light preprocessing (intensity cutoff).\n\n\nLibrary spectra import, cleaning of empty peak lists, optional polarity filtering, optional precursor-based library size reduction (when approx = FALSE).\n\n\nSimilarity computation via calculate_entropy_and_similarity().\n\n\nCandidate metadata extraction (formula, name, etc.).\n\n\nResult shaping: derive error (mz), select canonical output columns, threshold filtering, keep best per (feature_id, library, connectivity layer).\n\n\nExport of parameters & results to the configured output path.\n\n\nIf no annotations are produced (empty inputs or below threshold), a standardized empty template (see fake_annotations_columns()) is exported to ensure downstream code receives expected columns.\n\n\n\nCharacter scalar: the output file path (invisible). Side effect: writes the annotations table to output.\n\n\n\nThe function performs strict validation and logs informative messages. File existence is checked early; similarity computation is wrapped in a tryCatch to surface errors without leaving partially allocated objects.\n\n\n\nLibrary precursor reduction (when approx = FALSE) limits similarity computation to precursor-tolerant spectra, reducing complexity for large libraries. Repeated metadata extraction uses a single vectorized helper.\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_params(step = \"annotate_spectra\")$files$spectral$raw\n)\nget_file(\n  url = get_default_paths()$urls$examples$spectral_lib_mini$with_rt,\n  export = get_default_paths()$data$source$libraries$spectra$exp$with_rt\n)\nannotate_spectra(\n  libraries = get_default_paths()$data$source$libraries$spectra$exp$with_rt\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Annotate",
      "annotate_spectra"
    ]
  },
  {
    "objectID": "man/annotate_spectra.html#annotate-spectra",
    "href": "man/annotate_spectra.html#annotate-spectra",
    "title": "tima",
    "section": "",
    "text": "Annotates MS/MS query spectra against one or more spectral libraries, computing similarity scores and returning best candidate annotations above a similarity threshold.\n\n\n\nannotate_spectra(\n  input = get_params(step = \"annotate_spectra\")\\$files\\$spectral\\$raw,\n  libraries = get_params(step = \"annotate_spectra\")\\$files\\$libraries\\$spectral,\n  polarity = get_params(step = \"annotate_spectra\")\\$ms\\$polarity,\n  output = get_params(step = \"annotate_spectra\")\\$files\\$annotations\\$raw\\$spectral\\$spectral,\n  method = get_params(step = \"annotate_spectra\")\\$similarities\\$methods\\$annotations,\n  threshold = get_params(step = \"annotate_spectra\")\\$similarities\\$thresholds\\$annotations,\n  ppm = get_params(step = \"annotate_spectra\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms2,\n  dalton = get_params(step = \"annotate_spectra\")\\$ms\\$tolerances\\$mass\\$dalton\\$ms2,\n  qutoff = get_params(step = \"annotate_spectra\")\\$ms\\$thresholds\\$ms2\\$intensity,\n  approx = get_params(step = \"annotate_spectra\")\\$annotations\\$ms2approx\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector or list of query spectral file paths (.mgf).\n\n\n\n\nlibraries\n\n\nCharacter vector or list of library spectral file paths (.mgf / Spectra-supported). Must contain at least one path.\n\n\n\n\npolarity\n\n\nMS polarity; one of VALID_MS_MODES (\"pos\", \"neg\").\n\n\n\n\noutput\n\n\nOutput file path (the function writes a tabular file here).\n\n\n\n\nmethod\n\n\nSimilarity method; one of VALID_SIMILARITY_METHODS.\n\n\n\n\nthreshold\n\n\nMinimal similarity score to retain candidates (0-1).\n\n\n\n\nppm\n\n\nRelative mass tolerance (ppm) for MS/MS matching.\n\n\n\n\ndalton\n\n\nAbsolute mass tolerance (Daltons) for MS/MS matching.\n\n\n\n\nqutoff\n\n\nIntensity cutoff under which MS2 fragments are removed. (Parameter name kept for backwards compatibility; spelled \"cutoff\").\n\n\n\n\napprox\n\n\nLogical; if TRUE perform matching ignoring precursor masses (broader, slower); if FALSE restrict library to precursor-tolerant spectra first.\n\n\n\n\n\n\nThis is an orchestration wrapper that performs:\n\n\nInput validation & normalization (query + libraries, numeric params).\n\n\nQuery spectra import & light preprocessing (intensity cutoff).\n\n\nLibrary spectra import, cleaning of empty peak lists, optional polarity filtering, optional precursor-based library size reduction (when approx = FALSE).\n\n\nSimilarity computation via calculate_entropy_and_similarity().\n\n\nCandidate metadata extraction (formula, name, etc.).\n\n\nResult shaping: derive error (mz), select canonical output columns, threshold filtering, keep best per (feature_id, library, connectivity layer).\n\n\nExport of parameters & results to the configured output path.\n\n\nIf no annotations are produced (empty inputs or below threshold), a standardized empty template (see fake_annotations_columns()) is exported to ensure downstream code receives expected columns.\n\n\n\nCharacter scalar: the output file path (invisible). Side effect: writes the annotations table to output.\n\n\n\nThe function performs strict validation and logs informative messages. File existence is checked early; similarity computation is wrapped in a tryCatch to surface errors without leaving partially allocated objects.\n\n\n\nLibrary precursor reduction (when approx = FALSE) limits similarity computation to precursor-tolerant spectra, reducing complexity for large libraries. Repeated metadata extraction uses a single vectorized helper.\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_params(step = \"annotate_spectra\")$files$spectral$raw\n)\nget_file(\n  url = get_default_paths()$urls$examples$spectral_lib_mini$with_rt,\n  export = get_default_paths()$data$source$libraries$spectra$exp$with_rt\n)\nannotate_spectra(\n  libraries = get_default_paths()$data$source$libraries$spectra$exp$with_rt\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Annotate",
      "annotate_spectra"
    ]
  },
  {
    "objectID": "man/transform_score_sirius_csi.html",
    "href": "man/transform_score_sirius_csi.html",
    "title": "tima",
    "section": "",
    "text": "This function transforms SIRIUS CSI (Compound Structure Identification) scores using a sigmoid function. The transformation maps raw scores to a 0-1 range for better interpretability.\n\n\n\ntransform_score_sirius_csi(csi_score = NULL, K = 50, scale = 10)\n\n\n\n\n\n\n\ncsi_score\n\n\nNumeric SIRIUS CSI score (can be negative, NA, NULL, or absent)\n\n\n\n\nK\n\n\nNumeric shift parameter to adjust the sigmoid center (default: 50)\n\n\n\n\nscale\n\n\nNumeric scale parameter controlling sigmoid steepness (default: 10)\n\n\n\n\n\n\nThis is an experimental transformation not officially approved by SIRIUS developers. The sigmoid function is: 1 / (1 + exp(-(score + K) / scale))\n\n\n\nNumeric transformed score in the range (0, 1), or NA if input is NA/NULL/absent\n\n\n\n\nlibrary(\"tima\")\n\n# Transform a single score\ntransform_score_sirius_csi(csi_score = -20)\n\n# Transform with custom parameters\ntransform_score_sirius_csi(csi_score = -20, K = 30, scale = 5)\n\n# Vectorized transformation\nscores &lt;- c(-50, -20, 0, 20, 50)\ntransform_score_sirius_csi(csi_score = scores)\n\n# Handle NA values\nscores_with_na &lt;- c(-20, NA, 0, 20)\ntransform_score_sirius_csi(csi_score = scores_with_na)\n\n# Handle missing/absent score\ntransform_score_sirius_csi()"
  },
  {
    "objectID": "man/transform_score_sirius_csi.html#transform-sirius-csi-score",
    "href": "man/transform_score_sirius_csi.html#transform-sirius-csi-score",
    "title": "tima",
    "section": "",
    "text": "This function transforms SIRIUS CSI (Compound Structure Identification) scores using a sigmoid function. The transformation maps raw scores to a 0-1 range for better interpretability.\n\n\n\ntransform_score_sirius_csi(csi_score = NULL, K = 50, scale = 10)\n\n\n\n\n\n\n\ncsi_score\n\n\nNumeric SIRIUS CSI score (can be negative, NA, NULL, or absent)\n\n\n\n\nK\n\n\nNumeric shift parameter to adjust the sigmoid center (default: 50)\n\n\n\n\nscale\n\n\nNumeric scale parameter controlling sigmoid steepness (default: 10)\n\n\n\n\n\n\nThis is an experimental transformation not officially approved by SIRIUS developers. The sigmoid function is: 1 / (1 + exp(-(score + K) / scale))\n\n\n\nNumeric transformed score in the range (0, 1), or NA if input is NA/NULL/absent\n\n\n\n\nlibrary(\"tima\")\n\n# Transform a single score\ntransform_score_sirius_csi(csi_score = -20)\n\n# Transform with custom parameters\ntransform_score_sirius_csi(csi_score = -20, K = 30, scale = 5)\n\n# Vectorized transformation\nscores &lt;- c(-50, -20, 0, 20, 50)\ntransform_score_sirius_csi(csi_score = scores)\n\n# Handle NA values\nscores_with_na &lt;- c(-20, NA, 0, 20)\ntransform_score_sirius_csi(csi_score = scores_with_na)\n\n# Handle missing/absent score\ntransform_score_sirius_csi()"
  },
  {
    "objectID": "man/calculate_mass_of_m.html",
    "href": "man/calculate_mass_of_m.html",
    "title": "tima",
    "section": "",
    "text": "This function calculates the neutral mass (M) from an observed m/z value and adduct notation. It accounts for charge, multimers, isotopes, and adduct modifications.\n\nThe calculation follows the formula:\nM = ((z * (m/z + iso) - modifications - (z * sign * e_mass)) / n_mer)\n\nwhere:\n- z = number of charges\n- m/z = observed mass-to-charge ratio\n- iso = isotope shift (mass units)\n- modifications = total mass change from adduct modifications\n- sign = charge polarity (+1 or -1)\n- e_mass = electron mass\n- n_mer = multimer count\n\n\n\n\n\ncalculate_mass_of_m(mz, adduct_string, electron_mass = ELECTRON_MASS_DALTONS)\n\n\n\n\n\n\n\nmz\n\n\nNumeric observed m/z value in Daltons. Must be positive.\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct (e.g., [M+H]+, [2M+Na]+, [M-H2O+H]+)\n\n\n\n\nelectron_mass\n\n\nNumeric electron mass in Daltons (default: ELECTRON_MASS_DALTONS from constants.R - CODATA 2018 value)\n\n\n\n\n\n\nNumeric neutral mass (M) in Daltons. Returns 0 if: - Adduct parsing fails - Invalid input parameters - Division by zero would occur (n_mer = 0 or n_charges = 0) Returns NA if calculated mass is negative (physically impossible)\n\n\n\n\nlibrary(\"tima\")\n\n# Simple protonated molecule\ncalculate_mass_of_m(mz = 123.4567, adduct_string = \"[M+H]+\")\n# Expected: ~122.45 Da\n\n# Sodium adduct\ncalculate_mass_of_m(mz = 145.4421, adduct_string = \"[M+Na]+\")\n# Expected: ~122.45 Da\n\n# Complex adduct with water loss\ncalculate_mass_of_m(mz = 105.4467, adduct_string = \"[M+H-H2O]+\")\n# Expected: ~122.45 Da\n\n# Dimer\ncalculate_mass_of_m(mz = 245.9053, adduct_string = \"[2M+H]+\")\n# Expected: ~122.45 Da\n\n# Doubly charged\ncalculate_mass_of_m(mz = 62.2311, adduct_string = \"[M+2H]2+\")\n# Expected: ~122.45 Da\n\n# M+1 isotopologue\ncalculate_mass_of_m(mz = 124.4600, adduct_string = \"[M1+H]+\")\n# Expected: ~122.45 Da\n\n# Using custom electron mass (for testing)\ncalculate_mass_of_m(\n  mz = 123.4567,\n  adduct_string = \"[M+H]+\",\n  electron_mass = 0.0005486\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_mass_of_m"
    ]
  },
  {
    "objectID": "man/calculate_mass_of_m.html#calculate-mass-of-m",
    "href": "man/calculate_mass_of_m.html#calculate-mass-of-m",
    "title": "tima",
    "section": "",
    "text": "This function calculates the neutral mass (M) from an observed m/z value and adduct notation. It accounts for charge, multimers, isotopes, and adduct modifications.\n\nThe calculation follows the formula:\nM = ((z * (m/z + iso) - modifications - (z * sign * e_mass)) / n_mer)\n\nwhere:\n- z = number of charges\n- m/z = observed mass-to-charge ratio\n- iso = isotope shift (mass units)\n- modifications = total mass change from adduct modifications\n- sign = charge polarity (+1 or -1)\n- e_mass = electron mass\n- n_mer = multimer count\n\n\n\n\n\ncalculate_mass_of_m(mz, adduct_string, electron_mass = ELECTRON_MASS_DALTONS)\n\n\n\n\n\n\n\nmz\n\n\nNumeric observed m/z value in Daltons. Must be positive.\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct (e.g., [M+H]+, [2M+Na]+, [M-H2O+H]+)\n\n\n\n\nelectron_mass\n\n\nNumeric electron mass in Daltons (default: ELECTRON_MASS_DALTONS from constants.R - CODATA 2018 value)\n\n\n\n\n\n\nNumeric neutral mass (M) in Daltons. Returns 0 if: - Adduct parsing fails - Invalid input parameters - Division by zero would occur (n_mer = 0 or n_charges = 0) Returns NA if calculated mass is negative (physically impossible)\n\n\n\n\nlibrary(\"tima\")\n\n# Simple protonated molecule\ncalculate_mass_of_m(mz = 123.4567, adduct_string = \"[M+H]+\")\n# Expected: ~122.45 Da\n\n# Sodium adduct\ncalculate_mass_of_m(mz = 145.4421, adduct_string = \"[M+Na]+\")\n# Expected: ~122.45 Da\n\n# Complex adduct with water loss\ncalculate_mass_of_m(mz = 105.4467, adduct_string = \"[M+H-H2O]+\")\n# Expected: ~122.45 Da\n\n# Dimer\ncalculate_mass_of_m(mz = 245.9053, adduct_string = \"[2M+H]+\")\n# Expected: ~122.45 Da\n\n# Doubly charged\ncalculate_mass_of_m(mz = 62.2311, adduct_string = \"[M+2H]2+\")\n# Expected: ~122.45 Da\n\n# M+1 isotopologue\ncalculate_mass_of_m(mz = 124.4600, adduct_string = \"[M1+H]+\")\n# Expected: ~122.45 Da\n\n# Using custom electron mass (for testing)\ncalculate_mass_of_m(\n  mz = 123.4567,\n  adduct_string = \"[M+H]+\",\n  electron_mass = 0.0005486\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_mass_of_m"
    ]
  },
  {
    "objectID": "man/get_example_sirius.html",
    "href": "man/get_example_sirius.html",
    "title": "tima",
    "section": "",
    "text": "This function downloads example SIRIUS annotation files for testing and demonstration purposes. Downloads both SIRIUS v5 and v6 format files.\n\n\n\nget_example_sirius(\n  url = get_default_paths()\\$urls\\$examples\\$sirius,\n  export = get_default_paths()\\$data\\$interim\\$annotations\\$example_sirius\n)\n\n\n\n\n\n\n\nurl\n\n\nList containing URLs for SIRIUS examples (must have $v5 and $v6 elements)\n\n\n\n\nexport\n\n\nList containing export paths for SIRIUS examples (must have $v5 and $v6 elements)\n\n\n\n\n\n\nNULL (invisibly). Downloads files as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\nget_example_sirius()"
  },
  {
    "objectID": "man/get_example_sirius.html#get-example-sirius",
    "href": "man/get_example_sirius.html#get-example-sirius",
    "title": "tima",
    "section": "",
    "text": "This function downloads example SIRIUS annotation files for testing and demonstration purposes. Downloads both SIRIUS v5 and v6 format files.\n\n\n\nget_example_sirius(\n  url = get_default_paths()\\$urls\\$examples\\$sirius,\n  export = get_default_paths()\\$data\\$interim\\$annotations\\$example_sirius\n)\n\n\n\n\n\n\n\nurl\n\n\nList containing URLs for SIRIUS examples (must have $v5 and $v6 elements)\n\n\n\n\nexport\n\n\nList containing export paths for SIRIUS examples (must have $v5 and $v6 elements)\n\n\n\n\n\n\nNULL (invisibly). Downloads files as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\nget_example_sirius()"
  },
  {
    "objectID": "man/prepare_annotations_mzmine.html",
    "href": "man/prepare_annotations_mzmine.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares mzmine spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows.\n\n\n\nprepare_annotations_mzmine(\n  input = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$annotations\\$raw\\$spectral\\$mzmine,\n  output = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$annotations\\$prepared\\$structural\\$mzmine,\n  str_stereo = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string or vector of paths to mzmine annotation files\n\n\n\n\noutput\n\n\nCharacter string path for prepared mzmine annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared mzmine annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_mzmine()\nunlink(\"data\", recursive = TRUE)"
  },
  {
    "objectID": "man/prepare_annotations_mzmine.html#prepare-annotations-mzmine",
    "href": "man/prepare_annotations_mzmine.html#prepare-annotations-mzmine",
    "title": "tima",
    "section": "",
    "text": "This function prepares mzmine spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows.\n\n\n\nprepare_annotations_mzmine(\n  input = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$annotations\\$raw\\$spectral\\$mzmine,\n  output = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$annotations\\$prepared\\$structural\\$mzmine,\n  str_stereo = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_mzmine\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string or vector of paths to mzmine annotation files\n\n\n\n\noutput\n\n\nCharacter string path for prepared mzmine annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared mzmine annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_mzmine()\nunlink(\"data\", recursive = TRUE)"
  },
  {
    "objectID": "man/log_complete.html",
    "href": "man/log_complete.html",
    "title": "tima",
    "section": "",
    "text": "Logs the successful completion of an operation with results and timing.\n\n\n\nlog_complete(ctx, ...)\n\n\n\n\n\n\n\nctx\n\n\ntima_log_context object from log_operation()\n\n\n\n\n…\n\n\nNamed results to log\n\n\n\n\n\n\nThe input context (invisibly)"
  },
  {
    "objectID": "man/log_complete.html#log-operation-completion",
    "href": "man/log_complete.html#log-operation-completion",
    "title": "tima",
    "section": "",
    "text": "Logs the successful completion of an operation with results and timing.\n\n\n\nlog_complete(ctx, ...)\n\n\n\n\n\n\n\nctx\n\n\ntima_log_context object from log_operation()\n\n\n\n\n…\n\n\nNamed results to log\n\n\n\n\n\n\nThe input context (invisibly)"
  },
  {
    "objectID": "man/harmonize_adducts.html",
    "href": "man/harmonize_adducts.html",
    "title": "tima",
    "section": "",
    "text": "Standardizes adduct notations in a dataframe by replacing various forms with canonical representations. Uses a translation table for efficient batch replacement.\n\n\n\nharmonize_adducts(df, adducts_colname = \"adduct\", adducts_translations)\n\n\n\n\n\n\n\ndf\n\n\nData frame or tibble containing adduct column\n\n\n\n\nadducts_colname\n\n\nCharacter string name of the adduct column (default: \"adduct\")\n\n\n\n\nadducts_translations\n\n\nNamed character vector mapping original adduct notations (names) to standardized forms (values). If missing, returns dataframe unchanged.\n\n\n\n\n\n\nCommon adduct variations like \"M+H\", \"[M+H]\", and \"(M+H)+\" are standardized to a consistent format (e.g., \"[M+H]+\"). This ensures compatibility across different MS tools and databases.\n\n\n\nData frame with harmonized adduct column\n\n\n\n\nlibrary(\"tima\")\n\ndf &lt;- data.frame(adduct = c(\"M+H\", \"[M+Na]+\", \"(M-H)-\"))\ntranslations &lt;- c(\"M+H\" = \"[M+H]+\", \"(M-H)-\" = \"[M-H]-\")\nharmonize_adducts(df, adducts_translations = translations)",
    "crumbs": [
      "Get started",
      "Functions",
      "Harmonize",
      "harmonize_adducts"
    ]
  },
  {
    "objectID": "man/harmonize_adducts.html#harmonize-adduct-notations",
    "href": "man/harmonize_adducts.html#harmonize-adduct-notations",
    "title": "tima",
    "section": "",
    "text": "Standardizes adduct notations in a dataframe by replacing various forms with canonical representations. Uses a translation table for efficient batch replacement.\n\n\n\nharmonize_adducts(df, adducts_colname = \"adduct\", adducts_translations)\n\n\n\n\n\n\n\ndf\n\n\nData frame or tibble containing adduct column\n\n\n\n\nadducts_colname\n\n\nCharacter string name of the adduct column (default: \"adduct\")\n\n\n\n\nadducts_translations\n\n\nNamed character vector mapping original adduct notations (names) to standardized forms (values). If missing, returns dataframe unchanged.\n\n\n\n\n\n\nCommon adduct variations like \"M+H\", \"[M+H]\", and \"(M+H)+\" are standardized to a consistent format (e.g., \"[M+H]+\"). This ensures compatibility across different MS tools and databases.\n\n\n\nData frame with harmonized adduct column\n\n\n\n\nlibrary(\"tima\")\n\ndf &lt;- data.frame(adduct = c(\"M+H\", \"[M+Na]+\", \"(M-H)-\"))\ntranslations &lt;- c(\"M+H\" = \"[M+H]+\", \"(M-H)-\" = \"[M-H]-\")\nharmonize_adducts(df, adducts_translations = translations)",
    "crumbs": [
      "Get started",
      "Functions",
      "Harmonize",
      "harmonize_adducts"
    ]
  },
  {
    "objectID": "man/get_organism_taxonomy_ott.html",
    "href": "man/get_organism_taxonomy_ott.html",
    "title": "tima",
    "section": "",
    "text": "This function retrieves taxonomic information from the Open Tree of Life (OTT) taxonomy service. It cleans organism names, queries the OTT API, and returns structured taxonomic data including OTT IDs and hierarchical classifications.\n\n\n\nget_organism_taxonomy_ott(\n  df,\n  url = \"https://api.opentreeoflife.org/v3/taxonomy/about\",\n  retry = TRUE\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing organism names in a column named \"organism\"\n\n\n\n\nurl\n\n\nCharacter string URL of the OTT API endpoint (default: production API, can be changed for testing)\n\n\n\n\nretry\n\n\nLogical indicating whether to retry failed queries using only the generic epithet (genus name) when full species names fail (default: TRUE)\n\n\n\n\n\n\nData frame with taxonomic information including OTT IDs, ranks, and taxonomic hierarchy. Returns empty template if API is unavailable.\n\n\n\n\nlibrary(\"tima\")\n\n# Single organism\ndf &lt;- data.frame(organism = \"Homo sapiens\")\ntaxonomy &lt;- get_organism_taxonomy_ott(df)\n\n# Multiple organisms\ndf &lt;- data.frame(organism = c(\"Homo sapiens\", \"Arabidopsis thaliana\"))\ntaxonomy &lt;- get_organism_taxonomy_ott(df)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_organism_taxonomy_ott"
    ]
  },
  {
    "objectID": "man/get_organism_taxonomy_ott.html#get-organism-taxonomy-open-tree-of-life-taxonomy",
    "href": "man/get_organism_taxonomy_ott.html#get-organism-taxonomy-open-tree-of-life-taxonomy",
    "title": "tima",
    "section": "",
    "text": "This function retrieves taxonomic information from the Open Tree of Life (OTT) taxonomy service. It cleans organism names, queries the OTT API, and returns structured taxonomic data including OTT IDs and hierarchical classifications.\n\n\n\nget_organism_taxonomy_ott(\n  df,\n  url = \"https://api.opentreeoflife.org/v3/taxonomy/about\",\n  retry = TRUE\n)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing organism names in a column named \"organism\"\n\n\n\n\nurl\n\n\nCharacter string URL of the OTT API endpoint (default: production API, can be changed for testing)\n\n\n\n\nretry\n\n\nLogical indicating whether to retry failed queries using only the generic epithet (genus name) when full species names fail (default: TRUE)\n\n\n\n\n\n\nData frame with taxonomic information including OTT IDs, ranks, and taxonomic hierarchy. Returns empty template if API is unavailable.\n\n\n\n\nlibrary(\"tima\")\n\n# Single organism\ndf &lt;- data.frame(organism = \"Homo sapiens\")\ntaxonomy &lt;- get_organism_taxonomy_ott(df)\n\n# Multiple organisms\ndf &lt;- data.frame(organism = c(\"Homo sapiens\", \"Arabidopsis thaliana\"))\ntaxonomy &lt;- get_organism_taxonomy_ott(df)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_organism_taxonomy_ott"
    ]
  },
  {
    "objectID": "man/create_dir.html",
    "href": "man/create_dir.html",
    "title": "tima",
    "section": "",
    "text": "This function creates a directory at the specified path if it does not already exist. Handles both directory paths and file paths (extracting the directory component). Includes validation for write permissions.\n\n\n\ncreate_dir(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path to the directory or file path from which to extract and create the directory\n\n\n\n\n\n\nNULL (invisibly). Creates directory as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Create a directory\ncreate_dir(export = \"path/to/directory\")\n\n# Extract directory from file path and create\ncreate_dir(export = \"path/to/file.txt\")",
    "crumbs": [
      "Get started",
      "Functions",
      "Create",
      "create_dir"
    ]
  },
  {
    "objectID": "man/create_dir.html#create-directory",
    "href": "man/create_dir.html#create-directory",
    "title": "tima",
    "section": "",
    "text": "This function creates a directory at the specified path if it does not already exist. Handles both directory paths and file paths (extracting the directory component). Includes validation for write permissions.\n\n\n\ncreate_dir(export)\n\n\n\n\n\n\n\nexport\n\n\nCharacter string path to the directory or file path from which to extract and create the directory\n\n\n\n\n\n\nNULL (invisibly). Creates directory as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Create a directory\ncreate_dir(export = \"path/to/directory\")\n\n# Extract directory from file path and create\ncreate_dir(export = \"path/to/file.txt\")",
    "crumbs": [
      "Get started",
      "Functions",
      "Create",
      "create_dir"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_closed.html",
    "href": "man/prepare_libraries_sop_closed.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares closed (private/restricted) structure- organism pair libraries by formatting columns, rounding values, and standardizing structure. Falls back to an empty template if the closed resource is not accessible.\n\n\n\nprepare_libraries_sop_closed(\n  input = get_params(step =\n    \"prepare_libraries_sop_closed\")\\$files\\$libraries\\$sop\\$raw\\$closed,\n  output = get_params(step =\n    \"prepare_libraries_sop_closed\")\\$files\\$libraries\\$sop\\$prepared\\$closed\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to input closed library file\n\n\n\n\noutput\n\n\nCharacter string path where prepared library should be saved\n\n\n\n\n\n\nCharacter string path to the prepared structure-organism pairs library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_closed()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_closed"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_closed.html#prepare-libraries-of-structure-organism-pairs-closed",
    "href": "man/prepare_libraries_sop_closed.html#prepare-libraries-of-structure-organism-pairs-closed",
    "title": "tima",
    "section": "",
    "text": "This function prepares closed (private/restricted) structure- organism pair libraries by formatting columns, rounding values, and standardizing structure. Falls back to an empty template if the closed resource is not accessible.\n\n\n\nprepare_libraries_sop_closed(\n  input = get_params(step =\n    \"prepare_libraries_sop_closed\")\\$files\\$libraries\\$sop\\$raw\\$closed,\n  output = get_params(step =\n    \"prepare_libraries_sop_closed\")\\$files\\$libraries\\$sop\\$prepared\\$closed\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to input closed library file\n\n\n\n\noutput\n\n\nCharacter string path where prepared library should be saved\n\n\n\n\n\n\nCharacter string path to the prepared structure-organism pairs library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_closed()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_closed"
    ]
  },
  {
    "objectID": "man/calculate_similarity.html",
    "href": "man/calculate_similarity.html",
    "title": "tima",
    "section": "",
    "text": "Calculates similarity scores between query and target spectra using either entropy, cosine, or GNPS methods.\n\n\n\ncalculate_similarity(\n  method,\n  query_spectrum,\n  target_spectrum,\n  query_precursor,\n  target_precursor,\n  dalton,\n  ppm,\n  return_matched_peaks = FALSE,\n  ...\n)\n\n\n\n\n\n\n\nmethod\n\n\nCharacter string specifying method: \"entropy\", \"gnps\", or \"cosine\"\n\n\n\n\nquery_spectrum\n\n\nNumeric matrix with columns for mz and intensity\n\n\n\n\ntarget_spectrum\n\n\nNumeric matrix with columns for mz and intensity\n\n\n\n\nquery_precursor\n\n\nNumeric precursor m/z value for query\n\n\n\n\ntarget_precursor\n\n\nNumeric precursor m/z value for target\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching\n\n\n\n\nreturn_matched_peaks\n\n\nLogical; return matched peaks count? Not compatible with ‘entropy’ method. Default: FALSE\n\n\n\n\n…\n\n\nAdditional arguments passed to MsCoreUtils::join\n\n\n\n\n\n\nNumeric similarity score (0-1), or list with score and matches if return_matched_peaks = TRUE. Returns 0.0 if calculation fails.\n\n\n\n\nlibrary(\"tima\")\n\nsp_1 &lt;- cbind(\n  mz = c(10, 36, 63, 91, 93),\n  intensity = c(14, 15, 999, 650, 1)\n)\nprecursor_1 &lt;- 123.4567\nprecursor_2 &lt;- precursor_1 + 14\nsp_2 &lt;- cbind(\n  mz = c(10, 12, 50, 63, 105),\n  intensity = c(35, 5, 16, 999, 450)\n)\ncalculate_similarity(\n  method = \"entropy\",\n  query_spectrum = sp_1,\n  target_spectrum = sp_2,\n  query_precursor = precursor_1,\n  target_precursor = precursor_2,\n  dalton = 0.005,\n  ppm = 10.0\n)\n\n[1] 0.5427377\n\ncalculate_similarity(\n  method = \"gnps\",\n  query_spectrum = sp_1,\n  target_spectrum = sp_2,\n  query_precursor = precursor_1,\n  target_precursor = precursor_2,\n  dalton = 0.005,\n  ppm = 10.0,\n  return_matched_peaks = TRUE\n)\n\n$score\n[1] 0.9942988\n\n$matches\n[1] 4",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_similarity"
    ]
  },
  {
    "objectID": "man/calculate_similarity.html#calculate-similarity-between-spectra",
    "href": "man/calculate_similarity.html#calculate-similarity-between-spectra",
    "title": "tima",
    "section": "",
    "text": "Calculates similarity scores between query and target spectra using either entropy, cosine, or GNPS methods.\n\n\n\ncalculate_similarity(\n  method,\n  query_spectrum,\n  target_spectrum,\n  query_precursor,\n  target_precursor,\n  dalton,\n  ppm,\n  return_matched_peaks = FALSE,\n  ...\n)\n\n\n\n\n\n\n\nmethod\n\n\nCharacter string specifying method: \"entropy\", \"gnps\", or \"cosine\"\n\n\n\n\nquery_spectrum\n\n\nNumeric matrix with columns for mz and intensity\n\n\n\n\ntarget_spectrum\n\n\nNumeric matrix with columns for mz and intensity\n\n\n\n\nquery_precursor\n\n\nNumeric precursor m/z value for query\n\n\n\n\ntarget_precursor\n\n\nNumeric precursor m/z value for target\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching\n\n\n\n\nreturn_matched_peaks\n\n\nLogical; return matched peaks count? Not compatible with ‘entropy’ method. Default: FALSE\n\n\n\n\n…\n\n\nAdditional arguments passed to MsCoreUtils::join\n\n\n\n\n\n\nNumeric similarity score (0-1), or list with score and matches if return_matched_peaks = TRUE. Returns 0.0 if calculation fails.\n\n\n\n\nlibrary(\"tima\")\n\nsp_1 &lt;- cbind(\n  mz = c(10, 36, 63, 91, 93),\n  intensity = c(14, 15, 999, 650, 1)\n)\nprecursor_1 &lt;- 123.4567\nprecursor_2 &lt;- precursor_1 + 14\nsp_2 &lt;- cbind(\n  mz = c(10, 12, 50, 63, 105),\n  intensity = c(35, 5, 16, 999, 450)\n)\ncalculate_similarity(\n  method = \"entropy\",\n  query_spectrum = sp_1,\n  target_spectrum = sp_2,\n  query_precursor = precursor_1,\n  target_precursor = precursor_2,\n  dalton = 0.005,\n  ppm = 10.0\n)\n\n[1] 0.5427377\n\ncalculate_similarity(\n  method = \"gnps\",\n  query_spectrum = sp_1,\n  target_spectrum = sp_2,\n  query_precursor = precursor_1,\n  target_precursor = precursor_2,\n  dalton = 0.005,\n  ppm = 10.0,\n  return_matched_peaks = TRUE\n)\n\n$score\n[1] 0.9942988\n\n$matches\n[1] 4",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_similarity"
    ]
  },
  {
    "objectID": "man/get_gnps_tables.html",
    "href": "man/get_gnps_tables.html",
    "title": "tima",
    "section": "",
    "text": "This function downloads and retrieves GNPS (Global Natural Products Social Molecular Networking) result tables from a completed job. It fetches features, metadata, spectra, and annotation files from GNPS servers. When a job ID is not provided or GNPS resources are missing, small fake files are written so downstream steps do not fail during testing.\n\n\n\nget_gnps_tables(\n  gnps_job_id,\n  gnps_job_example = get_default_paths()\\$gnps\\$example,\n  filename = \"\",\n  workflow = \"fbmn\",\n  path_features,\n  path_metadata,\n  path_spectra,\n  path_source = get_default_paths()\\$data\\$source\\$path,\n  path_interim_a = get_default_paths()\\$data\\$interim\\$annotations\\$path,\n  path_interim_f = get_default_paths()\\$data\\$interim\\$features\\$path\n)\n\n\n\n\n\n\n\ngnps_job_id\n\n\nCharacter string GNPS job ID (32 characters). Can be NULL or empty string to skip download.\n\n\n\n\ngnps_job_example\n\n\nCharacter string example GNPS job ID for testing\n\n\n\n\nfilename\n\n\nCharacter string name of the file to download (used for fake outputs)\n\n\n\n\nworkflow\n\n\nCharacter string indicating workflow type: \"fbmn\" (feature-based) or \"classical\" molecular networking\n\n\n\n\npath_features\n\n\nCharacter string path for features output (file path)\n\n\n\n\npath_metadata\n\n\nCharacter string path for metadata output (file path or list)\n\n\n\n\npath_spectra\n\n\nCharacter string path for spectra output (file path)\n\n\n\n\npath_source\n\n\nCharacter string path to store source files\n\n\n\n\npath_interim_a\n\n\nCharacter string path to store interim annotations\n\n\n\n\npath_interim_f\n\n\nCharacter string path to store interim features\n\n\n\n\n\n\nA named character vector with paths to the written/available files.\n\n\n\n\nlibrary(\"tima\")\n\n# Download GNPS FBMN results\npaths &lt;- get_gnps_tables(\n  gnps_job_id = \"1234567890abcdef\",\n  workflow = \"fbmn\",\n  path_features = \"data/interim/features/features.tsv\",\n  path_metadata = \"data/source/metadata.tsv\",\n  path_spectra = \"data/interim/annotations/spectra.mgf\"\n)\n\n# Access downloaded files\nfeatures &lt;- read.delim(paths[\"features\"])"
  },
  {
    "objectID": "man/get_gnps_tables.html#get-gnps-tables",
    "href": "man/get_gnps_tables.html#get-gnps-tables",
    "title": "tima",
    "section": "",
    "text": "This function downloads and retrieves GNPS (Global Natural Products Social Molecular Networking) result tables from a completed job. It fetches features, metadata, spectra, and annotation files from GNPS servers. When a job ID is not provided or GNPS resources are missing, small fake files are written so downstream steps do not fail during testing.\n\n\n\nget_gnps_tables(\n  gnps_job_id,\n  gnps_job_example = get_default_paths()\\$gnps\\$example,\n  filename = \"\",\n  workflow = \"fbmn\",\n  path_features,\n  path_metadata,\n  path_spectra,\n  path_source = get_default_paths()\\$data\\$source\\$path,\n  path_interim_a = get_default_paths()\\$data\\$interim\\$annotations\\$path,\n  path_interim_f = get_default_paths()\\$data\\$interim\\$features\\$path\n)\n\n\n\n\n\n\n\ngnps_job_id\n\n\nCharacter string GNPS job ID (32 characters). Can be NULL or empty string to skip download.\n\n\n\n\ngnps_job_example\n\n\nCharacter string example GNPS job ID for testing\n\n\n\n\nfilename\n\n\nCharacter string name of the file to download (used for fake outputs)\n\n\n\n\nworkflow\n\n\nCharacter string indicating workflow type: \"fbmn\" (feature-based) or \"classical\" molecular networking\n\n\n\n\npath_features\n\n\nCharacter string path for features output (file path)\n\n\n\n\npath_metadata\n\n\nCharacter string path for metadata output (file path or list)\n\n\n\n\npath_spectra\n\n\nCharacter string path for spectra output (file path)\n\n\n\n\npath_source\n\n\nCharacter string path to store source files\n\n\n\n\npath_interim_a\n\n\nCharacter string path to store interim annotations\n\n\n\n\npath_interim_f\n\n\nCharacter string path to store interim features\n\n\n\n\n\n\nA named character vector with paths to the written/available files.\n\n\n\n\nlibrary(\"tima\")\n\n# Download GNPS FBMN results\npaths &lt;- get_gnps_tables(\n  gnps_job_id = \"1234567890abcdef\",\n  workflow = \"fbmn\",\n  path_features = \"data/interim/features/features.tsv\",\n  path_metadata = \"data/source/metadata.tsv\",\n  path_spectra = \"data/interim/annotations/spectra.mgf\"\n)\n\n# Access downloaded files\nfeatures &lt;- read.delim(paths[\"features\"])"
  },
  {
    "objectID": "man/get_file.html",
    "href": "man/get_file.html",
    "title": "tima",
    "section": "",
    "text": "Downloads a file from a URL with robust error handling, retry logic, and validation. Automatically creates necessary directories and validates downloaded content. Skips download if file already exists.\n\n\n\nget_file(url, export, limit = 3600L)\n\n\n\n\n\n\n\nurl\n\n\nCharacter string URL of the file to download\n\n\n\n\nexport\n\n\nCharacter string file path where the file should be saved\n\n\n\n\nlimit\n\n\nInteger timeout limit in seconds (default: 3600 = 1 hour)\n\n\n\n\n\n\nPath to the downloaded file (invisibly)\n\n\n\n\nlibrary(\"tima\")\n\nget_file(\n  url = \"https://example.com/data.tsv\",\n  export = \"data/source/data.tsv\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_file"
    ]
  },
  {
    "objectID": "man/get_file.html#download-file-from-url",
    "href": "man/get_file.html#download-file-from-url",
    "title": "tima",
    "section": "",
    "text": "Downloads a file from a URL with robust error handling, retry logic, and validation. Automatically creates necessary directories and validates downloaded content. Skips download if file already exists.\n\n\n\nget_file(url, export, limit = 3600L)\n\n\n\n\n\n\n\nurl\n\n\nCharacter string URL of the file to download\n\n\n\n\nexport\n\n\nCharacter string file path where the file should be saved\n\n\n\n\nlimit\n\n\nInteger timeout limit in seconds (default: 3600 = 1 hour)\n\n\n\n\n\n\nPath to the downloaded file (invisibly)\n\n\n\n\nlibrary(\"tima\")\n\nget_file(\n  url = \"https://example.com/data.tsv\",\n  export = \"data/source/data.tsv\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_file"
    ]
  },
  {
    "objectID": "NEWS.html#breaking-changes",
    "href": "NEWS.html#breaking-changes",
    "title": "tima",
    "section": "Breaking Changes",
    "text": "Breaking Changes\n\nsanitize_spectra(): cutoff parameter now defaults to NULL (dynamic) instead of 0"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "Version 3, 19 November 2007 Copyright (C) 2007 Free Software Foundation, Inc. &lt;https://fsf.org/&gt;\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\nThe GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow.\n\n\n\n\n\n“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\n\nThe work must carry prominent notices stating that you modified it, and giving a relevant date.\n\n\nThe work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\n\n\nYou must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\n\n\nIf the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\n\n\nConvey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\n\n\nConvey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\n\n\nConvey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\n\nDisclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\n\n\nRequiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\n\n\nProhibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\n\n\nLimiting the use for publicity purposes of names of licensors or authors of the material; or\n\n\nDeclining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\n\n\nRequiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS\n\n\n\n\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "LICENSE.html#preamble",
    "href": "LICENSE.html#preamble",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "The GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow."
  },
  {
    "objectID": "LICENSE.html#terms-and-conditions",
    "href": "LICENSE.html#terms-and-conditions",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\n\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\n\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\n\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\n\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\n\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\n\n\nThe work must carry prominent notices stating that you modified it, and giving a relevant date.\n\n\nThe work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to “keep intact all notices”.\n\n\nYou must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it.\n\n\nIf the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so.\n\n\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\n\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange.\n\n\nConvey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge.\n\n\nConvey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b.\n\n\nConvey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements.\n\n\nConvey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d.\n\n\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\n\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\n\n\nDisclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or\n\n\nRequiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or\n\n\nProhibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or\n\n\nLimiting the use for publicity purposes of names of licensors or authors of the material; or\n\n\nDeclining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or\n\n\nRequiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors.\n\n\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\n\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\n\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\n\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\n\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\n\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\n\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\n\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\n\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\nEND OF TERMS AND CONDITIONS"
  },
  {
    "objectID": "LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "href": "LICENSE.html#how-to-apply-these-terms-to-your-new-programs",
    "title": "GNU Affero General Public License",
    "section": "",
    "text": "If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as\n    published by the Free Software Foundation, either version 3 of the\n    License, or (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "man/tima_full.html",
    "href": "man/tima_full.html",
    "title": "tima",
    "section": "",
    "text": "DEPRECATED: This function has been renamed to run_tima. Please use run_tima() instead. tima_full() will be removed in a future version.\n\n\n\ntima_full(\n  target_pattern = \"^ann_wei\\$\",\n  log_file = \"tima.log\",\n  clean_old_logs = TRUE,\n  log_level = \"info\"\n)\n\n\n\n\n\n\n\ntarget_pattern\n\n\nCharacter. Regex pattern for target selection. Default: \"^ann_wei$\"\n\n\n\n\nlog_file\n\n\nCharacter. Path to log file. Default: \"tima.log\"\n\n\n\n\nclean_old_logs\n\n\nLogical. Remove old log file before starting. Default: TRUE\n\n\n\n\nlog_level\n\n\nCharacter or numeric. Logging verbosity level. Default: \"info\"\n\n\n\n\n\n\nThis function is deprecated as of TIMA version 2.12.0 (November 2025). It now simply calls run_tima with all arguments passed through, but issues a deprecation warning.\nMigration Guide:\n\n\nOld: tima_full(target_pattern = “^ann_wei$”)\n\n\nNew: run_tima(target_pattern = “^ann_wei$”)\n\n\nAll parameters and behavior are identical between the two functions.\n\n\n\nInvisible NULL (same as run_tima)\n\n\n\nrun_tima for the current function\n\n\n\n\nlibrary(\"tima\")\n\n# DEPRECATED - Use run_tima() instead\n# tima_full()\n\n# RECOMMENDED\nrun_tima()"
  },
  {
    "objectID": "man/tima_full.html#run-complete-tima-workflow-deprecated",
    "href": "man/tima_full.html#run-complete-tima-workflow-deprecated",
    "title": "tima",
    "section": "",
    "text": "DEPRECATED: This function has been renamed to run_tima. Please use run_tima() instead. tima_full() will be removed in a future version.\n\n\n\ntima_full(\n  target_pattern = \"^ann_wei\\$\",\n  log_file = \"tima.log\",\n  clean_old_logs = TRUE,\n  log_level = \"info\"\n)\n\n\n\n\n\n\n\ntarget_pattern\n\n\nCharacter. Regex pattern for target selection. Default: \"^ann_wei$\"\n\n\n\n\nlog_file\n\n\nCharacter. Path to log file. Default: \"tima.log\"\n\n\n\n\nclean_old_logs\n\n\nLogical. Remove old log file before starting. Default: TRUE\n\n\n\n\nlog_level\n\n\nCharacter or numeric. Logging verbosity level. Default: \"info\"\n\n\n\n\n\n\nThis function is deprecated as of TIMA version 2.12.0 (November 2025). It now simply calls run_tima with all arguments passed through, but issues a deprecation warning.\nMigration Guide:\n\n\nOld: tima_full(target_pattern = “^ann_wei$”)\n\n\nNew: run_tima(target_pattern = “^ann_wei$”)\n\n\nAll parameters and behavior are identical between the two functions.\n\n\n\nInvisible NULL (same as run_tima)\n\n\n\nrun_tima for the current function\n\n\n\n\nlibrary(\"tima\")\n\n# DEPRECATED - Use run_tima() instead\n# tima_full()\n\n# RECOMMENDED\nrun_tima()"
  },
  {
    "objectID": "man/prepare_annotations_spectra.html",
    "href": "man/prepare_annotations_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares MS2 spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows. Handles various spectral matching result formats.\n\n\n\nprepare_annotations_spectra(\n  input = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$annotations\\$raw\\$spectral\\$spectral,\n  output = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$annotations\\$prepared\\$structural\\$spectral,\n  str_stereo = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to spectral matching results file\n\n\n\n\noutput\n\n\nCharacter string path for prepared spectral annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared spectral annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo)\ninput &lt;- get_params(step = \"prepare_annotations_spectra\")$files$annotations$raw$spectral$spectral |&gt;\n  gsub(pattern = \".tsv.gz\", replacement = \"_pos.tsv\", fixed = TRUE)\nget_file(url = paste0(dir, input), export = input)\ndir &lt;- paste0(dir, data_interim)\nprepare_annotations_spectra(\n  input = input,\n  str_stereo = paste0(dir, \"libraries/sop/merged/structures/stereo.tsv\"),\n  str_met = paste0(dir, \"libraries/sop/merged/structures/metadata.tsv\"),\n  str_nam = paste0(dir, \"libraries/sop/merged/structures/names.tsv\"),\n  str_tax_cla = paste0(dir, \"libraries/sop/merged/structures/taxonomies/classyfire.tsv\"),\n  str_tax_npc = paste0(dir, \"libraries/sop/merged/structures/taxonomies/npc.tsv\")\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_spectra"
    ]
  },
  {
    "objectID": "man/prepare_annotations_spectra.html#prepare-annotations-ms2",
    "href": "man/prepare_annotations_spectra.html#prepare-annotations-ms2",
    "title": "tima",
    "section": "",
    "text": "This function prepares MS2 spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows. Handles various spectral matching result formats.\n\n\n\nprepare_annotations_spectra(\n  input = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$annotations\\$raw\\$spectral\\$spectral,\n  output = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$annotations\\$prepared\\$structural\\$spectral,\n  str_stereo = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_spectra\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to spectral matching results file\n\n\n\n\noutput\n\n\nCharacter string path for prepared spectral annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared spectral annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo)\ninput &lt;- get_params(step = \"prepare_annotations_spectra\")$files$annotations$raw$spectral$spectral |&gt;\n  gsub(pattern = \".tsv.gz\", replacement = \"_pos.tsv\", fixed = TRUE)\nget_file(url = paste0(dir, input), export = input)\ndir &lt;- paste0(dir, data_interim)\nprepare_annotations_spectra(\n  input = input,\n  str_stereo = paste0(dir, \"libraries/sop/merged/structures/stereo.tsv\"),\n  str_met = paste0(dir, \"libraries/sop/merged/structures/metadata.tsv\"),\n  str_nam = paste0(dir, \"libraries/sop/merged/structures/names.tsv\"),\n  str_tax_cla = paste0(dir, \"libraries/sop/merged/structures/taxonomies/classyfire.tsv\"),\n  str_tax_npc = paste0(dir, \"libraries/sop/merged/structures/taxonomies/npc.tsv\")\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_spectra"
    ]
  },
  {
    "objectID": "man/validate_inputs.html",
    "href": "man/validate_inputs.html",
    "title": "tima",
    "section": "",
    "text": "Standalone command to validate all input data before starting the TIMA pipeline. This helps catch issues early and avoid wasting time on library downloads and processing.\n\n\n\nvalidate_inputs(\n  features = NULL,\n  spectra = NULL,\n  metadata = NULL,\n  sirius = NULL,\n  filename_col = \"filename\",\n  organism_col = \"organism\",\n  feature_col = \"feature_id\"\n)\n\n\n\n\n\n\n\nfeatures\n\n\nCharacter path to features CSV/TSV file\n\n\n\n\nspectra\n\n\nCharacter path to MGF spectra file\n\n\n\n\nmetadata\n\n\nCharacter path to metadata file\n\n\n\n\nsirius\n\n\nCharacter path to SIRIUS output directory or ZIP file\n\n\n\n\nfilename_col\n\n\nCharacter name of filename column (default: \"filename\")\n\n\n\n\norganism_col\n\n\nCharacter name of organism column (default: \"organism\")\n\n\n\n\nfeature_col\n\n\nCharacter name of feature ID column (default: \"feature_id\")\n\n\n\n\n\n\nInvisible TRUE if all checks pass, stops with error otherwise\n\n\n\n\nlibrary(\"tima\")\n\n# Validate all inputs before starting pipeline\nvalidate_inputs(\n  features = \"data/features.csv\",\n  spectra = \"data/spectra.mgf\",\n  sirius = \"data/sirius_output\"\n)\n\n# Validate with metadata consistency check\nvalidate_inputs(\n  features = \"data/features.csv\",\n  metadata = \"data/metadata.tsv\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Validate",
      "validate_inputs"
    ]
  },
  {
    "objectID": "man/validate_inputs.html#validate-input-data",
    "href": "man/validate_inputs.html#validate-input-data",
    "title": "tima",
    "section": "",
    "text": "Standalone command to validate all input data before starting the TIMA pipeline. This helps catch issues early and avoid wasting time on library downloads and processing.\n\n\n\nvalidate_inputs(\n  features = NULL,\n  spectra = NULL,\n  metadata = NULL,\n  sirius = NULL,\n  filename_col = \"filename\",\n  organism_col = \"organism\",\n  feature_col = \"feature_id\"\n)\n\n\n\n\n\n\n\nfeatures\n\n\nCharacter path to features CSV/TSV file\n\n\n\n\nspectra\n\n\nCharacter path to MGF spectra file\n\n\n\n\nmetadata\n\n\nCharacter path to metadata file\n\n\n\n\nsirius\n\n\nCharacter path to SIRIUS output directory or ZIP file\n\n\n\n\nfilename_col\n\n\nCharacter name of filename column (default: \"filename\")\n\n\n\n\norganism_col\n\n\nCharacter name of organism column (default: \"organism\")\n\n\n\n\nfeature_col\n\n\nCharacter name of feature ID column (default: \"feature_id\")\n\n\n\n\n\n\nInvisible TRUE if all checks pass, stops with error otherwise\n\n\n\n\nlibrary(\"tima\")\n\n# Validate all inputs before starting pipeline\nvalidate_inputs(\n  features = \"data/features.csv\",\n  spectra = \"data/spectra.mgf\",\n  sirius = \"data/sirius_output\"\n)\n\n# Validate with metadata consistency check\nvalidate_inputs(\n  features = \"data/features.csv\",\n  metadata = \"data/metadata.tsv\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Validate",
      "validate_inputs"
    ]
  },
  {
    "objectID": "man/prepare_annotations_gnps.html",
    "href": "man/prepare_annotations_gnps.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares GNPS spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows.\n\n\n\nprepare_annotations_gnps(\n  input = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$annotations\\$raw\\$spectral\\$gnps,\n  output = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$annotations\\$prepared\\$structural\\$gnps,\n  str_stereo = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string or vector of paths to GNPS annotation files\n\n\n\n\noutput\n\n\nCharacter string path for prepared GNPS annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared GNPS annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_gnps()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_gnps"
    ]
  },
  {
    "objectID": "man/prepare_annotations_gnps.html#prepare-annotations-gnps",
    "href": "man/prepare_annotations_gnps.html#prepare-annotations-gnps",
    "title": "tima",
    "section": "",
    "text": "This function prepares GNPS spectral library matching results by standardizing column names, integrating structure metadata, and formatting for downstream TIMA annotation workflows.\n\n\n\nprepare_annotations_gnps(\n  input = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$annotations\\$raw\\$spectral\\$gnps,\n  output = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$annotations\\$prepared\\$structural\\$gnps,\n  str_stereo = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"prepare_annotations_gnps\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string or vector of paths to GNPS annotation files\n\n\n\n\noutput\n\n\nCharacter string path for prepared GNPS annotations output\n\n\n\n\nstr_stereo\n\n\nCharacter string path to structures stereochemistry file\n\n\n\n\nstr_met\n\n\nCharacter string path to structures metadata file\n\n\n\n\nstr_nam\n\n\nCharacter string path to structures names file\n\n\n\n\nstr_tax_cla\n\n\nCharacter string path to ClassyFire taxonomy file\n\n\n\n\nstr_tax_npc\n\n\nCharacter string path to NPClassifier taxonomy file\n\n\n\n\n\n\nCharacter string path to prepared GNPS annotations\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_annotations_gnps()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Annotations",
      "prepare_annotations_gnps"
    ]
  },
  {
    "objectID": "man/calculate_mz_from_mass.html",
    "href": "man/calculate_mz_from_mass.html",
    "title": "tima",
    "section": "",
    "text": "This is the inverse of calculate_mass_of_m. Given a neutral mass and adduct, it calculates the expected m/z value.\n\n\n\ncalculate_mz_from_mass(\n  neutral_mass,\n  adduct_string,\n  electron_mass = ELECTRON_MASS_DALTONS\n)\n\n\n\n\n\n\n\nneutral_mass\n\n\nNumeric neutral mass (M) in Daltons\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct\n\n\n\n\nelectron_mass\n\n\nNumeric electron mass in Daltons\n\n\n\n\n\n\nNumeric m/z value in Daltons\n\n\n\n\nlibrary(\"tima\")\n\n# Calculate m/z for a protonated molecule\ncalculate_mz_from_mass(neutral_mass = 122.45, adduct_string = \"[M+H]+\")\n# Expected: ~123.4567\n\n# Verify round-trip calculation\nmass &lt;- 122.45\nadduct &lt;- \"[M+H]+\"\nmz &lt;- calculate_mz_from_mass(mass, adduct)\nmass_back &lt;- calculate_mass_of_m(mz, adduct)\nall.equal(mass, mass_back) # Should be TRUE",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_mz_from_mass"
    ]
  },
  {
    "objectID": "man/calculate_mz_from_mass.html#calculate-mz-from-neutral-mass-inverse-operation",
    "href": "man/calculate_mz_from_mass.html#calculate-mz-from-neutral-mass-inverse-operation",
    "title": "tima",
    "section": "",
    "text": "This is the inverse of calculate_mass_of_m. Given a neutral mass and adduct, it calculates the expected m/z value.\n\n\n\ncalculate_mz_from_mass(\n  neutral_mass,\n  adduct_string,\n  electron_mass = ELECTRON_MASS_DALTONS\n)\n\n\n\n\n\n\n\nneutral_mass\n\n\nNumeric neutral mass (M) in Daltons\n\n\n\n\nadduct_string\n\n\nCharacter string representing the adduct\n\n\n\n\nelectron_mass\n\n\nNumeric electron mass in Daltons\n\n\n\n\n\n\nNumeric m/z value in Daltons\n\n\n\n\nlibrary(\"tima\")\n\n# Calculate m/z for a protonated molecule\ncalculate_mz_from_mass(neutral_mass = 122.45, adduct_string = \"[M+H]+\")\n# Expected: ~123.4567\n\n# Verify round-trip calculation\nmass &lt;- 122.45\nadduct &lt;- \"[M+H]+\"\nmz &lt;- calculate_mz_from_mass(mass, adduct)\nmass_back &lt;- calculate_mass_of_m(mz, adduct)\nall.equal(mass, mass_back) # Should be TRUE",
    "crumbs": [
      "Get started",
      "Functions",
      "Calculate",
      "calculate_mz_from_mass"
    ]
  },
  {
    "objectID": "man/get_example_files.html",
    "href": "man/get_example_files.html",
    "title": "tima",
    "section": "",
    "text": "This function downloads example data files for testing and demonstration purposes. Supports downloading features, metadata, SIRIUS annotations, mass spectra, and spectral libraries with retention times.\n\n\n\nget_example_files(\n  example = c(\"features\", \"metadata\", \"sirius\", \"spectra\"),\n  in_cache = TRUE\n)\n\n\n\n\n\n\n\nexample\n\n\nCharacter vector specifying which example files to download. Valid options: \"features\", \"metadata\", \"sirius\", \"spectra\", \"spectral_lib_with_rt\"\n\n\n\n\nin_cache\n\n\nLogical whether to store files in the cache directory (default: TRUE)\n\n\n\n\n\n\nNULL (invisibly). Downloads files as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Download features and metadata examples\nget_example_files(example = c(\"features\", \"metadata\"))\n\n# Download all example files to cache\nget_example_files(\n  example = c(\"features\", \"metadata\", \"sirius\", \"spectra\"),\n  in_cache = TRUE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_example_files"
    ]
  },
  {
    "objectID": "man/get_example_files.html#get-example-files",
    "href": "man/get_example_files.html#get-example-files",
    "title": "tima",
    "section": "",
    "text": "This function downloads example data files for testing and demonstration purposes. Supports downloading features, metadata, SIRIUS annotations, mass spectra, and spectral libraries with retention times.\n\n\n\nget_example_files(\n  example = c(\"features\", \"metadata\", \"sirius\", \"spectra\"),\n  in_cache = TRUE\n)\n\n\n\n\n\n\n\nexample\n\n\nCharacter vector specifying which example files to download. Valid options: \"features\", \"metadata\", \"sirius\", \"spectra\", \"spectral_lib_with_rt\"\n\n\n\n\nin_cache\n\n\nLogical whether to store files in the cache directory (default: TRUE)\n\n\n\n\n\n\nNULL (invisibly). Downloads files as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Download features and metadata examples\nget_example_files(example = c(\"features\", \"metadata\"))\n\n# Download all example files to cache\nget_example_files(\n  example = c(\"features\", \"metadata\", \"sirius\", \"spectra\"),\n  in_cache = TRUE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_example_files"
    ]
  },
  {
    "objectID": "man/prepare_taxa.html",
    "href": "man/prepare_taxa.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares taxonomic information for features by matching organism names to Open Tree of Life taxonomy. Can attribute all features to a single organism or distribute them across multiple organisms based on relative intensities in samples.\n\n\n\nprepare_taxa(\n  input = get_params(step = \"prepare_taxa\")\\$files\\$features\\$prepared,\n  extension = get_params(step = \"prepare_taxa\")\\$names\\$extension,\n  name_filename = get_params(step = \"prepare_taxa\")\\$names\\$filename,\n  colname = get_params(step = \"prepare_taxa\")\\$names\\$taxon,\n  metadata = get_params(step = \"prepare_taxa\")\\$files\\$metadata\\$raw,\n  org_tax_ott = get_params(step =\n    \"prepare_taxa\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  output = get_params(step = \"prepare_taxa\")\\$files\\$metadata\\$prepared,\n  taxon = get_params(step = \"prepare_taxa\")\\$organisms\\$taxon\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to features file with intensities\n\n\n\n\nextension\n\n\nLogical whether column names contain file extensions\n\n\n\n\nname_filename\n\n\nCharacter string name of filename column in metadata\n\n\n\n\ncolname\n\n\nCharacter string name of column with biological source info\n\n\n\n\nmetadata\n\n\nCharacter string path to metadata file with organism info\n\n\n\n\norg_tax_ott\n\n\nCharacter string path to Open Tree of Life taxonomy file\n\n\n\n\noutput\n\n\nCharacter string path for output file\n\n\n\n\ntaxon\n\n\nCharacter string organism name to enforce for all features (e.g., \"Homo sapiens\"). If provided, overrides metadata-based assignment.\n\n\n\n\n\n\nDepending on whether features are aligned between samples from various organisms, this function either: - Attributes all features to a single organism (if taxon specified), or - Attributes features to multiple organisms based on their relative intensities across samples (using metadata)\n\n\n\nCharacter string path to the prepared taxa file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\norg_tax_ott &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/organisms/taxonomies/ott.tsv\"\n)\nget_file(url = paste0(dir, org_tax_ott), export = org_tax_ott)\nget_file(\n  url = paste0(dir, \"data/interim/features/example_features.tsv\"),\n  export = get_params(step = \"prepare_taxa\")$files$features$prepared\n)\nprepare_taxa(\n  taxon = \"Homo sapiens\",\n  org_tax_ott = org_tax_ott\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "prepare_taxa"
    ]
  },
  {
    "objectID": "man/prepare_taxa.html#prepare-taxa",
    "href": "man/prepare_taxa.html#prepare-taxa",
    "title": "tima",
    "section": "",
    "text": "This function prepares taxonomic information for features by matching organism names to Open Tree of Life taxonomy. Can attribute all features to a single organism or distribute them across multiple organisms based on relative intensities in samples.\n\n\n\nprepare_taxa(\n  input = get_params(step = \"prepare_taxa\")\\$files\\$features\\$prepared,\n  extension = get_params(step = \"prepare_taxa\")\\$names\\$extension,\n  name_filename = get_params(step = \"prepare_taxa\")\\$names\\$filename,\n  colname = get_params(step = \"prepare_taxa\")\\$names\\$taxon,\n  metadata = get_params(step = \"prepare_taxa\")\\$files\\$metadata\\$raw,\n  org_tax_ott = get_params(step =\n    \"prepare_taxa\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  output = get_params(step = \"prepare_taxa\")\\$files\\$metadata\\$prepared,\n  taxon = get_params(step = \"prepare_taxa\")\\$organisms\\$taxon\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to features file with intensities\n\n\n\n\nextension\n\n\nLogical whether column names contain file extensions\n\n\n\n\nname_filename\n\n\nCharacter string name of filename column in metadata\n\n\n\n\ncolname\n\n\nCharacter string name of column with biological source info\n\n\n\n\nmetadata\n\n\nCharacter string path to metadata file with organism info\n\n\n\n\norg_tax_ott\n\n\nCharacter string path to Open Tree of Life taxonomy file\n\n\n\n\noutput\n\n\nCharacter string path for output file\n\n\n\n\ntaxon\n\n\nCharacter string organism name to enforce for all features (e.g., \"Homo sapiens\"). If provided, overrides metadata-based assignment.\n\n\n\n\n\n\nDepending on whether features are aligned between samples from various organisms, this function either: - Attributes all features to a single organism (if taxon specified), or - Attributes features to multiple organisms based on their relative intensities across samples (using metadata)\n\n\n\nCharacter string path to the prepared taxa file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\norg_tax_ott &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/organisms/taxonomies/ott.tsv\"\n)\nget_file(url = paste0(dir, org_tax_ott), export = org_tax_ott)\nget_file(\n  url = paste0(dir, \"data/interim/features/example_features.tsv\"),\n  export = get_params(step = \"prepare_taxa\")$files$features$prepared\n)\nprepare_taxa(\n  taxon = \"Homo sapiens\",\n  org_tax_ott = org_tax_ott\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "prepare_taxa"
    ]
  },
  {
    "objectID": "man/copy_backbone.html",
    "href": "man/copy_backbone.html",
    "title": "tima",
    "section": "",
    "text": "This function copies the package backbone (default directory structure, configuration files, and parameters) to a cache directory. This sets up the working environment for TIMA workflows.\n\n\n\ncopy_backbone(cache_dir = fs::path_home(\".tima\"), package = \"tima\")\n\n\n\n\n\n\n\ncache_dir\n\n\nCharacter string path to the cache directory (default: \"~/.tima\" in user’s home directory)\n\n\n\n\npackage\n\n\nCharacter string name of the package (default: \"tima\")\n\n\n\n\n\n\nNULL (invisibly). Creates cache directory structure as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Copy to default cache location\ncopy_backbone()\n\n# Copy to custom location\ncopy_backbone(cache_dir = \"~/my_tima_cache\")"
  },
  {
    "objectID": "man/copy_backbone.html#copy-backbone",
    "href": "man/copy_backbone.html#copy-backbone",
    "title": "tima",
    "section": "",
    "text": "This function copies the package backbone (default directory structure, configuration files, and parameters) to a cache directory. This sets up the working environment for TIMA workflows.\n\n\n\ncopy_backbone(cache_dir = fs::path_home(\".tima\"), package = \"tima\")\n\n\n\n\n\n\n\ncache_dir\n\n\nCharacter string path to the cache directory (default: \"~/.tima\" in user’s home directory)\n\n\n\n\npackage\n\n\nCharacter string name of the package (default: \"tima\")\n\n\n\n\n\n\nNULL (invisibly). Creates cache directory structure as side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Copy to default cache location\ncopy_backbone()\n\n# Copy to custom location\ncopy_backbone(cache_dir = \"~/my_tima_cache\")"
  },
  {
    "objectID": "man/run_app.html",
    "href": "man/run_app.html",
    "title": "tima",
    "section": "",
    "text": "Launches the TIMA Shiny web application for interactive metabolite annotation. Automatically detects Docker containers and adjusts network settings accordingly.\n\n\n\nrun_app(host = \"127.0.0.1\", port = 3838, browser = TRUE)\n\n\n\n\n\n\n\nhost\n\n\nCharacter string specifying the host/IP address to listen on. Default: \"127.0.0.1\" (localhost). Use \"0.0.0.0\" to allow external connections.\n\n\n\n\nport\n\n\nInteger port number to listen on. Default: 3838. Valid range: 1-65535.\n\n\n\n\nbrowser\n\n\nLogical whether to automatically launch a web browser when starting the app. Default: TRUE. Automatically set to FALSE in Docker.\n\n\n\n\n\n\nNULL (invisibly). Launches the Shiny app as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Launch app on localhost\nrun_app()\n\n# Launch on custom port\nrun_app(port = 8080)\n\n# Allow external connections (useful in Docker)\nrun_app(host = \"0.0.0.0\", port = 3838)",
    "crumbs": [
      "Get started",
      "Functions",
      "Run",
      "run_app"
    ]
  },
  {
    "objectID": "man/run_app.html#run-tima-shiny-app",
    "href": "man/run_app.html#run-tima-shiny-app",
    "title": "tima",
    "section": "",
    "text": "Launches the TIMA Shiny web application for interactive metabolite annotation. Automatically detects Docker containers and adjusts network settings accordingly.\n\n\n\nrun_app(host = \"127.0.0.1\", port = 3838, browser = TRUE)\n\n\n\n\n\n\n\nhost\n\n\nCharacter string specifying the host/IP address to listen on. Default: \"127.0.0.1\" (localhost). Use \"0.0.0.0\" to allow external connections.\n\n\n\n\nport\n\n\nInteger port number to listen on. Default: 3838. Valid range: 1-65535.\n\n\n\n\nbrowser\n\n\nLogical whether to automatically launch a web browser when starting the app. Default: TRUE. Automatically set to FALSE in Docker.\n\n\n\n\n\n\nNULL (invisibly). Launches the Shiny app as a side effect.\n\n\n\n\nlibrary(\"tima\")\n\n# Launch app on localhost\nrun_app()\n\n# Launch on custom port\nrun_app(port = 8080)\n\n# Allow external connections (useful in Docker)\nrun_app(host = \"0.0.0.0\", port = 3838)",
    "crumbs": [
      "Get started",
      "Functions",
      "Run",
      "run_app"
    ]
  },
  {
    "objectID": "man/log_metadata.html",
    "href": "man/log_metadata.html",
    "title": "tima",
    "section": "",
    "text": "Adds metadata to an operation context and logs it.\n\n\n\nlog_metadata(ctx, ...)\n\n\n\n\n\n\n\nctx\n\n\ntima_log_context object from log_operation()\n\n\n\n\n…\n\n\nNamed metadata to add and log\n\n\n\n\n\n\nThe updated context (invisibly)"
  },
  {
    "objectID": "man/log_metadata.html#log-intermediate-operation-metadata",
    "href": "man/log_metadata.html#log-intermediate-operation-metadata",
    "title": "tima",
    "section": "",
    "text": "Adds metadata to an operation context and logs it.\n\n\n\nlog_metadata(ctx, ...)\n\n\n\n\n\n\n\nctx\n\n\ntima_log_context object from log_operation()\n\n\n\n\n…\n\n\nNamed metadata to add and log\n\n\n\n\n\n\nThe updated context (invisibly)"
  },
  {
    "objectID": "man/annotate_masses.html",
    "href": "man/annotate_masses.html",
    "title": "tima",
    "section": "",
    "text": "This function annotates a feature table based on exact mass match. It requires a structural library, its metadata, and lists of adducts, clusters, and neutral losses to be considered. The polarity has to be pos or neg and retention time and mass tolerances should be given. The feature table is expected to be pre-formatted.\n\n\n\nannotate_masses(\n  features = get_params(step = \"annotate_masses\")\\$files\\$features\\$prepared,\n  output_annotations = get_params(step =\n    \"annotate_masses\")\\$files\\$annotations\\$prepared\\$structural\\$ms1,\n  output_edges = get_params(step =\n    \"annotate_masses\")\\$files\\$networks\\$spectral\\$edges\\$raw\\$ms1,\n  name_source = get_params(step = \"annotate_masses\")\\$names\\$source,\n  name_target = get_params(step = \"annotate_masses\")\\$names\\$target,\n  library = get_params(step = \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  str_stereo = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc,\n  adducts_list = get_params(step = \"annotate_masses\")\\$ms\\$adducts,\n  clusters_list = get_params(step = \"annotate_masses\")\\$ms\\$clusters,\n  neutral_losses_list = get_params(step = \"annotate_masses\")\\$ms\\$neutral_losses,\n  ms_mode = get_params(step = \"annotate_masses\")\\$ms\\$polarity,\n  tolerance_ppm = get_params(step = \"annotate_masses\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms1,\n  tolerance_rt = get_params(step = \"annotate_masses\")\\$ms\\$tolerances\\$rt\\$adducts\n)\n\n\n\n\n\n\n\nfeatures\n\n\nTable containing your previous annotation to complement\n\n\n\n\noutput_annotations\n\n\nOutput for mass based structural annotations\n\n\n\n\noutput_edges\n\n\nOutput for mass based edges\n\n\n\n\nname_source\n\n\nName of the source features column\n\n\n\n\nname_target\n\n\nName of the target features column\n\n\n\n\nlibrary\n\n\nLibrary containing the keys\n\n\n\n\nstr_stereo\n\n\nFile containing structures stereo\n\n\n\n\nstr_met\n\n\nFile containing structures metadata\n\n\n\n\nstr_nam\n\n\nFile containing structures names\n\n\n\n\nstr_tax_cla\n\n\nFile containing Classyfire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nFile containing NPClassifier taxonomy\n\n\n\n\nadducts_list\n\n\nList of adducts to be used\n\n\n\n\nclusters_list\n\n\nList of clusters to be used\n\n\n\n\nneutral_losses_list\n\n\nList of neutral losses to be used\n\n\n\n\nms_mode\n\n\nIonization mode. Must be ‘pos’ or ‘neg’\n\n\n\n\ntolerance_ppm\n\n\nTolerance to perform annotation. Should be &lt;= 20 ppm\n\n\n\n\ntolerance_rt\n\n\nTolerance to group adducts. Should be &lt;= 0.05 minutes\n\n\n\n\n\n\nThe path to the files containing MS1 annotations and edges\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo)\ndir &lt;- paste0(dir, data_interim)\ndir_sop_mer &lt;- paste0(dir, \"libraries/sop/merged/\")\ndir_str &lt;- paste0(dir_sop_mer, \"structures/\")\ndir_tax &lt;- paste0(dir_str, \"taxonomies/\")\nannotate_masses(\n  features = paste0(dir, \"features/example_features.tsv\"),\n  library = paste0(dir_sop_mer, \"keys.tsv\"),\n  str_stereo = paste0(dir_str, \"stereo.tsv\"),\n  str_met = paste0(dir_str, \"metadata.tsv\"),\n  str_nam = paste0(dir_str, \"names.tsv\"),\n  str_tax_cla = paste0(dir_tax, \"classyfire.tsv\"),\n  str_tax_npc = paste0(dir_tax, \"npc.tsv\")\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Annotate",
      "annotate_masses"
    ]
  },
  {
    "objectID": "man/annotate_masses.html#annotate-masses",
    "href": "man/annotate_masses.html#annotate-masses",
    "title": "tima",
    "section": "",
    "text": "This function annotates a feature table based on exact mass match. It requires a structural library, its metadata, and lists of adducts, clusters, and neutral losses to be considered. The polarity has to be pos or neg and retention time and mass tolerances should be given. The feature table is expected to be pre-formatted.\n\n\n\nannotate_masses(\n  features = get_params(step = \"annotate_masses\")\\$files\\$features\\$prepared,\n  output_annotations = get_params(step =\n    \"annotate_masses\")\\$files\\$annotations\\$prepared\\$structural\\$ms1,\n  output_edges = get_params(step =\n    \"annotate_masses\")\\$files\\$networks\\$spectral\\$edges\\$raw\\$ms1,\n  name_source = get_params(step = \"annotate_masses\")\\$names\\$source,\n  name_target = get_params(step = \"annotate_masses\")\\$names\\$target,\n  library = get_params(step = \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  str_stereo = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  str_met = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  str_nam = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  str_tax_cla = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  str_tax_npc = get_params(step =\n    \"annotate_masses\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc,\n  adducts_list = get_params(step = \"annotate_masses\")\\$ms\\$adducts,\n  clusters_list = get_params(step = \"annotate_masses\")\\$ms\\$clusters,\n  neutral_losses_list = get_params(step = \"annotate_masses\")\\$ms\\$neutral_losses,\n  ms_mode = get_params(step = \"annotate_masses\")\\$ms\\$polarity,\n  tolerance_ppm = get_params(step = \"annotate_masses\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms1,\n  tolerance_rt = get_params(step = \"annotate_masses\")\\$ms\\$tolerances\\$rt\\$adducts\n)\n\n\n\n\n\n\n\nfeatures\n\n\nTable containing your previous annotation to complement\n\n\n\n\noutput_annotations\n\n\nOutput for mass based structural annotations\n\n\n\n\noutput_edges\n\n\nOutput for mass based edges\n\n\n\n\nname_source\n\n\nName of the source features column\n\n\n\n\nname_target\n\n\nName of the target features column\n\n\n\n\nlibrary\n\n\nLibrary containing the keys\n\n\n\n\nstr_stereo\n\n\nFile containing structures stereo\n\n\n\n\nstr_met\n\n\nFile containing structures metadata\n\n\n\n\nstr_nam\n\n\nFile containing structures names\n\n\n\n\nstr_tax_cla\n\n\nFile containing Classyfire taxonomy\n\n\n\n\nstr_tax_npc\n\n\nFile containing NPClassifier taxonomy\n\n\n\n\nadducts_list\n\n\nList of adducts to be used\n\n\n\n\nclusters_list\n\n\nList of clusters to be used\n\n\n\n\nneutral_losses_list\n\n\nList of neutral losses to be used\n\n\n\n\nms_mode\n\n\nIonization mode. Must be ‘pos’ or ‘neg’\n\n\n\n\ntolerance_ppm\n\n\nTolerance to perform annotation. Should be &lt;= 20 ppm\n\n\n\n\ntolerance_rt\n\n\nTolerance to group adducts. Should be &lt;= 0.05 minutes\n\n\n\n\n\n\nThe path to the files containing MS1 annotations and edges\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndata_interim &lt;- \"data/interim/\"\ndir &lt;- paste0(github, repo)\ndir &lt;- paste0(dir, data_interim)\ndir_sop_mer &lt;- paste0(dir, \"libraries/sop/merged/\")\ndir_str &lt;- paste0(dir_sop_mer, \"structures/\")\ndir_tax &lt;- paste0(dir_str, \"taxonomies/\")\nannotate_masses(\n  features = paste0(dir, \"features/example_features.tsv\"),\n  library = paste0(dir_sop_mer, \"keys.tsv\"),\n  str_stereo = paste0(dir_str, \"stereo.tsv\"),\n  str_met = paste0(dir_str, \"metadata.tsv\"),\n  str_nam = paste0(dir_str, \"names.tsv\"),\n  str_tax_cla = paste0(dir_tax, \"classyfire.tsv\"),\n  str_tax_npc = paste0(dir_tax, \"npc.tsv\")\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Annotate",
      "annotate_masses"
    ]
  },
  {
    "objectID": "man/prepare_features_tables.html",
    "href": "man/prepare_features_tables.html",
    "title": "tima",
    "section": "",
    "text": "Prepares LC-MS feature tables by standardizing column names, filtering to top-intensity samples per feature, and formatting for downstream analysis. Supports multiple formats (MZmine, SLAW, SIRIUS).\n\n\n\nprepare_features_tables(\n  features = get_params(step = \"prepare_features_tables\")\\$files\\$features\\$raw,\n  output = get_params(step = \"prepare_features_tables\")\\$files\\$features\\$prepared,\n  candidates = get_params(step = \"prepare_features_tables\")\\$annotations\\$canidates\\$samples,\n  name_adduct = get_params(step = \"prepare_features_tables\")\\$names\\$adduct,\n  name_features = get_params(step = \"prepare_features_tables\")\\$names\\$features,\n  name_rt = get_params(step = \"prepare_features_tables\")\\$names\\$rt\\$features,\n  name_mz = get_params(step = \"prepare_features_tables\")\\$names\\$precursor\n)\n\n\n\n\n\n\n\nfeatures\n\n\ncharacter(1) Path to raw features file (CSV/TSV).\n\n\n\n\noutput\n\n\ncharacter(1) Path where prepared features should be saved.\n\n\n\n\ncandidates\n\n\ninteger(1) Number of top-intensity samples to retain per feature (default: from params; recommended ≤5 to balance data size and coverage).\n\n\n\n\nname_adduct\n\n\ncharacter(1) Name of the adduct column in input.\n\n\n\n\nname_features\n\n\ncharacter(1) Name of the feature ID column in input.\n\n\n\n\nname_rt\n\n\ncharacter(1) Name of the retention time column in input.\n\n\n\n\nname_mz\n\n\ncharacter(1) Name of the m/z column in input.\n\n\n\n\n\n\ncharacter(1) Path to the prepared feature table (invisibly).\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$features,\n  export = get_params(step = \"prepare_features_tables\")$files$features$raw\n)\nprepare_features_tables()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_tables"
    ]
  },
  {
    "objectID": "man/prepare_features_tables.html#prepare-features-table",
    "href": "man/prepare_features_tables.html#prepare-features-table",
    "title": "tima",
    "section": "",
    "text": "Prepares LC-MS feature tables by standardizing column names, filtering to top-intensity samples per feature, and formatting for downstream analysis. Supports multiple formats (MZmine, SLAW, SIRIUS).\n\n\n\nprepare_features_tables(\n  features = get_params(step = \"prepare_features_tables\")\\$files\\$features\\$raw,\n  output = get_params(step = \"prepare_features_tables\")\\$files\\$features\\$prepared,\n  candidates = get_params(step = \"prepare_features_tables\")\\$annotations\\$canidates\\$samples,\n  name_adduct = get_params(step = \"prepare_features_tables\")\\$names\\$adduct,\n  name_features = get_params(step = \"prepare_features_tables\")\\$names\\$features,\n  name_rt = get_params(step = \"prepare_features_tables\")\\$names\\$rt\\$features,\n  name_mz = get_params(step = \"prepare_features_tables\")\\$names\\$precursor\n)\n\n\n\n\n\n\n\nfeatures\n\n\ncharacter(1) Path to raw features file (CSV/TSV).\n\n\n\n\noutput\n\n\ncharacter(1) Path where prepared features should be saved.\n\n\n\n\ncandidates\n\n\ninteger(1) Number of top-intensity samples to retain per feature (default: from params; recommended ≤5 to balance data size and coverage).\n\n\n\n\nname_adduct\n\n\ncharacter(1) Name of the adduct column in input.\n\n\n\n\nname_features\n\n\ncharacter(1) Name of the feature ID column in input.\n\n\n\n\nname_rt\n\n\ncharacter(1) Name of the retention time column in input.\n\n\n\n\nname_mz\n\n\ncharacter(1) Name of the m/z column in input.\n\n\n\n\n\n\ncharacter(1) Path to the prepared feature table (invisibly).\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$features,\n  export = get_params(step = \"prepare_features_tables\")$files$features$raw\n)\nprepare_features_tables()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_tables"
    ]
  },
  {
    "objectID": "man/install.html",
    "href": "man/install.html",
    "title": "tima",
    "section": "",
    "text": "Installs or updates the TIMA package from r-universe and sets up a Python virtual environment with dependencies.\n\n\n\ninstall(\n  package = \"tima\",\n  repos = c(\"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\", \"https://cloud.r-project.org\"),\n  dependencies = TRUE\n)\n\n\n\n\n\n\n\npackage\n\n\nCharacter string name of the package (default: \"tima\")\n\n\n\n\nrepos\n\n\nCharacter vector of repository URLs\n\n\n\n\ndependencies\n\n\nLogical whether to install dependencies (default: TRUE)\n\n\n\n\n\n\nNULL (invisibly). Installs packages and sets up Python environment as side effects.\n\n\n\n\nlibrary(\"tima\")\n\ninstall()"
  },
  {
    "objectID": "man/install.html#install-tima-package-and-dependencies",
    "href": "man/install.html#install-tima-package-and-dependencies",
    "title": "tima",
    "section": "",
    "text": "Installs or updates the TIMA package from r-universe and sets up a Python virtual environment with dependencies.\n\n\n\ninstall(\n  package = \"tima\",\n  repos = c(\"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\", \"https://cloud.r-project.org\"),\n  dependencies = TRUE\n)\n\n\n\n\n\n\n\npackage\n\n\nCharacter string name of the package (default: \"tima\")\n\n\n\n\nrepos\n\n\nCharacter vector of repository URLs\n\n\n\n\ndependencies\n\n\nLogical whether to install dependencies (default: TRUE)\n\n\n\n\n\n\nNULL (invisibly). Installs packages and sets up Python environment as side effects.\n\n\n\n\nlibrary(\"tima\")\n\ninstall()"
  },
  {
    "objectID": "man/prepare_libraries_rt.html",
    "href": "man/prepare_libraries_rt.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares retention time libraries by combining experimental and in silico predicted retention times from multiple sources (MGF files, CSV files). It standardizes retention time units, validates structures, and creates both RT libraries and pseudo structure-organism pairs for RT-based annotation.\n\n\n\nprepare_libraries_rt(\n  mgf_exp = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$exp\\$mgf,\n  mgf_is = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$is\\$mgf,\n  temp_exp = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$exp\\$csv,\n  temp_is = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$is\\$csv,\n  output_rt = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$prepared,\n  output_sop = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$sop\\$prepared\\$rt,\n  col_ik = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$inchikey,\n  col_na = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$name,\n  col_rt = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$retention_time,\n  col_sm = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$smiles,\n  name_inchikey = get_params(step = \"prepare_libraries_rt\")\\$names\\$inchikey,\n  name_name = get_params(step = \"prepare_libraries_rt\")\\$names\\$compound_name,\n  name_rt = get_params(step = \"prepare_libraries_rt\")\\$names\\$rt\\$library,\n  name_smiles = get_params(step = \"prepare_libraries_rt\")\\$names\\$smiles,\n  unit_rt = get_params(step = \"prepare_libraries_rt\")\\$units\\$rt\n)\n\n\n\n\n\n\n\nmgf_exp\n\n\nCharacter vector of paths to MGF files with experimental RT\n\n\n\n\nmgf_is\n\n\nCharacter vector of paths to MGF files with in silico predicted RT\n\n\n\n\ntemp_exp\n\n\nCharacter vector of paths to CSV files with experimental RT\n\n\n\n\ntemp_is\n\n\nCharacter vector of paths to CSV files with in silico predicted RT\n\n\n\n\noutput_rt\n\n\nCharacter string path for prepared RT library output\n\n\n\n\noutput_sop\n\n\nCharacter string path for pseudo SOP output\n\n\n\n\ncol_ik\n\n\nCharacter string name of InChIKey column in MGF\n\n\n\n\ncol_na\n\n\nCharacter string name of chompound name column in MGF\n\n\n\n\ncol_rt\n\n\nCharacter string name of retention time column in MGF\n\n\n\n\ncol_sm\n\n\nCharacter string name of SMILES column in MGF\n\n\n\n\nname_inchikey\n\n\nCharacter string name of InChIKey column in CSV\n\n\n\n\nname_name\n\n\nCharacter string name of compound name column in CSV\n\n\n\n\nname_rt\n\n\nCharacter string name of retention time column in CSV\n\n\n\n\nname_smiles\n\n\nCharacter string name of SMILES column in CSV\n\n\n\n\nunit_rt\n\n\nCharacter string RT unit: \"seconds\" or \"minutes\"\n\n\n\n\n\n\nCharacter string path to the prepared retention time library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_rt()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "prepare_libraries_rt"
    ]
  },
  {
    "objectID": "man/prepare_libraries_rt.html#prepare-libraries-of-retention-times",
    "href": "man/prepare_libraries_rt.html#prepare-libraries-of-retention-times",
    "title": "tima",
    "section": "",
    "text": "This function prepares retention time libraries by combining experimental and in silico predicted retention times from multiple sources (MGF files, CSV files). It standardizes retention time units, validates structures, and creates both RT libraries and pseudo structure-organism pairs for RT-based annotation.\n\n\n\nprepare_libraries_rt(\n  mgf_exp = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$exp\\$mgf,\n  mgf_is = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$is\\$mgf,\n  temp_exp = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$exp\\$csv,\n  temp_is = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$is\\$csv,\n  output_rt = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$temporal\\$prepared,\n  output_sop = get_params(step = \"prepare_libraries_rt\")\\$files\\$libraries\\$sop\\$prepared\\$rt,\n  col_ik = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$inchikey,\n  col_na = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$name,\n  col_rt = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$retention_time,\n  col_sm = get_params(step = \"prepare_libraries_rt\")\\$names\\$mgf\\$smiles,\n  name_inchikey = get_params(step = \"prepare_libraries_rt\")\\$names\\$inchikey,\n  name_name = get_params(step = \"prepare_libraries_rt\")\\$names\\$compound_name,\n  name_rt = get_params(step = \"prepare_libraries_rt\")\\$names\\$rt\\$library,\n  name_smiles = get_params(step = \"prepare_libraries_rt\")\\$names\\$smiles,\n  unit_rt = get_params(step = \"prepare_libraries_rt\")\\$units\\$rt\n)\n\n\n\n\n\n\n\nmgf_exp\n\n\nCharacter vector of paths to MGF files with experimental RT\n\n\n\n\nmgf_is\n\n\nCharacter vector of paths to MGF files with in silico predicted RT\n\n\n\n\ntemp_exp\n\n\nCharacter vector of paths to CSV files with experimental RT\n\n\n\n\ntemp_is\n\n\nCharacter vector of paths to CSV files with in silico predicted RT\n\n\n\n\noutput_rt\n\n\nCharacter string path for prepared RT library output\n\n\n\n\noutput_sop\n\n\nCharacter string path for pseudo SOP output\n\n\n\n\ncol_ik\n\n\nCharacter string name of InChIKey column in MGF\n\n\n\n\ncol_na\n\n\nCharacter string name of chompound name column in MGF\n\n\n\n\ncol_rt\n\n\nCharacter string name of retention time column in MGF\n\n\n\n\ncol_sm\n\n\nCharacter string name of SMILES column in MGF\n\n\n\n\nname_inchikey\n\n\nCharacter string name of InChIKey column in CSV\n\n\n\n\nname_name\n\n\nCharacter string name of compound name column in CSV\n\n\n\n\nname_rt\n\n\nCharacter string name of retention time column in CSV\n\n\n\n\nname_smiles\n\n\nCharacter string name of SMILES column in CSV\n\n\n\n\nunit_rt\n\n\nCharacter string RT unit: \"seconds\" or \"minutes\"\n\n\n\n\n\n\nCharacter string path to the prepared retention time library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_rt()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "prepare_libraries_rt"
    ]
  },
  {
    "objectID": "man/weight_annotations.html",
    "href": "man/weight_annotations.html",
    "title": "tima",
    "section": "",
    "text": "This function weights annotations.\n\n\n\nweight_annotations(\n  library = get_params(step = \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  org_tax_ott = get_params(step =\n    \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  str_stereo = get_params(step =\n    \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  annotations = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$filtered,\n  canopus = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$prepared\\$canopus,\n  formula = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$prepared\\$formula,\n  components = get_params(step =\n    \"weight_annotations\")\\$files\\$networks\\$spectral\\$components\\$prepared,\n  edges = get_params(step = \"weight_annotations\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  taxa = get_params(step = \"weight_annotations\")\\$files\\$metadata\\$prepared,\n  output = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$processed,\n  candidates_neighbors = get_params(step =\n    \"weight_annotations\")\\$annotations\\$candidates\\$neighbors,\n  candidates_final = get_params(step = \"weight_annotations\")\\$annotations\\$candidates\\$final,\n  best_percentile = get_params(step =\n    \"weight_annotations\")\\$annotations\\$candidates\\$best_percentile,\n  weight_spectral = get_params(step = \"weight_annotations\")\\$weights\\$global\\$spectral,\n  weight_chemical = get_params(step = \"weight_annotations\")\\$weights\\$global\\$chemical,\n  weight_biological = get_params(step = \"weight_annotations\")\\$weights\\$global\\$biological,\n  score_biological_domain = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$domain,\n  score_biological_kingdom = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$kingdom,\n  score_biological_phylum = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$phylum,\n  score_biological_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$class,\n  score_biological_order = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$order,\n  score_biological_infraorder = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$infraorder,\n  score_biological_family = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$family,\n  score_biological_subfamily = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subfamily,\n  score_biological_tribe = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$tribe,\n  score_biological_subtribe = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subtribe,\n  score_biological_genus = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$genus,\n  score_biological_subgenus = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subgenus,\n  score_biological_species = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$species,\n  score_biological_subspecies = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subspecies,\n  score_biological_variety = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$variety,\n  score_biological_biota = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$biota,\n  score_chemical_cla_kingdom = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$kingdom,\n  score_chemical_cla_superclass = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$superclass,\n  score_chemical_cla_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$class,\n  score_chemical_cla_parent = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$parent,\n  score_chemical_npc_pathway = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$pathway,\n  score_chemical_npc_superclass = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$superclass,\n  score_chemical_npc_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$class,\n  minimal_consistency = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$consistency,\n  minimal_ms1_bio = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$biological,\n  minimal_ms1_chemo = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$chemical,\n  minimal_ms1_condition = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$condition,\n  ms1_only = get_params(step = \"weight_annotations\")\\$annotations\\$ms1only,\n  compounds_names = get_params(step = \"weight_annotations\")\\$options\\$compounds_names,\n  high_confidence = get_params(step = \"weight_annotations\")\\$options\\$high_confidence,\n  remove_ties = get_params(step = \"weight_annotations\")\\$options\\$remove_ties,\n  summarize = get_params(step = \"weight_annotations\")\\$options\\$summarize,\n  pattern = get_params(step = \"weight_annotations\")\\$files\\$pattern,\n  force = get_params(step = \"weight_annotations\")\\$options\\$force\n)\n\n\n\n\n\n\n\nlibrary\n\n\nLibrary containing the keys\n\n\n\n\norg_tax_ott\n\n\nFile containing organisms taxonomy (OTT)\n\n\n\n\nstr_stereo\n\n\nFile containing structures stereo\n\n\n\n\nannotations\n\n\nPrepared annotations file\n\n\n\n\ncanopus\n\n\nPrepared canopus file\n\n\n\n\nformula\n\n\nPrepared formula file\n\n\n\n\ncomponents\n\n\nPrepared components file\n\n\n\n\nedges\n\n\nPrepared edges file\n\n\n\n\ntaxa\n\n\nPrepared taxed features file\n\n\n\n\noutput\n\n\nOutput file\n\n\n\n\ncandidates_neighbors\n\n\nNumber of neighbors candidates to keep\n\n\n\n\ncandidates_final\n\n\nNumber of final candidates to keep\n\n\n\n\nbest_percentile\n\n\nNumeric percentile threshold (0-1) for selecting top candidates within each feature (default: 0.9). Used for consistent filtering between mini and filtered outputs.\n\n\n\n\nweight_spectral\n\n\nWeight for the spectral score\n\n\n\n\nweight_chemical\n\n\nWeight for the biological score\n\n\n\n\nweight_biological\n\n\nWeight for the chemical consistency score\n\n\n\n\nscore_biological_domain\n\n\nScore for a domain match (should be lower than kingdom)\n\n\n\n\nscore_biological_kingdom\n\n\nScore for a kingdom match (should be lower than phylum)\n\n\n\n\nscore_biological_phylum\n\n\nScore for a phylum match (should be lower than class)\n\n\n\n\nscore_biological_class\n\n\nScore for a class match (should be lower than order)\n\n\n\n\nscore_biological_order\n\n\nScore for a order match (should be lower than infraorder)\n\n\n\n\nscore_biological_infraorder\n\n\nScore for a infraorder match (should be lower than order)\n\n\n\n\nscore_biological_family\n\n\nScore for a family match (should be lower than subfamily)\n\n\n\n\nscore_biological_subfamily\n\n\nScore for a subfamily match (should be lower than family)\n\n\n\n\nscore_biological_tribe\n\n\nScore for a tribe match (should be lower than subtribe)\n\n\n\n\nscore_biological_subtribe\n\n\nScore for a subtribe match (should be lower than genus)\n\n\n\n\nscore_biological_genus\n\n\nScore for a genus match (should be lower than subgenus)\n\n\n\n\nscore_biological_subgenus\n\n\nScore for a subgenus match (should be lower than species)\n\n\n\n\nscore_biological_species\n\n\nScore for a species match (should be lower than subspecies)\n\n\n\n\nscore_biological_subspecies\n\n\nScore for a subspecies match (should be lower than variety)\n\n\n\n\nscore_biological_variety\n\n\nScore for a variety match (should be the highest)\n\n\n\n\nscore_biological_biota\n\n\nScore for a Biota match (should be the highest, special)\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nScore for a Classyfire kingdom match (should be lower than  Classyfire superclass)\n\n\n\n\nscore_chemical_cla_superclass\n\n\nScore for a Classyfire superclass match (should be lower than Classyfire class)\n\n\n\n\nscore_chemical_cla_class\n\n\nScore for a Classyfire class match (should be lower than Classyfire parent)\n\n\n\n\nscore_chemical_cla_parent\n\n\nScore for a Classyfire parent match (should be the highest)\n\n\n\n\nscore_chemical_npc_pathway\n\n\nScore for a NPC pathway match (should be lower than  NPC superclass)\n\n\n\n\nscore_chemical_npc_superclass\n\n\nScore for a NPC superclass match (should be lower than NPC class)\n\n\n\n\nscore_chemical_npc_class\n\n\nScore for a NPC class match (should be the highest)\n\n\n\n\nminimal_consistency\n\n\nMinimal consistency score for a class. FLOAT\n\n\n\n\nminimal_ms1_bio\n\n\nMinimal biological score to keep MS1 based annotation\n\n\n\n\nminimal_ms1_chemo\n\n\nMinimal chemical score to keep MS1 based annotation\n\n\n\n\nminimal_ms1_condition\n\n\nCondition to be used. Must be \"OR\" or \"AND\".\n\n\n\n\nms1_only\n\n\nKeep only MS1 annotations. BOOLEAN\n\n\n\n\ncompounds_names\n\n\nReport compounds names. Can be very large. BOOLEAN\n\n\n\n\nhigh_confidence\n\n\nReport high confidence candidates only. BOOLEAN\n\n\n\n\nremove_ties\n\n\nRemove ties. BOOLEAN\n\n\n\n\nsummarize\n\n\nSummarize results (1 row per feature). BOOLEAN\n\n\n\n\npattern\n\n\nPattern to identify your job. STRING\n\n\n\n\nforce\n\n\nForce parameters. Use it at your own risk\n\n\n\n\n\n\nThe path to the weighted annotations\n\n\n\nannotate_masses weight_bio weight_chemo\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nlibrary &lt;- get_params(step = \"weight_annotations\")$files$libraries$sop$merged$keys |&gt;\n  gsub(\n    pattern = \".gz\",\n    replacement = \"\",\n    fixed = TRUE\n  )\norg_tax_ott &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/organisms/taxonomies/ott.tsv\"\n)\nstr_stereo &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/structures/stereo.tsv\"\n)\nannotations &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_annotationsFiltered.tsv\"\n)\ncanopus &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_canopusPrepared.tsv\"\n)\nformula &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_formulaPrepared.tsv\"\n)\ncomponents &lt;- paste0(\n  \"data/interim/features/\",\n  \"example_componentsPrepared.tsv\"\n)\nedges &lt;- paste0(\n  \"data/interim/features/\",\n  \"example_edges.tsv\"\n)\ntaxa &lt;- paste0(\n  \"data/interim/taxa/\",\n  \"example_taxed.tsv\"\n)\nget_file(url = paste0(dir, library), export = library)\nget_file(url = paste0(dir, org_tax_ott), export = org_tax_ott)\nget_file(url = paste0(dir, str_stereo), export = str_stereo)\nget_file(url = paste0(dir, annotations), export = annotations)\nget_file(url = paste0(dir, canopus), export = canopus)\nget_file(url = paste0(dir, formula), export = formula)\nget_file(url = paste0(dir, components), export = components)\nget_file(url = paste0(dir, edges), export = edges)\nget_file(url = paste0(dir, taxa), export = taxa)\nweight_annotations(\n  library = library,\n  org_tax_ott = org_tax_ott,\n  str_stereo = str_stereo,\n  annotations = annotations,\n  canopus = canopus,\n  formula = formula,\n  components = components,\n  edges = edges,\n  taxa = taxa\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Weight",
      "weight_annotations"
    ]
  },
  {
    "objectID": "man/weight_annotations.html#weight-annotations",
    "href": "man/weight_annotations.html#weight-annotations",
    "title": "tima",
    "section": "",
    "text": "This function weights annotations.\n\n\n\nweight_annotations(\n  library = get_params(step = \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  org_tax_ott = get_params(step =\n    \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  str_stereo = get_params(step =\n    \"weight_annotations\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  annotations = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$filtered,\n  canopus = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$prepared\\$canopus,\n  formula = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$prepared\\$formula,\n  components = get_params(step =\n    \"weight_annotations\")\\$files\\$networks\\$spectral\\$components\\$prepared,\n  edges = get_params(step = \"weight_annotations\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  taxa = get_params(step = \"weight_annotations\")\\$files\\$metadata\\$prepared,\n  output = get_params(step = \"weight_annotations\")\\$files\\$annotations\\$processed,\n  candidates_neighbors = get_params(step =\n    \"weight_annotations\")\\$annotations\\$candidates\\$neighbors,\n  candidates_final = get_params(step = \"weight_annotations\")\\$annotations\\$candidates\\$final,\n  best_percentile = get_params(step =\n    \"weight_annotations\")\\$annotations\\$candidates\\$best_percentile,\n  weight_spectral = get_params(step = \"weight_annotations\")\\$weights\\$global\\$spectral,\n  weight_chemical = get_params(step = \"weight_annotations\")\\$weights\\$global\\$chemical,\n  weight_biological = get_params(step = \"weight_annotations\")\\$weights\\$global\\$biological,\n  score_biological_domain = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$domain,\n  score_biological_kingdom = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$kingdom,\n  score_biological_phylum = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$phylum,\n  score_biological_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$class,\n  score_biological_order = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$order,\n  score_biological_infraorder = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$infraorder,\n  score_biological_family = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$family,\n  score_biological_subfamily = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subfamily,\n  score_biological_tribe = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$tribe,\n  score_biological_subtribe = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subtribe,\n  score_biological_genus = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$genus,\n  score_biological_subgenus = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subgenus,\n  score_biological_species = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$species,\n  score_biological_subspecies = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$subspecies,\n  score_biological_variety = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$variety,\n  score_biological_biota = get_params(step =\n    \"weight_annotations\")\\$weights\\$biological\\$biota,\n  score_chemical_cla_kingdom = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$kingdom,\n  score_chemical_cla_superclass = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$superclass,\n  score_chemical_cla_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$class,\n  score_chemical_cla_parent = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$cla\\$parent,\n  score_chemical_npc_pathway = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$pathway,\n  score_chemical_npc_superclass = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$superclass,\n  score_chemical_npc_class = get_params(step =\n    \"weight_annotations\")\\$weights\\$chemical\\$npc\\$class,\n  minimal_consistency = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$consistency,\n  minimal_ms1_bio = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$biological,\n  minimal_ms1_chemo = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$chemical,\n  minimal_ms1_condition = get_params(step =\n    \"weight_annotations\")\\$annotations\\$thresholds\\$ms1\\$condition,\n  ms1_only = get_params(step = \"weight_annotations\")\\$annotations\\$ms1only,\n  compounds_names = get_params(step = \"weight_annotations\")\\$options\\$compounds_names,\n  high_confidence = get_params(step = \"weight_annotations\")\\$options\\$high_confidence,\n  remove_ties = get_params(step = \"weight_annotations\")\\$options\\$remove_ties,\n  summarize = get_params(step = \"weight_annotations\")\\$options\\$summarize,\n  pattern = get_params(step = \"weight_annotations\")\\$files\\$pattern,\n  force = get_params(step = \"weight_annotations\")\\$options\\$force\n)\n\n\n\n\n\n\n\nlibrary\n\n\nLibrary containing the keys\n\n\n\n\norg_tax_ott\n\n\nFile containing organisms taxonomy (OTT)\n\n\n\n\nstr_stereo\n\n\nFile containing structures stereo\n\n\n\n\nannotations\n\n\nPrepared annotations file\n\n\n\n\ncanopus\n\n\nPrepared canopus file\n\n\n\n\nformula\n\n\nPrepared formula file\n\n\n\n\ncomponents\n\n\nPrepared components file\n\n\n\n\nedges\n\n\nPrepared edges file\n\n\n\n\ntaxa\n\n\nPrepared taxed features file\n\n\n\n\noutput\n\n\nOutput file\n\n\n\n\ncandidates_neighbors\n\n\nNumber of neighbors candidates to keep\n\n\n\n\ncandidates_final\n\n\nNumber of final candidates to keep\n\n\n\n\nbest_percentile\n\n\nNumeric percentile threshold (0-1) for selecting top candidates within each feature (default: 0.9). Used for consistent filtering between mini and filtered outputs.\n\n\n\n\nweight_spectral\n\n\nWeight for the spectral score\n\n\n\n\nweight_chemical\n\n\nWeight for the biological score\n\n\n\n\nweight_biological\n\n\nWeight for the chemical consistency score\n\n\n\n\nscore_biological_domain\n\n\nScore for a domain match (should be lower than kingdom)\n\n\n\n\nscore_biological_kingdom\n\n\nScore for a kingdom match (should be lower than phylum)\n\n\n\n\nscore_biological_phylum\n\n\nScore for a phylum match (should be lower than class)\n\n\n\n\nscore_biological_class\n\n\nScore for a class match (should be lower than order)\n\n\n\n\nscore_biological_order\n\n\nScore for a order match (should be lower than infraorder)\n\n\n\n\nscore_biological_infraorder\n\n\nScore for a infraorder match (should be lower than order)\n\n\n\n\nscore_biological_family\n\n\nScore for a family match (should be lower than subfamily)\n\n\n\n\nscore_biological_subfamily\n\n\nScore for a subfamily match (should be lower than family)\n\n\n\n\nscore_biological_tribe\n\n\nScore for a tribe match (should be lower than subtribe)\n\n\n\n\nscore_biological_subtribe\n\n\nScore for a subtribe match (should be lower than genus)\n\n\n\n\nscore_biological_genus\n\n\nScore for a genus match (should be lower than subgenus)\n\n\n\n\nscore_biological_subgenus\n\n\nScore for a subgenus match (should be lower than species)\n\n\n\n\nscore_biological_species\n\n\nScore for a species match (should be lower than subspecies)\n\n\n\n\nscore_biological_subspecies\n\n\nScore for a subspecies match (should be lower than variety)\n\n\n\n\nscore_biological_variety\n\n\nScore for a variety match (should be the highest)\n\n\n\n\nscore_biological_biota\n\n\nScore for a Biota match (should be the highest, special)\n\n\n\n\nscore_chemical_cla_kingdom\n\n\nScore for a Classyfire kingdom match (should be lower than  Classyfire superclass)\n\n\n\n\nscore_chemical_cla_superclass\n\n\nScore for a Classyfire superclass match (should be lower than Classyfire class)\n\n\n\n\nscore_chemical_cla_class\n\n\nScore for a Classyfire class match (should be lower than Classyfire parent)\n\n\n\n\nscore_chemical_cla_parent\n\n\nScore for a Classyfire parent match (should be the highest)\n\n\n\n\nscore_chemical_npc_pathway\n\n\nScore for a NPC pathway match (should be lower than  NPC superclass)\n\n\n\n\nscore_chemical_npc_superclass\n\n\nScore for a NPC superclass match (should be lower than NPC class)\n\n\n\n\nscore_chemical_npc_class\n\n\nScore for a NPC class match (should be the highest)\n\n\n\n\nminimal_consistency\n\n\nMinimal consistency score for a class. FLOAT\n\n\n\n\nminimal_ms1_bio\n\n\nMinimal biological score to keep MS1 based annotation\n\n\n\n\nminimal_ms1_chemo\n\n\nMinimal chemical score to keep MS1 based annotation\n\n\n\n\nminimal_ms1_condition\n\n\nCondition to be used. Must be \"OR\" or \"AND\".\n\n\n\n\nms1_only\n\n\nKeep only MS1 annotations. BOOLEAN\n\n\n\n\ncompounds_names\n\n\nReport compounds names. Can be very large. BOOLEAN\n\n\n\n\nhigh_confidence\n\n\nReport high confidence candidates only. BOOLEAN\n\n\n\n\nremove_ties\n\n\nRemove ties. BOOLEAN\n\n\n\n\nsummarize\n\n\nSummarize results (1 row per feature). BOOLEAN\n\n\n\n\npattern\n\n\nPattern to identify your job. STRING\n\n\n\n\nforce\n\n\nForce parameters. Use it at your own risk\n\n\n\n\n\n\nThe path to the weighted annotations\n\n\n\nannotate_masses weight_bio weight_chemo\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nlibrary &lt;- get_params(step = \"weight_annotations\")$files$libraries$sop$merged$keys |&gt;\n  gsub(\n    pattern = \".gz\",\n    replacement = \"\",\n    fixed = TRUE\n  )\norg_tax_ott &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/organisms/taxonomies/ott.tsv\"\n)\nstr_stereo &lt;- paste0(\n  \"data/interim/libraries/\",\n  \"sop/merged/structures/stereo.tsv\"\n)\nannotations &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_annotationsFiltered.tsv\"\n)\ncanopus &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_canopusPrepared.tsv\"\n)\nformula &lt;- paste0(\n  \"data/interim/annotations/\",\n  \"example_formulaPrepared.tsv\"\n)\ncomponents &lt;- paste0(\n  \"data/interim/features/\",\n  \"example_componentsPrepared.tsv\"\n)\nedges &lt;- paste0(\n  \"data/interim/features/\",\n  \"example_edges.tsv\"\n)\ntaxa &lt;- paste0(\n  \"data/interim/taxa/\",\n  \"example_taxed.tsv\"\n)\nget_file(url = paste0(dir, library), export = library)\nget_file(url = paste0(dir, org_tax_ott), export = org_tax_ott)\nget_file(url = paste0(dir, str_stereo), export = str_stereo)\nget_file(url = paste0(dir, annotations), export = annotations)\nget_file(url = paste0(dir, canopus), export = canopus)\nget_file(url = paste0(dir, formula), export = formula)\nget_file(url = paste0(dir, components), export = components)\nget_file(url = paste0(dir, edges), export = edges)\nget_file(url = paste0(dir, taxa), export = taxa)\nweight_annotations(\n  library = library,\n  org_tax_ott = org_tax_ott,\n  str_stereo = str_stereo,\n  annotations = annotations,\n  canopus = canopus,\n  formula = formula,\n  components = components,\n  edges = edges,\n  taxa = taxa\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Weight",
      "weight_annotations"
    ]
  },
  {
    "objectID": "man/create_edges_spectra.html",
    "href": "man/create_edges_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function creates molecular network edges based on MS2 fragmentation spectra similarity. Compares all spectra against each other using spectral similarity metrics to identify related features.\n\n\n\ncreate_edges_spectra(\n  input = get_params(step = \"create_edges_spectra\")\\$files\\$spectral\\$raw,\n  output = get_params(step =\n    \"create_edges_spectra\")\\$files\\$networks\\$spectral\\$edges\\$raw\\$spectral,\n  name_source = get_params(step = \"create_edges_spectra\")\\$names\\$source,\n  name_target = get_params(step = \"create_edges_spectra\")\\$names\\$target,\n  method = get_params(step = \"create_edges_spectra\")\\$similarities\\$methods\\$edges,\n  threshold = get_params(step = \"create_edges_spectra\")\\$similarities\\$thresholds\\$edges,\n  matched_peaks = get_params(step =\n    \"create_edges_spectra\")\\$similarities\\$thresholds\\$matched_peaks,\n  ppm = get_params(step = \"create_edges_spectra\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms2,\n  dalton = get_params(step = \"create_edges_spectra\")\\$ms\\$tolerances\\$mass\\$dalton\\$ms2,\n  qutoff = get_params(step = \"create_edges_spectra\")\\$ms\\$thresholds\\$ms2\\$intensity\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path or list of paths to query MGF file(s) containing spectra\n\n\n\n\noutput\n\n\nCharacter string path for output edges file\n\n\n\n\nname_source\n\n\nCharacter string name of source feature column\n\n\n\n\nname_target\n\n\nCharacter string name of target feature column\n\n\n\n\nmethod\n\n\nCharacter string similarity method to use\n\n\n\n\nthreshold\n\n\nNumeric minimum similarity threshold (0-1) to report edge\n\n\n\n\nmatched_peaks\n\n\nInteger minimum number of matched peaks required\n\n\n\n\nppm\n\n\nNumeric relative mass tolerance in ppm\n\n\n\n\ndalton\n\n\nNumeric absolute mass tolerance in Daltons\n\n\n\n\nqutoff\n\n\nNumeric intensity cutoff below which MS2 fragments are removed\n\n\n\n\n\n\nCharacter string path to the created spectral edges file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_params(step = \"create_edges_spectra\")$files$spectral$raw\n)\ncreate_edges_spectra()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Create",
      "create_edges_spectra"
    ]
  },
  {
    "objectID": "man/create_edges_spectra.html#create-edges-spectra",
    "href": "man/create_edges_spectra.html#create-edges-spectra",
    "title": "tima",
    "section": "",
    "text": "This function creates molecular network edges based on MS2 fragmentation spectra similarity. Compares all spectra against each other using spectral similarity metrics to identify related features.\n\n\n\ncreate_edges_spectra(\n  input = get_params(step = \"create_edges_spectra\")\\$files\\$spectral\\$raw,\n  output = get_params(step =\n    \"create_edges_spectra\")\\$files\\$networks\\$spectral\\$edges\\$raw\\$spectral,\n  name_source = get_params(step = \"create_edges_spectra\")\\$names\\$source,\n  name_target = get_params(step = \"create_edges_spectra\")\\$names\\$target,\n  method = get_params(step = \"create_edges_spectra\")\\$similarities\\$methods\\$edges,\n  threshold = get_params(step = \"create_edges_spectra\")\\$similarities\\$thresholds\\$edges,\n  matched_peaks = get_params(step =\n    \"create_edges_spectra\")\\$similarities\\$thresholds\\$matched_peaks,\n  ppm = get_params(step = \"create_edges_spectra\")\\$ms\\$tolerances\\$mass\\$ppm\\$ms2,\n  dalton = get_params(step = \"create_edges_spectra\")\\$ms\\$tolerances\\$mass\\$dalton\\$ms2,\n  qutoff = get_params(step = \"create_edges_spectra\")\\$ms\\$thresholds\\$ms2\\$intensity\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path or list of paths to query MGF file(s) containing spectra\n\n\n\n\noutput\n\n\nCharacter string path for output edges file\n\n\n\n\nname_source\n\n\nCharacter string name of source feature column\n\n\n\n\nname_target\n\n\nCharacter string name of target feature column\n\n\n\n\nmethod\n\n\nCharacter string similarity method to use\n\n\n\n\nthreshold\n\n\nNumeric minimum similarity threshold (0-1) to report edge\n\n\n\n\nmatched_peaks\n\n\nInteger minimum number of matched peaks required\n\n\n\n\nppm\n\n\nNumeric relative mass tolerance in ppm\n\n\n\n\ndalton\n\n\nNumeric absolute mass tolerance in Daltons\n\n\n\n\nqutoff\n\n\nNumeric intensity cutoff below which MS2 fragments are removed\n\n\n\n\n\n\nCharacter string path to the created spectral edges file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_params(step = \"create_edges_spectra\")$files$spectral$raw\n)\ncreate_edges_spectra()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Create",
      "create_edges_spectra"
    ]
  },
  {
    "objectID": "man/run_tima.html",
    "href": "man/run_tima.html",
    "title": "tima",
    "section": "",
    "text": "Executes the full Taxonomically Informed Metabolite Annotation (TIMA) workflow from start to finish. This includes data preparation, library loading, annotation, weighting, and output generation. The function runs the targets pipeline and archives logs with timestamps for reproducibility.\n\n\n\nrun_tima(\n  target_pattern = \"^ann_wei\\$\",\n  log_file = \"tima.log\",\n  clean_old_logs = TRUE,\n  log_level = \"info\"\n)\n\n\n\n\n\n\n\ntarget_pattern\n\n\nCharacter. Regex pattern for target selection. Default: \"^ann_wei$\" (annotation preparation target)\n\n\n\n\nlog_file\n\n\nCharacter. Path to log file. Default: \"tima.log\"\n\n\n\n\nclean_old_logs\n\n\nLogical. Remove old log file before starting. Default: TRUE\n\n\n\n\nlog_level\n\n\nCharacter or numeric. Logging verbosity level. Can be one of: \"trace\", \"debug\", \"info\", \"warn\", \"error\", \"fatal\" or numeric values: TRACE=600, DEBUG=500, INFO=400, WARN=300, ERROR=200, FATAL=100. Default: \"info\" (400). Use \"debug\" for detailed troubleshooting.\n\n\n\n\n\n\nThe workflow performs the following steps:\n\n\nInitializes logging and timing\n\n\nNavigates to cache directory\n\n\nExecutes the targets pipeline (annotation preparation)\n\n\nArchives timestamped logs to data/processed/\n\n\n\n\n\nInvisible NULL. Executes workflow as side effect and creates timestamped log files in data/processed/\n\n\n\n\nlibrary(\"tima\")\n\n# Run full workflow with defaults (INFO level)\nrun_tima()\n\n# Run with debug logging for troubleshooting\nrun_tima(log_level = \"debug\")\n\n# Run with minimal logging (warnings and errors only)\nrun_tima(log_level = \"warn\")\n\n# Run with custom target pattern\nrun_tima(target_pattern = \"^prepare_\")\n\n# Preserve existing logs\nrun_tima(clean_old_logs = FALSE)\n\n# Combine multiple options\nrun_tima(\n  target_pattern = \"^ann_\",\n  log_level = \"debug\",\n  clean_old_logs = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Run",
      "run_tima"
    ]
  },
  {
    "objectID": "man/run_tima.html#run-complete-tima-workflow",
    "href": "man/run_tima.html#run-complete-tima-workflow",
    "title": "tima",
    "section": "",
    "text": "Executes the full Taxonomically Informed Metabolite Annotation (TIMA) workflow from start to finish. This includes data preparation, library loading, annotation, weighting, and output generation. The function runs the targets pipeline and archives logs with timestamps for reproducibility.\n\n\n\nrun_tima(\n  target_pattern = \"^ann_wei\\$\",\n  log_file = \"tima.log\",\n  clean_old_logs = TRUE,\n  log_level = \"info\"\n)\n\n\n\n\n\n\n\ntarget_pattern\n\n\nCharacter. Regex pattern for target selection. Default: \"^ann_wei$\" (annotation preparation target)\n\n\n\n\nlog_file\n\n\nCharacter. Path to log file. Default: \"tima.log\"\n\n\n\n\nclean_old_logs\n\n\nLogical. Remove old log file before starting. Default: TRUE\n\n\n\n\nlog_level\n\n\nCharacter or numeric. Logging verbosity level. Can be one of: \"trace\", \"debug\", \"info\", \"warn\", \"error\", \"fatal\" or numeric values: TRACE=600, DEBUG=500, INFO=400, WARN=300, ERROR=200, FATAL=100. Default: \"info\" (400). Use \"debug\" for detailed troubleshooting.\n\n\n\n\n\n\nThe workflow performs the following steps:\n\n\nInitializes logging and timing\n\n\nNavigates to cache directory\n\n\nExecutes the targets pipeline (annotation preparation)\n\n\nArchives timestamped logs to data/processed/\n\n\n\n\n\nInvisible NULL. Executes workflow as side effect and creates timestamped log files in data/processed/\n\n\n\n\nlibrary(\"tima\")\n\n# Run full workflow with defaults (INFO level)\nrun_tima()\n\n# Run with debug logging for troubleshooting\nrun_tima(log_level = \"debug\")\n\n# Run with minimal logging (warnings and errors only)\nrun_tima(log_level = \"warn\")\n\n# Run with custom target pattern\nrun_tima(target_pattern = \"^prepare_\")\n\n# Preserve existing logs\nrun_tima(clean_old_logs = FALSE)\n\n# Combine multiple options\nrun_tima(\n  target_pattern = \"^ann_\",\n  log_level = \"debug\",\n  clean_old_logs = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Run",
      "run_tima"
    ]
  },
  {
    "objectID": "man/prepare_params.html",
    "href": "man/prepare_params.html",
    "title": "tima",
    "section": "",
    "text": "Prepares and validates main parameters for the TIMA workflow. Loads YAML configuration files, extracts all parameters, and sets up the parameter structure for downstream analysis steps.\n\n\n\nprepare_params(\n  params_small = get_params(step = \"prepare_params\"),\n  params_advanced = get_params(step = \"prepare_params_advanced\"),\n  step = NA\n)\n\n\n\n\n\n\n\nparams_small\n\n\nList of basic parameters for the workflow\n\n\n\n\nparams_advanced\n\n\nList of advanced parameters for the workflow\n\n\n\n\nstep\n\n\nWorkflow step identifier (default: NA)\n\n\n\n\n\n\nCharacter vector of paths to YAML files containing prepared parameters\n\n\n\n\nlibrary(\"tima\")\n\n# Prepare parameters for TIMA workflow\nparam_files &lt;- prepare_params(\n  params_small = get_params(step = \"prepare_params\"),\n  params_advanced = get_params(step = \"prepare_params_advanced\")\n)\n\n# Parameters are exported to timestamped files\n# and can be loaded later for reproducibility"
  },
  {
    "objectID": "man/prepare_params.html#prepare-workflow-parameters",
    "href": "man/prepare_params.html#prepare-workflow-parameters",
    "title": "tima",
    "section": "",
    "text": "Prepares and validates main parameters for the TIMA workflow. Loads YAML configuration files, extracts all parameters, and sets up the parameter structure for downstream analysis steps.\n\n\n\nprepare_params(\n  params_small = get_params(step = \"prepare_params\"),\n  params_advanced = get_params(step = \"prepare_params_advanced\"),\n  step = NA\n)\n\n\n\n\n\n\n\nparams_small\n\n\nList of basic parameters for the workflow\n\n\n\n\nparams_advanced\n\n\nList of advanced parameters for the workflow\n\n\n\n\nstep\n\n\nWorkflow step identifier (default: NA)\n\n\n\n\n\n\nCharacter vector of paths to YAML files containing prepared parameters\n\n\n\n\nlibrary(\"tima\")\n\n# Prepare parameters for TIMA workflow\nparam_files &lt;- prepare_params(\n  params_small = get_params(step = \"prepare_params\"),\n  params_advanced = get_params(step = \"prepare_params_advanced\")\n)\n\n# Parameters are exported to timestamped files\n# and can be loaded later for reproducibility"
  },
  {
    "objectID": "man/import_spectra.html",
    "href": "man/import_spectra.html",
    "title": "tima",
    "section": "",
    "text": "This function imports mass spectra from various file formats (.mgf, .msp, .rds), harmonizes metadata field names, filters by MS level and polarity, optionally combines replicate spectra, and sanitizes peak data.\n\n\n\nimport_spectra(\n  file,\n  cutoff = NULL,\n  dalton = 0.01,\n  polarity = NA,\n  ppm = 10,\n  sanitize = TRUE,\n  combine = TRUE\n)\n\n\n\n\n\n\n\nfile\n\n\nCharacter string path to the spectrum file (.mgf, .msp, or .rds)\n\n\n\n\ncutoff\n\n\nNumeric absolute minimal intensity threshold (default: NULL)\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching (default: 0.01)\n\n\n\n\npolarity\n\n\nCharacter string for polarity filtering: \"pos\", \"neg\", or NA to keep all (default: NA)\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching (default: 10)\n\n\n\n\nsanitize\n\n\nLogical flag indicating whether to sanitize spectra (default: TRUE)\n\n\n\n\ncombine\n\n\nLogical flag indicating whether to combine replicate spectra (default: TRUE)\n\n\n\n\n\n\nSpectra object containing the imported and processed spectra\n\n\n\n\nlibrary(\"tima\")\n\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_default_paths()$data$source$spectra\n)\nimport_spectra(file = get_default_paths()$data$source$spectra)\nimport_spectra(\n  file = get_default_paths()$data$source$spectra,\n  sanitize = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Import",
      "import_spectra"
    ]
  },
  {
    "objectID": "man/import_spectra.html#import-spectra",
    "href": "man/import_spectra.html#import-spectra",
    "title": "tima",
    "section": "",
    "text": "This function imports mass spectra from various file formats (.mgf, .msp, .rds), harmonizes metadata field names, filters by MS level and polarity, optionally combines replicate spectra, and sanitizes peak data.\n\n\n\nimport_spectra(\n  file,\n  cutoff = NULL,\n  dalton = 0.01,\n  polarity = NA,\n  ppm = 10,\n  sanitize = TRUE,\n  combine = TRUE\n)\n\n\n\n\n\n\n\nfile\n\n\nCharacter string path to the spectrum file (.mgf, .msp, or .rds)\n\n\n\n\ncutoff\n\n\nNumeric absolute minimal intensity threshold (default: NULL)\n\n\n\n\ndalton\n\n\nNumeric Dalton tolerance for peak matching (default: 0.01)\n\n\n\n\npolarity\n\n\nCharacter string for polarity filtering: \"pos\", \"neg\", or NA to keep all (default: NA)\n\n\n\n\nppm\n\n\nNumeric PPM tolerance for peak matching (default: 10)\n\n\n\n\nsanitize\n\n\nLogical flag indicating whether to sanitize spectra (default: TRUE)\n\n\n\n\ncombine\n\n\nLogical flag indicating whether to combine replicate spectra (default: TRUE)\n\n\n\n\n\n\nSpectra object containing the imported and processed spectra\n\n\n\n\nlibrary(\"tima\")\n\nget_file(\n  url = get_default_paths()$urls$examples$spectra_mini,\n  export = get_default_paths()$data$source$spectra\n)\nimport_spectra(file = get_default_paths()$data$source$spectra)\nimport_spectra(\n  file = get_default_paths()$data$source$spectra,\n  sanitize = FALSE\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Import",
      "import_spectra"
    ]
  },
  {
    "objectID": "man/sanitize_all_inputs.html",
    "href": "man/sanitize_all_inputs.html",
    "title": "tima",
    "section": "",
    "text": "Comprehensive validation of all input data before starting expensive processing. Reports issues immediately to save time.\n\n\n\nsanitize_all_inputs(\n  features_file = NULL,\n  mgf_file = NULL,\n  metadata_file = NULL,\n  sirius_dir = NULL,\n  filename_col = \"filename\",\n  organism_col = \"organism\",\n  feature_col = \"feature_id\"\n)\n\n\n\n\n\n\n\nfeatures_file\n\n\nCharacter path to features CSV/TSV (optional)\n\n\n\n\nmgf_file\n\n\nCharacter path to MGF file (optional)\n\n\n\n\nmetadata_file\n\n\nCharacter path to metadata file (optional)\n\n\n\n\nsirius_dir\n\n\nCharacter path to SIRIUS output directory or ZIP (optional)\n\n\n\n\nfilename_col\n\n\nCharacter name of filename column (default: \"filename\")\n\n\n\n\norganism_col\n\n\nCharacter name of organism column (default: \"organism\")\n\n\n\n\nfeature_col\n\n\nCharacter name of feature ID column (default: \"feature_id\")\n\n\n\n\n\n\nInvisible TRUE if all validations pass, stops with error otherwise\n\n\n\n\nlibrary(\"tima\")\n\n# Validate all inputs before starting pipeline\nsanitize_all_inputs(\n  features_file = \"data/features.csv\",\n  mgf_file = \"data/spectra.mgf\",\n  metadata_file = \"data/metadata.tsv\",\n  sirius_dir = \"data/sirius_output\"\n)"
  },
  {
    "objectID": "man/sanitize_all_inputs.html#sanitize-all-input-data",
    "href": "man/sanitize_all_inputs.html#sanitize-all-input-data",
    "title": "tima",
    "section": "",
    "text": "Comprehensive validation of all input data before starting expensive processing. Reports issues immediately to save time.\n\n\n\nsanitize_all_inputs(\n  features_file = NULL,\n  mgf_file = NULL,\n  metadata_file = NULL,\n  sirius_dir = NULL,\n  filename_col = \"filename\",\n  organism_col = \"organism\",\n  feature_col = \"feature_id\"\n)\n\n\n\n\n\n\n\nfeatures_file\n\n\nCharacter path to features CSV/TSV (optional)\n\n\n\n\nmgf_file\n\n\nCharacter path to MGF file (optional)\n\n\n\n\nmetadata_file\n\n\nCharacter path to metadata file (optional)\n\n\n\n\nsirius_dir\n\n\nCharacter path to SIRIUS output directory or ZIP (optional)\n\n\n\n\nfilename_col\n\n\nCharacter name of filename column (default: \"filename\")\n\n\n\n\norganism_col\n\n\nCharacter name of organism column (default: \"organism\")\n\n\n\n\nfeature_col\n\n\nCharacter name of feature ID column (default: \"feature_id\")\n\n\n\n\n\n\nInvisible TRUE if all validations pass, stops with error otherwise\n\n\n\n\nlibrary(\"tima\")\n\n# Validate all inputs before starting pipeline\nsanitize_all_inputs(\n  features_file = \"data/features.csv\",\n  mgf_file = \"data/spectra.mgf\",\n  metadata_file = \"data/metadata.tsv\",\n  sirius_dir = \"data/sirius_output\"\n)"
  },
  {
    "objectID": "man/prepare_libraries_spectra.html",
    "href": "man/prepare_libraries_spectra.html",
    "title": "tima",
    "section": "",
    "text": "Prepares spectral libraries for matching by importing, harmonizing, and splitting spectra by polarity. Exports results as Spectra RDS files (pos/neg) and a structure-organism pair (SOP) table.\n\n\n\nprepare_libraries_spectra(\n  input = get_params(step = \"prepare_libraries_spectra\")\\$files\\$libraries\\$spectral\\$raw,\n  nam_lib = get_params(step = \"prepare_libraries_spectra\")\\$names\\$libraries,\n  col_ad = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$adduct,\n  col_ce = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$collision_energy,\n  col_ci = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$compound_id,\n  col_em = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$exact_mass,\n  col_in = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchi,\n  col_io = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchi_no_stereo,\n  col_ik = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchikey,\n  col_il = get_params(step =\n    \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchikey_connectivity_layer,\n  col_mf = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$molecular_formula,\n  col_na = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$name,\n  col_po = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$polarity,\n  col_sm = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$smiles,\n  col_sn = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$smiles_no_stereo,\n  col_si = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$spectrum_id,\n  col_sp = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$splash,\n  col_sy = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$synonyms,\n  col_xl = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$xlogp\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of file paths containing spectral data.\n\n\n\n\nnam_lib\n\n\nCharacter library name for metadata.\n\n\n\n\ncol_ad\n\n\nName of the adduct column in MGF.\n\n\n\n\ncol_ce\n\n\nName of the collision energy column in MGF.\n\n\n\n\ncol_ci\n\n\nName of the compound ID column in MGF.\n\n\n\n\ncol_em\n\n\nName of the exact mass column in MGF.\n\n\n\n\ncol_in\n\n\nName of the InChI column in MGF.\n\n\n\n\ncol_io\n\n\nName of the InChI without stereo column in MGF.\n\n\n\n\ncol_ik\n\n\nName of the InChIKey column in MGF.\n\n\n\n\ncol_il\n\n\nName of the InChIKey connectivity layer column in MGF.\n\n\n\n\ncol_mf\n\n\nName of the molecular formula column in MGF.\n\n\n\n\ncol_na\n\n\nName of the name column in MGF.\n\n\n\n\ncol_po\n\n\nName of the polarity column in MGF.\n\n\n\n\ncol_sm\n\n\nName of the SMILES column in MGF.\n\n\n\n\ncol_sn\n\n\nName of the SMILES without stereo column in MGF.\n\n\n\n\ncol_si\n\n\nName of the spectrum ID column in MGF.\n\n\n\n\ncol_sp\n\n\nName of the SPLASH column in MGF.\n\n\n\n\ncol_sy\n\n\nName of the synonyms column in MGF.\n\n\n\n\ncol_xl\n\n\nName of the xlogp column in MGF.\n\n\n\n\n\n\nThis function:\n\n\nChecks if output files already exist (idempotent).\n\n\nImports spectral data from input files.\n\n\nExtracts and harmonizes spectra for positive and negative modes.\n\n\nFixes precursor m/z and InChIKey connectivity layer issues.\n\n\nExports polarity-specific Spectra objects and SOP table.\n\n\nReturns empty templates if input files are missing.\n\n\n\n\n\nCharacter vector with paths to prepared library files (invisible).\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_spectra()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "prepare_libraries_spectra"
    ]
  },
  {
    "objectID": "man/prepare_libraries_spectra.html#prepare-libraries-of-spectra",
    "href": "man/prepare_libraries_spectra.html#prepare-libraries-of-spectra",
    "title": "tima",
    "section": "",
    "text": "Prepares spectral libraries for matching by importing, harmonizing, and splitting spectra by polarity. Exports results as Spectra RDS files (pos/neg) and a structure-organism pair (SOP) table.\n\n\n\nprepare_libraries_spectra(\n  input = get_params(step = \"prepare_libraries_spectra\")\\$files\\$libraries\\$spectral\\$raw,\n  nam_lib = get_params(step = \"prepare_libraries_spectra\")\\$names\\$libraries,\n  col_ad = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$adduct,\n  col_ce = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$collision_energy,\n  col_ci = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$compound_id,\n  col_em = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$exact_mass,\n  col_in = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchi,\n  col_io = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchi_no_stereo,\n  col_ik = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchikey,\n  col_il = get_params(step =\n    \"prepare_libraries_spectra\")\\$names\\$mgf\\$inchikey_connectivity_layer,\n  col_mf = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$molecular_formula,\n  col_na = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$name,\n  col_po = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$polarity,\n  col_sm = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$smiles,\n  col_sn = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$smiles_no_stereo,\n  col_si = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$spectrum_id,\n  col_sp = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$splash,\n  col_sy = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$synonyms,\n  col_xl = get_params(step = \"prepare_libraries_spectra\")\\$names\\$mgf\\$xlogp\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter vector of file paths containing spectral data.\n\n\n\n\nnam_lib\n\n\nCharacter library name for metadata.\n\n\n\n\ncol_ad\n\n\nName of the adduct column in MGF.\n\n\n\n\ncol_ce\n\n\nName of the collision energy column in MGF.\n\n\n\n\ncol_ci\n\n\nName of the compound ID column in MGF.\n\n\n\n\ncol_em\n\n\nName of the exact mass column in MGF.\n\n\n\n\ncol_in\n\n\nName of the InChI column in MGF.\n\n\n\n\ncol_io\n\n\nName of the InChI without stereo column in MGF.\n\n\n\n\ncol_ik\n\n\nName of the InChIKey column in MGF.\n\n\n\n\ncol_il\n\n\nName of the InChIKey connectivity layer column in MGF.\n\n\n\n\ncol_mf\n\n\nName of the molecular formula column in MGF.\n\n\n\n\ncol_na\n\n\nName of the name column in MGF.\n\n\n\n\ncol_po\n\n\nName of the polarity column in MGF.\n\n\n\n\ncol_sm\n\n\nName of the SMILES column in MGF.\n\n\n\n\ncol_sn\n\n\nName of the SMILES without stereo column in MGF.\n\n\n\n\ncol_si\n\n\nName of the spectrum ID column in MGF.\n\n\n\n\ncol_sp\n\n\nName of the SPLASH column in MGF.\n\n\n\n\ncol_sy\n\n\nName of the synonyms column in MGF.\n\n\n\n\ncol_xl\n\n\nName of the xlogp column in MGF.\n\n\n\n\n\n\nThis function:\n\n\nChecks if output files already exist (idempotent).\n\n\nImports spectral data from input files.\n\n\nExtracts and harmonizes spectra for positive and negative modes.\n\n\nFixes precursor m/z and InChIKey connectivity layer issues.\n\n\nExports polarity-specific Spectra objects and SOP table.\n\n\nReturns empty templates if input files are missing.\n\n\n\n\n\nCharacter vector with paths to prepared library files (invisible).\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_spectra()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "prepare_libraries_spectra"
    ]
  },
  {
    "objectID": "man/process_smiles.html",
    "href": "man/process_smiles.html",
    "title": "tima",
    "section": "",
    "text": "Processes SMILES using RDKit (via Python) to standardize structures, generate InChIKeys, calculate molecular properties, and extract 2D representations. Results are cached to avoid reprocessing.\n\n\n\nprocess_smiles(df, smiles_colname = \"structure_smiles_initial\", cache = NULL)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing SMILES strings\n\n\n\n\nsmiles_colname\n\n\nColumn name containing SMILES (default: \"structure_smiles_initial\")\n\n\n\n\ncache\n\n\nPath to cached processed SMILES file, or NULL to skip caching\n\n\n\n\n\n\nData frame with processed SMILES including InChIKey, molecular formula (with isotopes shown), exact mass (with isotope contributions), 2D SMILES, xLogP, and connectivity layer\n\n\n\n\nlibrary(\"tima\")\n\n# Natural compound\ndf &lt;- data.frame(\n  structure_smiles_initial = \"OC[C@H]1OC(O)[C@H](O)[C@H](O)[C@H]1O\"\n)\nresult &lt;- process_smiles(df)\n# Formula: C6H12O6, Mass: 180.063 Da\n\n# Isotope-labeled compound (4× 13C)\ndf_labeled &lt;- data.frame(\n  structure_smiles_initial = \"OC[13C@H]1OC(O)[13C@H](O)[13C@H](O)[13C@H]1O\"\n)\nresult_labeled &lt;- process_smiles(df_labeled)\n# Formula: C2[13C]4H12O6 (isotopes shown separately)\n# Mass: 184.077 Da (difference of ~4.013 Da from natural)\n# SMILES preserves [13C] notation\n# InChIKey differs from natural glucose",
    "crumbs": [
      "Get started",
      "Functions",
      "Process",
      "process_smiles"
    ]
  },
  {
    "objectID": "man/process_smiles.html#process-smiles-strings",
    "href": "man/process_smiles.html#process-smiles-strings",
    "title": "tima",
    "section": "",
    "text": "Processes SMILES using RDKit (via Python) to standardize structures, generate InChIKeys, calculate molecular properties, and extract 2D representations. Results are cached to avoid reprocessing.\n\n\n\nprocess_smiles(df, smiles_colname = \"structure_smiles_initial\", cache = NULL)\n\n\n\n\n\n\n\ndf\n\n\nData frame containing SMILES strings\n\n\n\n\nsmiles_colname\n\n\nColumn name containing SMILES (default: \"structure_smiles_initial\")\n\n\n\n\ncache\n\n\nPath to cached processed SMILES file, or NULL to skip caching\n\n\n\n\n\n\nData frame with processed SMILES including InChIKey, molecular formula (with isotopes shown), exact mass (with isotope contributions), 2D SMILES, xLogP, and connectivity layer\n\n\n\n\nlibrary(\"tima\")\n\n# Natural compound\ndf &lt;- data.frame(\n  structure_smiles_initial = \"OC[C@H]1OC(O)[C@H](O)[C@H](O)[C@H]1O\"\n)\nresult &lt;- process_smiles(df)\n# Formula: C6H12O6, Mass: 180.063 Da\n\n# Isotope-labeled compound (4× 13C)\ndf_labeled &lt;- data.frame(\n  structure_smiles_initial = \"OC[13C@H]1OC(O)[13C@H](O)[13C@H](O)[13C@H]1O\"\n)\nresult_labeled &lt;- process_smiles(df_labeled)\n# Formula: C2[13C]4H12O6 (isotopes shown separately)\n# Mass: 184.077 Da (difference of ~4.013 Da from natural)\n# SMILES preserves [13C] notation\n# InChIKey differs from natural glucose",
    "crumbs": [
      "Get started",
      "Functions",
      "Process",
      "process_smiles"
    ]
  },
  {
    "objectID": "man/prepare_features_edges.html",
    "href": "man/prepare_features_edges.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares molecular network edges by combining MS1-based and spectral similarity edges, adding entropy information, and standardizing column names. Edges represent relationships between features in the molecular network.\n\n\n\nprepare_features_edges(\n  input = get_params(step = \"prepare_features_edges\")\\$files\\$networks\\$spectral\\$edges\\$raw,\n  output = get_params(step =\n    \"prepare_features_edges\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  name_source = get_params(step = \"prepare_features_edges\")\\$names\\$source,\n  name_target = get_params(step = \"prepare_features_edges\")\\$names\\$target\n)\n\n\n\n\n\n\n\ninput\n\n\nNamed list containing paths to edge files. Must have \"ms1\" and \"spectral\" elements pointing to respective edge files.\n\n\n\n\noutput\n\n\nCharacter string path where prepared edges should be saved\n\n\n\n\nname_source\n\n\nCharacter string name of the source feature column in input files\n\n\n\n\nname_target\n\n\nCharacter string name of the target feature column in input files\n\n\n\n\n\n\nCharacter string path to the prepared edges file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\ninput_1 &lt;- get_params(step = \"prepare_features_edges\")$files$networks$spectral$edges$raw$ms1\ninput_2 &lt;- get_params(step = \"prepare_features_edges\")$files$networks$spectral$edges$raw$spectral\nget_file(url = paste0(dir, input_1), export = input_1)\nget_file(url = paste0(dir, input_2), export = input_2)\nprepare_features_edges(\n  input = list(\"ms1\" = input_1, \"spectral\" = input_2)\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_edges"
    ]
  },
  {
    "objectID": "man/prepare_features_edges.html#prepare-features-edges",
    "href": "man/prepare_features_edges.html#prepare-features-edges",
    "title": "tima",
    "section": "",
    "text": "This function prepares molecular network edges by combining MS1-based and spectral similarity edges, adding entropy information, and standardizing column names. Edges represent relationships between features in the molecular network.\n\n\n\nprepare_features_edges(\n  input = get_params(step = \"prepare_features_edges\")\\$files\\$networks\\$spectral\\$edges\\$raw,\n  output = get_params(step =\n    \"prepare_features_edges\")\\$files\\$networks\\$spectral\\$edges\\$prepared,\n  name_source = get_params(step = \"prepare_features_edges\")\\$names\\$source,\n  name_target = get_params(step = \"prepare_features_edges\")\\$names\\$target\n)\n\n\n\n\n\n\n\ninput\n\n\nNamed list containing paths to edge files. Must have \"ms1\" and \"spectral\" elements pointing to respective edge files.\n\n\n\n\noutput\n\n\nCharacter string path where prepared edges should be saved\n\n\n\n\nname_source\n\n\nCharacter string name of the source feature column in input files\n\n\n\n\nname_target\n\n\nCharacter string name of the target feature column in input files\n\n\n\n\n\n\nCharacter string path to the prepared edges file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\ninput_1 &lt;- get_params(step = \"prepare_features_edges\")$files$networks$spectral$edges$raw$ms1\ninput_2 &lt;- get_params(step = \"prepare_features_edges\")$files$networks$spectral$edges$raw$spectral\nget_file(url = paste0(dir, input_1), export = input_1)\nget_file(url = paste0(dir, input_2), export = input_2)\nprepare_features_edges(\n  input = list(\"ms1\" = input_1, \"spectral\" = input_2)\n)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Features",
      "prepare_features_edges"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_ecmdb.html",
    "href": "man/prepare_libraries_sop_ecmdb.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares ECMDB (E. coli Metabolome Database) structure-organism pairs by parsing JSON data, extracting metabolite information, and formatting for TIMA workflows. Handles E. coli metabolite data with structures.\n\n\n\nprepare_libraries_sop_ecmdb(\n  input = get_params(step = \"prepare_libraries_sop_ecmdb\")\\$files\\$libraries\\$sop\\$raw\\$ecmdb,\n  output = get_params(step =\n    \"prepare_libraries_sop_ecmdb\")\\$files\\$libraries\\$sop\\$prepared\\$ecmdb\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to ECMDB JSON zip file\n\n\n\n\noutput\n\n\nCharacter string path for prepared ECMDB library output\n\n\n\n\n\n\nCharacter string path to prepared ECMDB structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_ecmdb()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_ecmdb"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_ecmdb.html#prepare-libraries-of-structure-organism-pairs-ecmdb",
    "href": "man/prepare_libraries_sop_ecmdb.html#prepare-libraries-of-structure-organism-pairs-ecmdb",
    "title": "tima",
    "section": "",
    "text": "This function prepares ECMDB (E. coli Metabolome Database) structure-organism pairs by parsing JSON data, extracting metabolite information, and formatting for TIMA workflows. Handles E. coli metabolite data with structures.\n\n\n\nprepare_libraries_sop_ecmdb(\n  input = get_params(step = \"prepare_libraries_sop_ecmdb\")\\$files\\$libraries\\$sop\\$raw\\$ecmdb,\n  output = get_params(step =\n    \"prepare_libraries_sop_ecmdb\")\\$files\\$libraries\\$sop\\$prepared\\$ecmdb\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to ECMDB JSON zip file\n\n\n\n\noutput\n\n\nCharacter string path for prepared ECMDB library output\n\n\n\n\n\n\nCharacter string path to prepared ECMDB structure-organism pairs\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_ecmdb()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_ecmdb"
    ]
  },
  {
    "objectID": "man/get_last_version_from_zenodo.html",
    "href": "man/get_last_version_from_zenodo.html",
    "title": "tima",
    "section": "",
    "text": "Retrieves the latest version of a file from a Zenodo repository record. This function checks the file size and only downloads if the local file is missing or differs from the remote version. Implements robust error handling and retry logic.\n\n\n\nget_last_version_from_zenodo(doi, pattern, path)\n\n\n\n\n\n\n\ndoi\n\n\nCharacter. Zenodo DOI (e.g., \"10.5281/zenodo.5794106\")\n\n\n\n\npattern\n\n\nCharacter. Pattern to identify the specific file to download\n\n\n\n\npath\n\n\nCharacter. Local path where the file should be saved\n\n\n\n\n\n\nCredit goes partially to https://inbo.github.io/inborutils/\nThis function:\n\n\nValidates DOI format and input parameters\n\n\nFetches the latest version metadata from Zenodo API\n\n\nFinds files matching the specified pattern\n\n\nCompares local and remote file sizes to avoid unnecessary downloads\n\n\nDownloads only if needed, with retry logic\n\n\nCreates necessary directories automatically\n\n\n\n\n\nCharacter path to the downloaded (or existing) file\n\n\n\n\nlibrary(\"tima\")\n\n# Download LOTUS database from Zenodo\nget_last_version_from_zenodo(\n  doi = \"10.5281/zenodo.5794106\",\n  pattern = \"lotus.csv.gz\",\n  path = \"data/source/libraries/sop/lotus.csv.gz\"\n)\n\n# The function will skip download if file exists with correct size\nget_last_version_from_zenodo(\n  doi = \"10.5281/zenodo.5794106\",\n  pattern = \"lotus.csv.gz\",\n  path = \"data/source/libraries/sop/lotus.csv.gz\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_last_version_from_zenodo"
    ]
  },
  {
    "objectID": "man/get_last_version_from_zenodo.html#get-latest-version-from-zenodo",
    "href": "man/get_last_version_from_zenodo.html#get-latest-version-from-zenodo",
    "title": "tima",
    "section": "",
    "text": "Retrieves the latest version of a file from a Zenodo repository record. This function checks the file size and only downloads if the local file is missing or differs from the remote version. Implements robust error handling and retry logic.\n\n\n\nget_last_version_from_zenodo(doi, pattern, path)\n\n\n\n\n\n\n\ndoi\n\n\nCharacter. Zenodo DOI (e.g., \"10.5281/zenodo.5794106\")\n\n\n\n\npattern\n\n\nCharacter. Pattern to identify the specific file to download\n\n\n\n\npath\n\n\nCharacter. Local path where the file should be saved\n\n\n\n\n\n\nCredit goes partially to https://inbo.github.io/inborutils/\nThis function:\n\n\nValidates DOI format and input parameters\n\n\nFetches the latest version metadata from Zenodo API\n\n\nFinds files matching the specified pattern\n\n\nCompares local and remote file sizes to avoid unnecessary downloads\n\n\nDownloads only if needed, with retry logic\n\n\nCreates necessary directories automatically\n\n\n\n\n\nCharacter path to the downloaded (or existing) file\n\n\n\n\nlibrary(\"tima\")\n\n# Download LOTUS database from Zenodo\nget_last_version_from_zenodo(\n  doi = \"10.5281/zenodo.5794106\",\n  pattern = \"lotus.csv.gz\",\n  path = \"data/source/libraries/sop/lotus.csv.gz\"\n)\n\n# The function will skip download if file exists with correct size\nget_last_version_from_zenodo(\n  doi = \"10.5281/zenodo.5794106\",\n  pattern = \"lotus.csv.gz\",\n  path = \"data/source/libraries/sop/lotus.csv.gz\"\n)",
    "crumbs": [
      "Get started",
      "Functions",
      "Get",
      "get_last_version_from_zenodo"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_lotus.html",
    "href": "man/prepare_libraries_sop_lotus.html",
    "title": "tima",
    "section": "",
    "text": "This function prepares the LOTUS (LOng-lasting, cUraTed collection of cOnnectivity daTa for natural products) structure-organism pairs database. It standardizes columns, extracts 2D InChIKeys, rounds numeric values, and removes duplicates.\n\n\n\nprepare_libraries_sop_lotus(\n  input = get_params(step = \"prepare_libraries_sop_lotus\")\\$files\\$libraries\\$sop\\$raw\\$lotus,\n  output = get_params(step =\n    \"prepare_libraries_sop_lotus\")\\$files\\$libraries\\$sop\\$prepared\\$lotus\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to the raw LOTUS data file\n\n\n\n\noutput\n\n\nCharacter string path for the prepared output file\n\n\n\n\n\n\nCharacter string path to the prepared structure-organism pairs library file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_lotus()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_lotus"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_lotus.html#prepare-libraries-of-structure-organism-pairs-lotus",
    "href": "man/prepare_libraries_sop_lotus.html#prepare-libraries-of-structure-organism-pairs-lotus",
    "title": "tima",
    "section": "",
    "text": "This function prepares the LOTUS (LOng-lasting, cUraTed collection of cOnnectivity daTa for natural products) structure-organism pairs database. It standardizes columns, extracts 2D InChIKeys, rounds numeric values, and removes duplicates.\n\n\n\nprepare_libraries_sop_lotus(\n  input = get_params(step = \"prepare_libraries_sop_lotus\")\\$files\\$libraries\\$sop\\$raw\\$lotus,\n  output = get_params(step =\n    \"prepare_libraries_sop_lotus\")\\$files\\$libraries\\$sop\\$prepared\\$lotus\n)\n\n\n\n\n\n\n\ninput\n\n\nCharacter string path to the raw LOTUS data file\n\n\n\n\noutput\n\n\nCharacter string path for the prepared output file\n\n\n\n\n\n\nCharacter string path to the prepared structure-organism pairs library file\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\nprepare_libraries_sop_lotus()\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_lotus"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_merged.html",
    "href": "man/prepare_libraries_sop_merged.html",
    "title": "tima",
    "section": "",
    "text": "This function merges all structure-organism pair libraries (LOTUS, HMDB, ECMDB, etc.) into a single comprehensive library. Can optionally filter by taxonomic level to create biologically-focused subsets. Also splits structures into separate metadata tables.\n\n\n\nprepare_libraries_sop_merged(\n  files = get_params(step = \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$prepared,\n  filter = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$mode,\n  level = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$level,\n  value = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$value,\n  cache = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$processed,\n  output_key = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  output_org_tax_ott = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  output_str_stereo = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  output_str_met = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  output_str_nam = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  output_str_tax_cla = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  output_str_tax_npc = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\nfiles\n\n\nCharacter vector or list of paths to prepared library files\n\n\n\n\nfilter\n\n\nLogical whether to filter the merged library by taxonomy\n\n\n\n\nlevel\n\n\nCharacter string taxonomic rank for filtering (kingdom, phylum, family, genus, etc.)\n\n\n\n\nvalue\n\n\nCharacter string taxon name(s) to keep (can use | for multiple, e.g., ‘Gentianaceae|Apocynaceae’)\n\n\n\n\ncache\n\n\nCharacter string path to cache directory for processed SMILES\n\n\n\n\noutput_key\n\n\nCharacter string path for output keys file\n\n\n\n\noutput_org_tax_ott\n\n\nCharacter string path for organisms taxonomy (OTT) file\n\n\n\n\noutput_str_stereo\n\n\nCharacter string path for structures stereochemistry file\n\n\n\n\noutput_str_met\n\n\nCharacter string path for structures metadata file\n\n\n\n\noutput_str_nam\n\n\nCharacter string path for structures names file\n\n\n\n\noutput_str_tax_cla\n\n\nCharacter string path for ClassyFire taxonomy file\n\n\n\n\noutput_str_tax_npc\n\n\nCharacter string path for NPClassifier taxonomy file\n\n\n\n\n\n\nCreates merged library by combining all available SOP sources, optionally filtering by taxonomic criteria (e.g., only Gentianaceae). Splits output into structures metadata, names, taxonomy, and organisms.\n\n\n\nCharacter string path to the prepared merged SOP library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nfiles &lt;- get_params(step = \"prepare_libraries_sop_merged\")$files$libraries$sop$prepared$lotus |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nget_file(url = paste0(dir, files), export = files)\nprepare_libraries_sop_merged(files = files)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_merged"
    ]
  },
  {
    "objectID": "man/prepare_libraries_sop_merged.html#prepare-merged-structure-organism-pairs-libraries",
    "href": "man/prepare_libraries_sop_merged.html#prepare-merged-structure-organism-pairs-libraries",
    "title": "tima",
    "section": "",
    "text": "This function merges all structure-organism pair libraries (LOTUS, HMDB, ECMDB, etc.) into a single comprehensive library. Can optionally filter by taxonomic level to create biologically-focused subsets. Also splits structures into separate metadata tables.\n\n\n\nprepare_libraries_sop_merged(\n  files = get_params(step = \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$prepared,\n  filter = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$mode,\n  level = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$level,\n  value = get_params(step = \"prepare_libraries_sop_merged\")\\$organisms\\$filter\\$value,\n  cache = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$processed,\n  output_key = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$keys,\n  output_org_tax_ott = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$organisms\\$taxonomies\\$ott,\n  output_str_stereo = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$stereo,\n  output_str_met = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$metadata,\n  output_str_nam = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$names,\n  output_str_tax_cla = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$cla,\n  output_str_tax_npc = get_params(step =\n    \"prepare_libraries_sop_merged\")\\$files\\$libraries\\$sop\\$merged\\$structures\\$taxonomies\\$npc\n)\n\n\n\n\n\n\n\nfiles\n\n\nCharacter vector or list of paths to prepared library files\n\n\n\n\nfilter\n\n\nLogical whether to filter the merged library by taxonomy\n\n\n\n\nlevel\n\n\nCharacter string taxonomic rank for filtering (kingdom, phylum, family, genus, etc.)\n\n\n\n\nvalue\n\n\nCharacter string taxon name(s) to keep (can use | for multiple, e.g., ‘Gentianaceae|Apocynaceae’)\n\n\n\n\ncache\n\n\nCharacter string path to cache directory for processed SMILES\n\n\n\n\noutput_key\n\n\nCharacter string path for output keys file\n\n\n\n\noutput_org_tax_ott\n\n\nCharacter string path for organisms taxonomy (OTT) file\n\n\n\n\noutput_str_stereo\n\n\nCharacter string path for structures stereochemistry file\n\n\n\n\noutput_str_met\n\n\nCharacter string path for structures metadata file\n\n\n\n\noutput_str_nam\n\n\nCharacter string path for structures names file\n\n\n\n\noutput_str_tax_cla\n\n\nCharacter string path for ClassyFire taxonomy file\n\n\n\n\noutput_str_tax_npc\n\n\nCharacter string path for NPClassifier taxonomy file\n\n\n\n\n\n\nCreates merged library by combining all available SOP sources, optionally filtering by taxonomic criteria (e.g., only Gentianaceae). Splits output into structures metadata, names, taxonomy, and organisms.\n\n\n\nCharacter string path to the prepared merged SOP library\n\n\n\n\nlibrary(\"tima\")\n\ncopy_backbone()\ngo_to_cache()\ngithub &lt;- \"https://raw.githubusercontent.com/\"\nrepo &lt;- \"taxonomicallyinformedannotation/tima-example-files/main/\"\ndir &lt;- paste0(github, repo)\nfiles &lt;- get_params(step = \"prepare_libraries_sop_merged\")$files$libraries$sop$prepared$lotus |&gt;\n  gsub(pattern = \".gz\", replacement = \"\", fixed = TRUE)\nget_file(url = paste0(dir, files), export = files)\nprepare_libraries_sop_merged(files = files)\nunlink(\"data\", recursive = TRUE)",
    "crumbs": [
      "Get started",
      "Functions",
      "Prepare",
      "Libraries",
      "Structure Organism Pairs",
      "prepare_libraries_sop_merged"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tima ",
    "section": "",
    "text": "The initial work is available at https://doi.org/10.3389/fpls.2019.01329, with many improvements made since then. The workflow is illustrated below.\n\n\n\nWorkflow\n\n\nThis repository contains everything needed to perform Taxonomically Informed Metabolite Annotation.\n\n\n\n\nHere is what you minimally need:\n\nFeature quantification table (.csv/.tsv) - Peak areas/heights across samples (example)\n\nMust contain: feature ID, retention time, m/z, and sample intensity columns\nColumn names are customizable\n\nMS/MS spectra file (.mgf) - Fragment spectra for each or some features (example)\nSample metadata (.csv/.tsv) - Links samples to organisms (example)\n\nOptional if analyzing only a single organism\n\n\n\n\n\n\nStructure-organism pairs library - We provide LOTUS (&gt;650k pairs) as default\nExternal annotations - SIRIUS (v5/v6), GNPS-FBMN results\nCustom spectral libraries - For in-house compound matching\n\nTip: All column names and file paths are customizable through the Shiny app interface or YAML/CLI parameters - no need to rename your files!\n\n\n\n\nAs the package is not (yet) available on CRAN, you will need to install with:\ninstall.packages(\n  \"tima\",\n  repos = c(\n    \"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\",\n    \"https://cloud.r-project.org\"\n  )\n)\nThen, you should be able to install the rest with:\ntima::install()\nNormally, everything you need should then be installed (as tested in here). If for some reason, some packages were not installed, try to install them manually. To avoid such issues, we offer a containerized version (see Docker).\nOnce installed, you are ready to go through our documentation, with the major steps detailed.\nIn case you do not have your data ready, you can obtain some example data using:\ntima::get_example_files()\n\n\n\n\n\nStart by validating your input files to catch issues early and save debugging time:\n# Check if your data is matches expectations before processing\nvalidate_inputs(\n  features = \"data/source/example_features.csv\",\n  spectra = \"data/source/example_spectra.mgf\",\n  metadata = \"data/source/example_metadata.tsv\",\n  sirius = \"data/interim/annotations/example_sirius.zip\",\n  feature_col = \"row ID\",\n  filename_col = \"filename\",\n  organism_col = \"ATTRIBUTE_species\"\n  )\nThis will:\n\nCount spectra in MGF files\nCount features and check required columns\nCheck metadata file consistency\nReport eventual issues immediately\n\n\n\n\nOnce you are done, you can open a small GUI to adapt your parameters and launch your job:\ntima::run_app()\nThis command will open a small app in your default browser.\n\n\n\nA container is also available, together with a small compose file. Main commands are below:\ndocker pull adafede/tima-r\n# docker build . -t adafede/tima-r\ndocker run --user tima-user -v \"$(pwd)/.tima/data:/home/tima-user/.tima/data\" -p 3838:3838 adafede/tima-r Rscript -e \"tima::run_app()\"\n# docker run --user tima-user -v \"$(pwd)/.tima/data:/home/tima-user/.tima/data\" adafede/tima-r Rscript -e \"tima::run_tima()\"\n\n\n\n\nAccording to which steps you used, please give credit to the authors of the tools/resources used.\n\n\nGeneral: https://doi.org/10.3389/fpls.2019.01329\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5797920\n\n\n\nGeneral: https://doi.org/10.7554/eLife.70780\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5794106\n\n\n\nGeneral: https://doi.org/10.1021/acs.analchem.5b04804\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5607185\n\n\n\nGeneral: https://doi.org/10.1038/nbt.3597\n\n\n\nGeneral: https://doi.org/10.1038/s41592-019-0344-8\n\nCSI:FingerId: https://doi.org/10.1073/pnas.1509788112\nZODIAC: https://doi.org/10.1038/s42256-020-00234-6\nCANOPUS: https://doi.org/10.1038/s41587-020-0740-8\nCOSMIC: https://doi.org/10.1038/s41587-021-01045-9\n\n\n\n\n\nBiGG: https://doi.org/10.1093/nar/gkv1049\nECMDB 2.0: https://doi.org/10.1093/nar/gkv1060\nHMDB 5.0: https://doi.org/10.1093/nar/gkab1062\nMassBank: https://doi.org/10.5281/zenodo.3378723\nMerlin: https://doi.org/10.5281/zenodo.13911806\nNPClassifier: https://doi.org/10.1021/acs.jnatprod.1c00399\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\n\narchive\n1.1.12.1\nHester and Csárdi (2025)\n\n\nbase\n4.5.2\nR Core Team (2025)\n\n\nBiocManager\n1.30.27\nMorgan and Ramos (2025)\n\n\nBiocParallel\n1.44.0\nWang et al. (2025)\n\n\nBiocVersion\n3.22.0\nMorgan (2025)\n\n\ndocopt\n0.7.2\nde Jonge (2025)\n\n\nfs\n1.6.6\nHester, Wickham, and Csárdi (2025)\n\n\nhttr2\n1.2.2\nWickham (2025)\n\n\nigraph\n2.2.1\nCsárdi and Nepusz (2006); Antonov et al. (2023); Csárdi et al. (2025)\n\n\nIRanges\n2.44.0\nLawrence et al. (2013)\n\n\nknitr\n1.51\nXie (2014); Xie (2015); Xie (2025)\n\n\nlgr\n0.5.0\nFleck (2025)\n\n\nMetaboCoreUtils\n1.18.1\nRainer et al. (2022a)\n\n\nMsBackendMgf\n1.18.0\nGatto, Rainer, and Gibb (2025)\n\n\nMsBackendMsp\n1.14.0\nRainer et al. (2022b)\n\n\nMsCoreUtils\n1.22.1\nRainer et al. (2022c)\n\n\nmsentropy\n0.1.4\nLi (2023)\n\n\nR.utils\n2.13.0\nBengtsson (2025)\n\n\nreticulate\n1.44.1\nUshey, Allaire, and Tang (2025)\n\n\nrmarkdown\n2.30\nXie, Allaire, and Grolemund (2018); Xie, Dervieux, and Riederer (2020); Allaire et al. (2025)\n\n\nrotl\n3.1.0\nMichonneau, Brown, and Winter (2016); OpenTreeOfLife et al. (2019)\n\n\nshiny\n1.12.1\nChang et al. (2025)\n\n\nshinyhelper\n0.3.2\nMason-Thom (2019)\n\n\nshinyjs\n2.1.0\nAttali (2021)\n\n\nshinytest2\n0.4.1\nSchloerke (2025)\n\n\nshinyvalidate\n0.1.3\nSievert, Iannone, and Cheng (2023)\n\n\nSpectra\n1.20.0\nRainer et al. (2022d)\n\n\nstringi\n1.8.7\nGagolewski (2022)\n\n\ntargets\n1.11.4\nLandau (2021)\n\n\ntestthat\n3.3.1\nWickham (2011)\n\n\ntidyselect\n1.2.1\nHenry and Wickham (2024)\n\n\ntidytable\n0.11.2\nFairbanks (2024)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\ntima\n2.12.0\nRutz et al. (2019); Rutz and Allard (2025)\n\n\nwithr\n3.0.2\nHester et al. (2024)\n\n\nyaml\n2.3.12\nStephens and Simonov (2025)\n\n\n\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2025. rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nAntonov, Michael, Gábor Csárdi, Szabolcs Horvát, Kirill Müller, Tamás Nepusz, Daniel Noom, Maëlle Salmon, Vincent Traag, Brooke Foucault Welles, and Fabio Zanini. 2023. “Igraph Enables Fast and Robust Network Analysis Across Programming Languages.” arXiv Preprint arXiv:2311.10260. https://doi.org/10.48550/arXiv.2311.10260.\n\n\nAttali, Dean. 2021. shinyjs: Easily Improve the User Experience of Your Shiny Apps in Seconds. https://doi.org/10.32614/CRAN.package.shinyjs.\n\n\nBengtsson, Henrik. 2025. R.utils: Various Programming Utilities. https://doi.org/10.32614/CRAN.package.R.utils.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Garrick Aden-Buie, Yihui Xie, et al. 2025. shiny: Web Application Framework for r. https://doi.org/10.32614/CRAN.package.shiny.\n\n\nCsárdi, Gábor, and Tamás Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nCsárdi, Gábor, Tamás Nepusz, Vincent Traag, Szabolcs Horvát, Fabio Zanini, Daniel Noom, Kirill Müller, David Schoch, and Maëlle Salmon. 2025. igraph: Network Analysis and Visualization in r. https://doi.org/10.5281/zenodo.7682609.\n\n\nde Jonge, Edwin. 2025. docopt: Command-Line Interface Specification Language. https://doi.org/10.32614/CRAN.package.docopt.\n\n\nFairbanks, Mark. 2024. tidytable: Tidy Interface to “data.table”. https://doi.org/10.32614/CRAN.package.tidytable.\n\n\nFleck, Stefan. 2025. lgr: A Fully Featured Logging Framework. https://doi.org/10.32614/CRAN.package.lgr.\n\n\nGagolewski, Marek. 2022. “stringi: Fast and Portable Character String Processing in R.” Journal of Statistical Software 103 (2): 1–59. https://doi.org/10.18637/jss.v103.i02.\n\n\nGatto, Laurent, Johannes Rainer, and Sebastian Gibb. 2025. MsBackendMgf: Mass Spectrometry Data Backend for Mascot Generic Format (Mgf) Files. https://doi.org/10.18129/B9.bioc.MsBackendMgf.\n\n\nHenry, Lionel, and Hadley Wickham. 2024. tidyselect: Select from a Set of Strings. https://doi.org/10.32614/CRAN.package.tidyselect.\n\n\nHester, Jim, and Gábor Csárdi. 2025. archive: Multi-Format Archive and Compression Support. https://doi.org/10.32614/CRAN.package.archive.\n\n\nHester, Jim, Lionel Henry, Kirill Müller, Kevin Ushey, Hadley Wickham, and Winston Chang. 2024. withr: Run Code “With” Temporarily Modified Global State. https://doi.org/10.32614/CRAN.package.withr.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2025. fs: Cross-Platform File System Operations Based on “libuv”. https://doi.org/10.32614/CRAN.package.fs.\n\n\nLandau, William Michael. 2021. “The Targets r Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin Morgan, and Vincent Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Computational Biology 9. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nLi, Yuanyue. 2023. msentropy: Spectral Entropy for Mass Spectrometry Data. https://doi.org/10.32614/CRAN.package.msentropy.\n\n\nMason-Thom, Chris. 2019. shinyhelper: Easily Add Markdown Help Files to “shiny” App Elements. https://doi.org/10.32614/CRAN.package.shinyhelper.\n\n\nMichonneau, Francois, Joseph W. Brown, and David J. Winter. 2016. “rotl: An r Package to Interact with the Open Tree of Life Data.” Methods in Ecology and Evolution 7 (12): 1476–81. https://doi.org/10.1111/2041-210X.12593.\n\n\nMorgan, Martin. 2025. BiocVersion: Set the Appropriate Version of Bioconductor Packages. https://doi.org/10.18129/B9.bioc.BiocVersion.\n\n\nMorgan, Martin, and Marcel Ramos. 2025. BiocManager: Access the Bioconductor Project Package Repository. https://doi.org/10.32614/CRAN.package.BiocManager.\n\n\nOpenTreeOfLife, Benjamin Redelings, Luna Luisa Sanchez Reyes, Karen A. Cranston, Jim Allman, Mark T. Holder, and Emily Jane McTavish. 2019. “Open Tree of Life Synthetic Tree.” Zenodo. https://doi.org/10.5281/zenodo.3937741.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRainer, Johannes, Andrea Vicini, Liesa Salzer, Jan Stanstrup, Josep M. Badia, Steffen Neumann, Michael A. Stravs, et al. 2022a. “A Modular and Expandable Ecosystem for Metabolomics Data Annotation in r.” Metabolites 12: 173. https://doi.org/10.3390/metabo12020173.\n\n\n———, et al. 2022b. “A Modular and Expandable Ecosystem for Metabolomics Data Annotation in r.” Metabolites 12: 173. https://doi.org/10.3390/metabo12020173.\n\n\n———, et al. 2022c. “A Modular and Expandable Ecosystem for Metabolomics Data Annotation in r.” Metabolites 12: 173. https://doi.org/10.3390/metabo12020173.\n\n\n———, et al. 2022d. “A Modular and Expandable Ecosystem for Metabolomics Data Annotation in r.” Metabolites 12: 173. https://doi.org/10.3390/metabo12020173.\n\n\nRutz, Adriano, and Pierre-Marie Allard. 2025. tima: Taxonomically Informed Metabolite Annotation. https://doi.org/10.5281/zenodo.5797920.\n\n\nRutz, Adriano, Miwa Dounoue-Kubo, Simon Ollivier, Jonathan Bisson, Mohsen Bagheri, Tongchai Saesong, Samad Nejad Ebrahimi, Kornkanok Ingkaninan, Jean-Luc Wolfender, and Pierre-Marie Allard. 2019. “Taxonomically Informed Scoring Enhances Confidence in Natural Products Annotation.” Frontiers in Plant Science 10. https://doi.org/10.3389/FPLS.2019.01329.\n\n\nSchloerke, Barret. 2025. Shinytest2: Testing for Shiny Applications. https://doi.org/10.32614/CRAN.package.shinytest2.\n\n\nSievert, Carson, Richard Iannone, and Joe Cheng. 2023. shinyvalidate: Input Validation for Shiny Apps. https://doi.org/10.32614/CRAN.package.shinyvalidate.\n\n\nStephens, Jeremy, and Kirill Simonov. 2025. yaml: Methods to Convert r Data to YAML and Back. https://doi.org/10.32614/CRAN.package.yaml.\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2025. reticulate: Interface to “Python”. https://doi.org/10.32614/CRAN.package.reticulate.\n\n\nWang, Jiefei, Martin Morgan, Valerie Obenchain, Michel Lang, Ryan Thompson, and Nitesh Turaga. 2025. BiocParallel: Bioconductor Facilities for Parallel Evaluation. https://doi.org/10.18129/B9.bioc.BiocParallel.\n\n\nWickham, Hadley. 2011. “testthat: Get Started with Testing.” The R Journal 3: 5–10. https://journal.r-project.org/articles/RJ-2011-002/.\n\n\n———. 2025. Httr2: Perform HTTP Requests and Process the Responses. https://doi.org/10.32614/CRAN.package.httr2.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2014. “knitr: A Comprehensive Tool for Reproducible Research in R.” In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\n———. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n———. 2025. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook."
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "tima ",
    "section": "",
    "text": "Here is what you minimally need:\n\nFeature quantification table (.csv/.tsv) - Peak areas/heights across samples (example)\n\nMust contain: feature ID, retention time, m/z, and sample intensity columns\nColumn names are customizable\n\nMS/MS spectra file (.mgf) - Fragment spectra for each or some features (example)\nSample metadata (.csv/.tsv) - Links samples to organisms (example)\n\nOptional if analyzing only a single organism\n\n\n\n\n\n\nStructure-organism pairs library - We provide LOTUS (&gt;650k pairs) as default\nExternal annotations - SIRIUS (v5/v6), GNPS-FBMN results\nCustom spectral libraries - For in-house compound matching\n\nTip: All column names and file paths are customizable through the Shiny app interface or YAML/CLI parameters - no need to rename your files!"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "tima ",
    "section": "",
    "text": "As the package is not (yet) available on CRAN, you will need to install with:\ninstall.packages(\n  \"tima\",\n  repos = c(\n    \"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\",\n    \"https://cloud.r-project.org\"\n  )\n)\nThen, you should be able to install the rest with:\ntima::install()\nNormally, everything you need should then be installed (as tested in here). If for some reason, some packages were not installed, try to install them manually. To avoid such issues, we offer a containerized version (see Docker).\nOnce installed, you are ready to go through our documentation, with the major steps detailed.\nIn case you do not have your data ready, you can obtain some example data using:\ntima::get_example_files()"
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "tima ",
    "section": "",
    "text": "Start by validating your input files to catch issues early and save debugging time:\n# Check if your data is matches expectations before processing\nvalidate_inputs(\n  features = \"data/source/example_features.csv\",\n  spectra = \"data/source/example_spectra.mgf\",\n  metadata = \"data/source/example_metadata.tsv\",\n  sirius = \"data/interim/annotations/example_sirius.zip\",\n  feature_col = \"row ID\",\n  filename_col = \"filename\",\n  organism_col = \"ATTRIBUTE_species\"\n  )\nThis will:\n\nCount spectra in MGF files\nCount features and check required columns\nCheck metadata file consistency\nReport eventual issues immediately\n\n\n\n\nOnce you are done, you can open a small GUI to adapt your parameters and launch your job:\ntima::run_app()\nThis command will open a small app in your default browser.\n\n\n\nA container is also available, together with a small compose file. Main commands are below:\ndocker pull adafede/tima-r\n# docker build . -t adafede/tima-r\ndocker run --user tima-user -v \"$(pwd)/.tima/data:/home/tima-user/.tima/data\" -p 3838:3838 adafede/tima-r Rscript -e \"tima::run_app()\"\n# docker run --user tima-user -v \"$(pwd)/.tima/data:/home/tima-user/.tima/data\" adafede/tima-r Rscript -e \"tima::run_tima()\""
  },
  {
    "objectID": "index.html#main-citations",
    "href": "index.html#main-citations",
    "title": "tima ",
    "section": "",
    "text": "According to which steps you used, please give credit to the authors of the tools/resources used.\n\n\nGeneral: https://doi.org/10.3389/fpls.2019.01329\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5797920\n\n\n\nGeneral: https://doi.org/10.7554/eLife.70780\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5794106\n\n\n\nGeneral: https://doi.org/10.1021/acs.analchem.5b04804\n⚠️ Do not forget to cite which version you used: https://doi.org/10.5281/zenodo.5607185\n\n\n\nGeneral: https://doi.org/10.1038/nbt.3597\n\n\n\nGeneral: https://doi.org/10.1038/s41592-019-0344-8\n\nCSI:FingerId: https://doi.org/10.1073/pnas.1509788112\nZODIAC: https://doi.org/10.1038/s42256-020-00234-6\nCANOPUS: https://doi.org/10.1038/s41587-020-0740-8\nCOSMIC: https://doi.org/10.1038/s41587-021-01045-9\n\n\n\n\n\nBiGG: https://doi.org/10.1093/nar/gkv1049\nECMDB 2.0: https://doi.org/10.1093/nar/gkv1060\nHMDB 5.0: https://doi.org/10.1093/nar/gkab1062\nMassBank: https://doi.org/10.5281/zenodo.3378723\nMerlin: https://doi.org/10.5281/zenodo.13911806\nNPClassifier: https://doi.org/10.1021/acs.jnatprod.1c00399"
  },
  {
    "objectID": "index.html#additional-software-credits",
    "href": "index.html#additional-software-credits",
    "title": "tima ",
    "section": "",
    "text": "Package\nVersion\nCitation\n\n\n\n\narchive\n1.1.12.1\nHester and Csárdi (2025)\n\n\nbase\n4.5.2\nR Core Team (2025)\n\n\nBiocManager\n1.30.27\nMorgan and Ramos (2025)\n\n\nBiocParallel\n1.44.0\nWang et al. (2025)\n\n\nBiocVersion\n3.22.0\nMorgan (2025)\n\n\ndocopt\n0.7.2\nde Jonge (2025)\n\n\nfs\n1.6.6\nHester, Wickham, and Csárdi (2025)\n\n\nhttr2\n1.2.2\nWickham (2025)\n\n\nigraph\n2.2.1\nCsárdi and Nepusz (2006); Antonov et al. (2023); Csárdi et al. (2025)\n\n\nIRanges\n2.44.0\nLawrence et al. (2013)\n\n\nknitr\n1.51\nXie (2014); Xie (2015); Xie (2025)\n\n\nlgr\n0.5.0\nFleck (2025)\n\n\nMetaboCoreUtils\n1.18.1\nRainer et al. (2022a)\n\n\nMsBackendMgf\n1.18.0\nGatto, Rainer, and Gibb (2025)\n\n\nMsBackendMsp\n1.14.0\nRainer et al. (2022b)\n\n\nMsCoreUtils\n1.22.1\nRainer et al. (2022c)\n\n\nmsentropy\n0.1.4\nLi (2023)\n\n\nR.utils\n2.13.0\nBengtsson (2025)\n\n\nreticulate\n1.44.1\nUshey, Allaire, and Tang (2025)\n\n\nrmarkdown\n2.30\nXie, Allaire, and Grolemund (2018); Xie, Dervieux, and Riederer (2020); Allaire et al. (2025)\n\n\nrotl\n3.1.0\nMichonneau, Brown, and Winter (2016); OpenTreeOfLife et al. (2019)\n\n\nshiny\n1.12.1\nChang et al. (2025)\n\n\nshinyhelper\n0.3.2\nMason-Thom (2019)\n\n\nshinyjs\n2.1.0\nAttali (2021)\n\n\nshinytest2\n0.4.1\nSchloerke (2025)\n\n\nshinyvalidate\n0.1.3\nSievert, Iannone, and Cheng (2023)\n\n\nSpectra\n1.20.0\nRainer et al. (2022d)\n\n\nstringi\n1.8.7\nGagolewski (2022)\n\n\ntargets\n1.11.4\nLandau (2021)\n\n\ntestthat\n3.3.1\nWickham (2011)\n\n\ntidyselect\n1.2.1\nHenry and Wickham (2024)\n\n\ntidytable\n0.11.2\nFairbanks (2024)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\ntima\n2.12.0\nRutz et al. (2019); Rutz and Allard (2025)\n\n\nwithr\n3.0.2\nHester et al. (2024)\n\n\nyaml\n2.3.12\nStephens and Simonov (2025)\n\n\n\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2025. rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nAntonov, Michael, Gábor Csárdi, Szabolcs Horvát, Kirill Müller, Tamás Nepusz, Daniel Noom, Maëlle Salmon, Vincent Traag, Brooke Foucault Welles, and Fabio Zanini. 2023. “Igraph Enables Fast and Robust Network Analysis Across Programming Languages.” arXiv Preprint arXiv:2311.10260. https://doi.org/10.48550/arXiv.2311.10260.\n\n\nAttali, Dean. 2021. shinyjs: Easily Improve the User Experience of Your Shiny Apps in Seconds. https://doi.org/10.32614/CRAN.package.shinyjs.\n\n\nBengtsson, Henrik. 2025. R.utils: Various Programming Utilities. https://doi.org/10.32614/CRAN.package.R.utils.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Garrick Aden-Buie, Yihui Xie, et al. 2025. shiny: Web Application Framework for r. https://doi.org/10.32614/CRAN.package.shiny.\n\n\nCsárdi, Gábor, and Tamás Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nCsárdi, Gábor, Tamás Nepusz, Vincent Traag, Szabolcs Horvát, Fabio Zanini, Daniel Noom, Kirill Müller, David Schoch, and Maëlle Salmon. 2025. igraph: Network Analysis and Visualization in r. https://doi.org/10.5281/zenodo.7682609.\n\n\nde Jonge, Edwin. 2025. docopt: Command-Line Interface Specification Language. https://doi.org/10.32614/CRAN.package.docopt.\n\n\nFairbanks, Mark. 2024. tidytable: Tidy Interface to “data.table”. https://doi.org/10.32614/CRAN.package.tidytable.\n\n\nFleck, Stefan. 2025. lgr: A Fully Featured Logging Framework. https://doi.org/10.32614/CRAN.package.lgr.\n\n\nGagolewski, Marek. 2022. “stringi: Fast and Portable Character String Processing in R.” Journal of Statistical Software 103 (2): 1–59. https://doi.org/10.18637/jss.v103.i02.\n\n\nGatto, Laurent, Johannes Rainer, and Sebastian Gibb. 2025. MsBackendMgf: Mass Spectrometry Data Backend for Mascot Generic Format (Mgf) Files. https://doi.org/10.18129/B9.bioc.MsBackendMgf.\n\n\nHenry, Lionel, and Hadley Wickham. 2024. tidyselect: Select from a Set of Strings. https://doi.org/10.32614/CRAN.package.tidyselect.\n\n\nHester, Jim, and Gábor Csárdi. 2025. archive: Multi-Format Archive and Compression Support. https://doi.org/10.32614/CRAN.package.archive.\n\n\nHester, Jim, Lionel Henry, Kirill Müller, Kevin Ushey, Hadley Wickham, and Winston Chang. 2024. withr: Run Code “With” Temporarily Modified Global State. https://doi.org/10.32614/CRAN.package.withr.\n\n\nHester, Jim, Hadley Wickham, and Gábor Csárdi. 2025. fs: Cross-Platform File System Operations Based on “libuv”. https://doi.org/10.32614/CRAN.package.fs.\n\n\nLandau, William Michael. 2021. “The Targets r Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nLawrence, Michael, Wolfgang Huber, Hervé Pagès, Patrick Aboyoun, Marc Carlson, Robert Gentleman, Martin Morgan, and Vincent Carey. 2013. “Software for Computing and Annotating Genomic Ranges.” PLoS Computational Biology 9. https://doi.org/10.1371/journal.pcbi.1003118.\n\n\nLi, Yuanyue. 2023. msentropy: Spectral Entropy for Mass Spectrometry Data. https://doi.org/10.32614/CRAN.package.msentropy.\n\n\nMason-Thom, Chris. 2019. shinyhelper: Easily Add Markdown Help Files to “shiny” App Elements. https://doi.org/10.32614/CRAN.package.shinyhelper.\n\n\nMichonneau, Francois, Joseph W. Brown, and David J. Winter. 2016. “rotl: An r Package to Interact with the Open Tree of Life Data.” Methods in Ecology and Evolution 7 (12): 1476–81. https://doi.org/10.1111/2041-210X.12593.\n\n\nMorgan, Martin. 2025. BiocVersion: Set the Appropriate Version of Bioconductor Packages. https://doi.org/10.18129/B9.bioc.BiocVersion.\n\n\nMorgan, Martin, and Marcel Ramos. 2025. BiocManager: Access the Bioconductor Project Package Repository. https://doi.org/10.32614/CRAN.package.BiocManager.\n\n\nOpenTreeOfLife, Benjamin Redelings, Luna Luisa Sanchez Reyes, Karen A. Cranston, Jim Allman, Mark T. Holder, and Emily Jane McTavish. 2019. “Open Tree of Life Synthetic Tree.” Zenodo. https://doi.org/10.5281/zenodo.3937741.\n\n\nR Core Team. 2025. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRainer, Johannes, Andrea Vicini, Liesa Salzer, Jan Stanstrup, Josep M. Badia, Steffen Neumann, Michael A. Stravs, et al. 2022a. “A Modular and Expandable Ecosystem for Metabolomics Data Annotation in r.” Metabolites 12: 173. https://doi.org/10.3390/metabo12020173.\n\n\n———, et al. 2022b. “A Modular and Expandable Ecosystem for Metabolomics Data Annotation in r.” Metabolites 12: 173. https://doi.org/10.3390/metabo12020173.\n\n\n———, et al. 2022c. “A Modular and Expandable Ecosystem for Metabolomics Data Annotation in r.” Metabolites 12: 173. https://doi.org/10.3390/metabo12020173.\n\n\n———, et al. 2022d. “A Modular and Expandable Ecosystem for Metabolomics Data Annotation in r.” Metabolites 12: 173. https://doi.org/10.3390/metabo12020173.\n\n\nRutz, Adriano, and Pierre-Marie Allard. 2025. tima: Taxonomically Informed Metabolite Annotation. https://doi.org/10.5281/zenodo.5797920.\n\n\nRutz, Adriano, Miwa Dounoue-Kubo, Simon Ollivier, Jonathan Bisson, Mohsen Bagheri, Tongchai Saesong, Samad Nejad Ebrahimi, Kornkanok Ingkaninan, Jean-Luc Wolfender, and Pierre-Marie Allard. 2019. “Taxonomically Informed Scoring Enhances Confidence in Natural Products Annotation.” Frontiers in Plant Science 10. https://doi.org/10.3389/FPLS.2019.01329.\n\n\nSchloerke, Barret. 2025. Shinytest2: Testing for Shiny Applications. https://doi.org/10.32614/CRAN.package.shinytest2.\n\n\nSievert, Carson, Richard Iannone, and Joe Cheng. 2023. shinyvalidate: Input Validation for Shiny Apps. https://doi.org/10.32614/CRAN.package.shinyvalidate.\n\n\nStephens, Jeremy, and Kirill Simonov. 2025. yaml: Methods to Convert r Data to YAML and Back. https://doi.org/10.32614/CRAN.package.yaml.\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2025. reticulate: Interface to “Python”. https://doi.org/10.32614/CRAN.package.reticulate.\n\n\nWang, Jiefei, Martin Morgan, Valerie Obenchain, Michel Lang, Ryan Thompson, and Nitesh Turaga. 2025. BiocParallel: Bioconductor Facilities for Parallel Evaluation. https://doi.org/10.18129/B9.bioc.BiocParallel.\n\n\nWickham, Hadley. 2011. “testthat: Get Started with Testing.” The R Journal 3: 5–10. https://journal.r-project.org/articles/RJ-2011-002/.\n\n\n———. 2025. Httr2: Perform HTTP Requests and Process the Responses. https://doi.org/10.32614/CRAN.package.httr2.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nXie, Yihui. 2014. “knitr: A Comprehensive Tool for Reproducible Research in R.” In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\n———. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n———. 2025. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://yihui.org/knitr/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook."
  },
  {
    "objectID": "vignettes/tima.html",
    "href": "vignettes/tima.html",
    "title": "General comments about the infrastructure",
    "section": "",
    "text": "This vignette describes the philosophy behind the infrastructure of TIMA."
  },
  {
    "objectID": "vignettes/tima.html#philosophy",
    "href": "vignettes/tima.html#philosophy",
    "title": "General comments about the infrastructure",
    "section": "Philosophy",
    "text": "Philosophy\nOur main goals were flexibility and reproducibility.\n\nFlexibility\nTo ensure flexibility, we tried to split the process in as much tiny parts as needed. So you can decide whether to skip an optional part, add your own processing, etc. We tried to cover most use cases, but of course they are not exhaustive. If you feel like something useful to other users is missing, please fill an issue.\n\n\nReproducibility\nAfter some time using TIMA, you will probably wonder: “What was the parameters I used to generate this file?” … Or a collaborator might ask you to share your data and parameters. Writing them down each time might be time-consuming and not really in line with modern computational approaches. Therefore, we chose to implement all parameters of all steps (almost…) as YAML files. They are human-readable and can be used in batches. If you do not like YAML, parameters of each step can also be given as command line arguments. They will then be saved as YAML you will be able to share.\nTo ensure optimal reproducibility and avoiding re-computing endlessly steps that did not change, we decided to build a {targets} pipeline. Each step of the whole pipeline will be described next."
  },
  {
    "objectID": "vignettes/tima.html#use",
    "href": "vignettes/tima.html#use",
    "title": "General comments about the infrastructure",
    "section": "Use",
    "text": "Use\nAll coming steps admit you already installed tima:\n\ninstall.packages(\n  \"tima\",\n  repos = c(\n    \"https://taxonomicallyinformedannotation.r-universe.dev\",\n    \"https://bioc.r-universe.dev\",\n    \"https://cloud.r-project.org\"\n  )\n)\ntima::install()\ntima::get_example_files()\n\nWe now recommend you to read the following vignettes:\n\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/0-validating.html Start here! Validate your data before running the pipeline\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/I-gathering.html\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/II-preparing.html\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/III-processing.html\nhttps://taxonomicallyinformedannotation.github.io/tima/vignettes/articles/IV-benchmarking.html\n\n\ntl;dr\nImportant: Always validate your data to catch issues early!\n\nvalidate_inputs(\n  features = \"data/source/example_features.csv\",\n  spectra = \"data/source/example_spectra.mgf\",\n  metadata = \"data/source/example_metadata.tsv\",\n  sirius = \"data/interim/annotations/example_sirius.zip\",\n  feature_col = \"row ID\",\n  filename_col = \"filename\",\n  organism_col = \"ATTRIBUTE_species\"\n  )\n\nIf you do not feel like going through all the steps, then just do 🚀:\n\ntima::run_app()\n\nIf you do not even need a GUI ☠️:\n\ntima::run_tima()\n\nIn case you just want to change some small parameters between jobs, a convenience function is available:\n\ntima::change_params_small(\n  fil_pat = \"myExamplePattern\",\n  fil_fea_raw = \"myExampleDir/myExampleFeatures.csv\",\n  fil_met_raw = \"myExampleDir2SomeWhereElse/myOptionalMetadata.tsv\",\n  fil_sir_raw = \"myExampleDir3/myAwesomeSiriusProject.zip\",\n  fil_spe_raw = \"myBeautifulSpectra.mgf\",\n  ms_pol = \"pos\",\n  org_tax = \"Gentiana lutea\",\n  hig_con = TRUE,\n  summarize = FALSE\n)"
  },
  {
    "objectID": "vignettes/tima.html#biological-weighting-and-core-metabolism",
    "href": "vignettes/tima.html#biological-weighting-and-core-metabolism",
    "title": "General comments about the infrastructure",
    "section": "Biological Weighting and Core Metabolism",
    "text": "Biological Weighting and Core Metabolism\nTIMA’s core principle is that candidates that are close to your sample’s taxonomic origin receive higher biological scores. For example, if you are analyzing a plant extract (Gentiana lutea), metabolites reported from plants will score higher than those only reported from bacteria or animals.\n\nThe Special Case: “Biota” Superdomain\nHowever, some metabolites are universal, they are part of the shared core metabolism found across all domains of life. To ensure these universal metabolites are always considered in annotations regardless of sample taxonomy, TIMA implements a special \"Biota\" Superdomain. When preparing structure-organism pair libraries (e.g., from BiGG metabolic models), metabolites present in all model organisms are assigned to the special “Biota” organism with:\n\norganism_taxonomy_01domain = \"Biota\"\norganism_taxonomy_ottid = 0\n\nDuring biological weighting, any candidate from the Biota domain receives the maximal biological score. This ensures that core metabolic pathways are never inappropriately filtered out due to taxonomic mismatches, while still maintaining the taxonomic prioritization for organism-specific specialized metabolism."
  },
  {
    "objectID": "vignettes/articles/I-gathering.html",
    "href": "vignettes/articles/I-gathering.html",
    "title": "1 Gathering everything you need",
    "section": "",
    "text": "This vignette describes…"
  },
  {
    "objectID": "vignettes/articles/I-gathering.html#parameters",
    "href": "vignettes/articles/I-gathering.html#parameters",
    "title": "1 Gathering everything you need",
    "section": "Parameters",
    "text": "Parameters\nAll steps require parameters. Some default parameters are available and can be accessed in the params/default directory. If you prefer accessing them through the GUI, you can do so. Each parameter contains a small help menu, you can click on, as illustrated below.\n\n\n\n\nFor example, if you want to have an output compatible with Cytoscape, with multiple annotations per features:\n\n\nAll parameters will be saved and reported at the end of your analysis."
  },
  {
    "objectID": "vignettes/articles/I-gathering.html#inputs",
    "href": "vignettes/articles/I-gathering.html#inputs",
    "title": "1 Gathering everything you need",
    "section": "Inputs",
    "text": "Inputs\n\nYour own files\nYou should provide your own files in the main menu. For this tutorial, we will use some example files you can get running:\n\ntima::get_example_files()\n\n\n\nLibraries\nThe following paragraph describes the libraries available by default.\n\nSpectra\nAs a first step, you need spectral libraries to perform MS2-based annotation.\n\nExperimental\nYou can of course use your own experimental spectral library to perform MS2 annotation. We currently support spectral libraries in MSP or MGF format.\nTo get a small example:\n\ntima::get_example_files(\"spectral_lib_with_rt\")\n\n\nGNPS, MassBank & MERLIN\nGNPS, MassBank & MERLIN are downloaded and used by default, for more info about them, see https://github.com/Adafede/SpectRalLibRaRies.\nIn case you want to format your own spectral library to use it for spectral matching, adapt it the same way.\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_spe_exp\")\n)\n\n\n\n\nIn silico\nAs the availability of experimental spectra is limited, we can take advantage of in silico generated spectra.\n\n\n\nWikidata\nWe generated an in silico spectral library of the structures found in Wikidata using CFM4. For more info, see https://doi.org/10.5281/zenodo.5607185. It is made available in both polarities.\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_spe_is_wik\")\n)\n\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_spe_is_wik_pre\")\n)\n\nYou can also complement with the in silico spectra from HMDB (not running by default as quite long):\n\n\nHMDB\n\ntima::get_example_files(\"hmdb_is\")\n\n\n\nRetention times\nThis library is optional. As no standard LC method is shared (for now) among laboratories, this library will be heavily laboratory-dependent. It could also be a library of in silico predicted retention times. If you want to prepare you own library, have a look at params/user/prepare_libraries_rt.yaml.\n\n\nStructure-Organism Pairs\n\nLOTUS\nAs we developed LOTUS1 with Taxonomically Informed Metabolite Annotation in mind, we provide it here as a starting point for your structure-organism pairs library.\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_sop_lot$\")\n)\n\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_sop_lot_pre\")\n)\n\nThe process to download LOTUS looks like this:\n\n\n\n\n\n\nAs you can see, the targets seem outdated. In reality, we force it to search if a new version of LOTUS exists each time. If a newer version exists, it will fetch it and re-run needed steps accordingly.\n\n\nBiGG\nBy default, we also complement LOTUS pairs with the ones coming from BiGG.\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_sop_big_pre\")\n)\n\n\n\nECMDB\nAnd the ones from ECMDB.\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_sop_ecm_pre\")\n)\n\n\n\nHMDB\nAnd we do the same with the ones coming from HMDB.\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_sop_hmd_pre\")\n)\n\nFor these first steps, you do not need to change any parameters as they are implemented by default.\n\n\nOther libraries\nAs we want our tool to be flexible, you can also add your own library to LOTUS. You just need to format it in order to be compatible. As example, we prepared some ways too format closed, in house libraries. If you need help formatting your library or would like to share it with us for it to be implemented, feel free to contact us. Before running the corresponding code, do not forget to modify params/user/prepare_libraries_sop_closed.yaml.\n\n\nMerging\nOnce all sub-libraries are ready, they are then merged in a single file that will be used for the next steps.\n\ntargets::tar_make(\n  names = tidyselect::matches(\"lib_sop_mer\")\n)\n\n\n\n\n\n\n\nWe now recommend you to read the next vignette."
  },
  {
    "objectID": "vignettes/articles/I-gathering.html#footnotes",
    "href": "vignettes/articles/I-gathering.html#footnotes",
    "title": "1 Gathering everything you need",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more informations, see https://doi.org/10.7554/eLife.70780↩︎"
  },
  {
    "objectID": "vignettes/articles/II-preparing.html",
    "href": "vignettes/articles/II-preparing.html",
    "title": "2 Preparing inputs",
    "section": "",
    "text": "This vignette describes the main steps of the annotation process."
  },
  {
    "objectID": "vignettes/articles/II-preparing.html#structural-annotations-of-your-features",
    "href": "vignettes/articles/II-preparing.html#structural-annotations-of-your-features",
    "title": "2 Preparing inputs",
    "section": "Structural annotations of your features",
    "text": "Structural annotations of your features\nFor the moment, we support 3 different types of annotations:\n\n\nInternal MS1 exact mass-based library search\nInternal MS2 library search (experimental and in silico)\nSIRIUS\n\nIf needed, you can get an example of what your minimal feature table should look like by running:\n\ntima::get_example_files(example = \"features\")\n\nThen, you can prepare your features for the next steps:\n\ntima::prepare_features_tables()\n\n\nMS1-based\nThese annotations are of the lowest possible quality. However, they allow to annotate unusual adducts, in-source fragments thanks to different small tricks implemented. Try to really restrict the adduct list and structure-organism pairs you want to consider as possibilities explode rapidly.\n\ntima::annotate_masses()\n\n\n\nSpectral\n\ntima::annotate_spectra()\ntima::prepare_annotations_spectra()\n\nThere are multiple similarities available, including spectral entropy from https://doi.org/10.1038/s41592-021-01331-z for matching.\n\nGNPS\nOptional\n\ntima::prepare_annotations_gnps()\n\n\n\n\nFingerprint-based\n\nSirius\nAs SIRIUS jobs are long to perform, we provide example SIRIUS workspaces (both SIRIUS 5 and 6). Note that spectral matches from SIRIUS are not supported for now. They have been generated on the 20 first lines of the example MGF with the following command:\n\n# this is run on SIRIUS 6\nsirius \\\n--noCite \\\n--input=data/source/example_spectra_mini.mgf \\\n--output=data/interim/annotations/example_sirius.sirius/ \\\n--maxmz=800 \\\nconfig \\\n--AlgorithmProfile=orbitrap \\\n--StructureSearchDB=BIO \\\n--Timeout.secondsPerTree=10 \\\n--Timeout.secondsPerInstance=10 \\\nformulas \\\nzodiac \\\nfingerprints \\\nclasses \\\nstructures \\\ndenovo-structures \\\nsummaries \\\n--chemvista \\\n--feature-quality-summary \\\n--full-summary\n\n# this is run on SIRIUS 5\nsirius \\\n--noCite \\\n--input data/source/example_spectra_mini.mgf \\\n--output data/interim/annotations/example_sirius/ \\\n--maxmz 800 \\\nconfig \\\n--AlgorithmProfile orbitrap \\\n--StructureSearchDB BIO \\\n--Timeout.secondsPerTree 10 \\\n--Timeout.secondsPerInstance 10 \\\nformula \\\nzodiac \\\nfingerprint \\\nstructure \\\ncompound-classes \\\nwrite-summaries \\\n--full-summary\n\nThese parameters were not optimized and were only used to give an example output. If you are using the cli, do not forget to generate the summaries with the --full-summary option, or if you use the gui, generate them by clicking the corresponding icon. You can get an example running:\n\ntima:::get_example_sirius()\n\nThe sirius workspace should ideally have yourPattern_sirius as name and be placed in data/interim/annotations (else it will not be found by default except you provide the right path).\n\ntima::prepare_annotations_sirius()\n\nIf you want to know how we attempt to combine the CSI score with other ones, see R/transform_score_sirius_csi.R Note that starting from SIRIUS 6, the approx confidence score is the one considered, and not the exact one.\n\n\n\n\n\n\nAnnotations are now prepared and can be used for further processing. Your features are not only informed with structural information but also, chemical class information. The latter might be corresponding or not to the chemical class of your annotated structure, depending on the consistency of your annotations."
  },
  {
    "objectID": "vignettes/articles/II-preparing.html#chemical-class-annotation-of-your-features",
    "href": "vignettes/articles/II-preparing.html#chemical-class-annotation-of-your-features",
    "title": "2 Preparing inputs",
    "section": "Chemical class annotation of your features",
    "text": "Chemical class annotation of your features\nWithin our workflow, we offer a new way to attribute chemical classes to your features. It is analog to Network Annotation Propagation, but uses the edges of your network instead of the clusters. This makes more sense in our view, as also recently illustrated by CANOPUS.\n\nWe are currently also working on CANOPUS integration for chemical class annotation but this implies way heavier computations and we want to offer our users a fast solution.\n\nGenerating a network\nA network is generated during the process. The edges are created based on the spectral entropy similarity calculated between your spectra (see https://doi.org/10.1038/s41592-021-01331-z).\n\ntima::create_edges_spectra()\n\nPrepare the edges:\n\ntima::prepare_features_edges()\n\nCreate and prepare the components:\n\ntima::create_components()\n\n\ntima::prepare_features_components()"
  },
  {
    "objectID": "vignettes/articles/II-preparing.html#biological-source-annotation",
    "href": "vignettes/articles/II-preparing.html#biological-source-annotation",
    "title": "2 Preparing inputs",
    "section": "Biological source annotation",
    "text": "Biological source annotation\nThis step allows you to attribute biological source information to your features. If all your features come from a single extract, it will attribute the biological source of your extract to all your features. If you have multiple extracts aligned, it will take the n (according to your parameters) highest intensities of your aligned feature table and attribute the biological source of corresponding extracts. \n\ntima::prepare_taxa()"
  },
  {
    "objectID": "vignettes/articles/II-preparing.html#filter-annotations-based-on-retention-time",
    "href": "vignettes/articles/II-preparing.html#filter-annotations-based-on-retention-time",
    "title": "2 Preparing inputs",
    "section": "Filter annotations (based on retention time)",
    "text": "Filter annotations (based on retention time)\nThis step allows you to filter out the annotation of all the tools used, based on your own internal (experimental or predicted) retention times library. It is optional. If you do not have one, it will simply group the annotations of all tools.\n\ntima::filter_annotations()\n\nYou are almost there! See already all the steps accomplished!\n\n\n\n\n\n\nWe now recommend you to read the next vignette."
  }
]